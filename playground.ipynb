{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c933e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-uutun5j5\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-uutun5j5\n",
      "  Resolved https://github.com/openai/CLIP.git to commit 573315e83f07b53a61ff5098757e8fc885f1703e\n",
      "Requirement already satisfied: ftfy in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from clip==1.0) (6.0.3)\n",
      "Requirement already satisfied: regex in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from clip==1.0) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from clip==1.0) (4.62.2)\n",
      "Requirement already satisfied: torch in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from clip==1.0) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from clip==1.0) (0.11.1)\n",
      "Requirement already satisfied: wcwidth in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from torch->clip==1.0) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from torchvision->clip==1.0) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.21.2)\n",
      "--2021-12-01 13:00:59--  https://people.eecs.berkeley.edu/~efros/img/Efros__photo_Peter_Badge_crop.jpg\n",
      "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456031 (445K) [image/jpeg]\n",
      "Saving to: ‘Efros__photo_Peter_Badge_crop.jpg’\n",
      "\n",
      "Efros__photo_Peter_ 100%[===================>] 445.34K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-12-01 13:00:59 (26.3 MB/s) - ‘Efros__photo_Peter_Badge_crop.jpg’ saved [456031/456031]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!wget https://people.eecs.berkeley.edu/~efros/img/Efros__photo_Peter_Badge_crop.jpg\n",
    "!mv Efros__photo_Peter_Badge_crop.jpg efros.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ceeeadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  9 21:02:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Quadro R...  On   | 00000000:1A:00.0 Off |                  Off |\n",
      "| 33%   54C    P2   242W / 260W |  22852MiB / 24220MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Quadro R...  On   | 00000000:1B:00.0 Off |                  Off |\n",
      "| 33%   30C    P8    17W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA Quadro R...  On   | 00000000:1C:00.0 Off |                  Off |\n",
      "| 33%   30C    P8    13W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA Quadro R...  On   | 00000000:1D:00.0 Off |                  Off |\n",
      "| 33%   56C    P2   238W / 260W |  22852MiB / 24220MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA Quadro R...  On   | 00000000:1E:00.0 Off |                  Off |\n",
      "| 33%   59C    P2   226W / 260W |  20982MiB / 24220MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA Quadro R...  On   | 00000000:3D:00.0 Off |                  Off |\n",
      "| 33%   30C    P8    12W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA Quadro R...  On   | 00000000:3E:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    12W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA Quadro R...  On   | 00000000:3F:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    12W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  NVIDIA Quadro R...  On   | 00000000:40:00.0 Off |                  Off |\n",
      "| 33%   29C    P8    14W / 260W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  NVIDIA Quadro R...  On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   28C    P8    14W / 260W |      3MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   4161084      C   python                          22849MiB |\n",
      "|    3   N/A  N/A   4161293      C   python                          22849MiB |\n",
      "|    4   N/A  N/A   4167748      C   python                          20979MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff78c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876a8d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from time import perf_counter\n",
    "import click\n",
    "import imageio\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "from projector import *\n",
    "\n",
    "from metrics.frechet_inception_distance import compute_fid_diffusion_vs_dataset, compute_fid_gaussian_vs_dataset\n",
    "from metrics.kernel_inception_distance import compute_kid_projector\n",
    "from metrics.metric_utils import MetricOptions, MetricOptionsDiffusion, MetricOptionsDistribution, MetricOptionsProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6912f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(projected_w_steps, projector, num_rows=1, outdir=\"out\", name=\"test.mp4\"):\n",
    "    # Render debug output: optional video and projected image and W vector.\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        video = imageio.get_writer(f'{outdir}/{name}', mode='I', fps=10, codec='libx264', bitrate='8M')\n",
    "        print (f'Saving optimization progress video {outdir}/{name}')\n",
    "        frame_count = 0\n",
    "        \n",
    "\n",
    "        for projected_w in projected_w_steps:\n",
    "            frame_count += 1\n",
    "            if projected_w_steps.shape[0] < 500 or frame_count % 10 == 0:\n",
    "                synth_image = projector.gen.latent_to_image(projected_w)\n",
    "                synth_image = (synth_image + 1) * (255/2)\n",
    "                synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "                grid_image = einops.rearrange(synth_image, \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "                video.append_data(grid_image)\n",
    "                # video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n",
    "        video.close()\n",
    "def save_image(w, projector, num_rows=1, outdir=\"out\", name=\"test.png\"):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print (f'Saving image {outdir}/{name}')\n",
    "    with torch.no_grad():\n",
    "        synth_image = projector.gen.latent_to_image(w)\n",
    "        synth_image = (synth_image + 1) * (255/2)\n",
    "        synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "        grid_image = einops.rearrange(synth_image, \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "        plt.imsave(f'{outdir}/{name}.png', grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf1c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
    "network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-f.pkl\"\n",
    "target_fname = \"efros.jpg\"\n",
    "outdir = \"out\"\n",
    "save_video = True\n",
    "seed = 203\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "num_steps = 200\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efc27f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Normalizing across all channels.\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(network_pkl, normalize_latent=\"all_dims\", device=device, latent_space=\"style\")\n",
    "# generator = Generator(network_pkl, device=device, latent_space=\"style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af267d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (generator.latent_to_image(generator.sample_latent(1))*127.5 + 128).detach().clamp(0, 255).to(torch.uint8)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "397e19bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_768615/1924709627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"(n1 n2) h w c-> (n1 h) (n2 w) c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularize_cluster_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# clusters.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gan-regularization/projector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gen, device, prior_type, regularize_w_l2, regularize_cluster_weight, num_clusters, cluster_samples, cluster_distance, optimize_cluster_individually)\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/projector.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_mean_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             self.clusters = self.sample_clusters(\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/projector.py\u001b[0m in \u001b[0;36msample_clusters\u001b[0;34m(self, distance, samples, num_clusters)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mlatent_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         cluster_ids_x, cluster_centers = kmeans(\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/kmeans_pytorch/__init__.py\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(X, num_clusters, distance, tol, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mtqdm_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'[running kmeans]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distance_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mchoice_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/kmeans_pytorch/__init__.py\u001b[0m in \u001b[0;36mpairwise_cosine\u001b[0;34m(data1, data2, device)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpairwise_cosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# transfer to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# N*1*M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = (generator.latent_to_image(generator.sample_latent(16))*127.5 + 128).detach().clamp(0, 255).to(torch.uint8)\n",
    "num_rows = 4\n",
    "grid_image = einops.rearrange(samples.permute(0, 2, 3, 1).cpu().numpy(), \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "plt.imsave(\"samples.png\", grid_image)\n",
    "prior = Prior(generator, device=device, prior_type='cluster', regularize_cluster_weight=1, num_clusters=4)# clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f429add",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(plt.imread(\"target2.png\")[:, :, :3]*255).to(torch.uint8).permute(2, 0, 1).to(device)\n",
    "# target.min()\n",
    "# plt.imsave(\"target2.png\", target.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c246617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target.permute(1, 2, 0).cpu().numpy())\n",
    "target.shape\n",
    "\n",
    "# test different samples\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c03562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior = Prior(generator, device=device, prior_type='cluster', regularize_cluster_weight=1, num_clusters=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd11da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using group norm group size = 32.\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# prior = DiffusionPrior(device=device, hidden_h=8960, hidden_dim=10240, num_hidden=5, normalize=True)\n",
    "prior = DiffusionPrior(device=device)\n",
    "prior.load_network(\"/home/oleksiiv/logs/ominous-wraith-125/epoch_2000_ckpt.pth\")\n",
    "# prior.load_network(\"/home/oleksiiv/logs/giddy-mountain-154/most_recent_ckpt.pth\")\n",
    "\n",
    "\n",
    "# prior = Prior(generator, device=device, prior_type='l2', regularize_w_l2=1)\n",
    "\n",
    "# target = torch.cat((torch.zeros(2, 512, 512), 255*torch.ones(1, 512, 512)), 0)\n",
    "# task = Task(device=device, target=target)\n",
    "# task = Task(device=device, task_type = 'clip_text', target_str = 'racecar')\n",
    "\n",
    "mask = torch.tensor(np.concatenate((np.zeros((3, 256, 512)), np.ones((3, 256, 512))), axis =1))\n",
    "plt.imsave(\"mask.png\", mask.permute(1, 2, 0).cpu().numpy())\n",
    "task = Task(device=device, target=target, task_type = \"perceptual_inpainting\", mask = mask) # lr = 10\n",
    "# task = Task(device=device, target=target, task_type = \"inpainting\", mask = mask)\n",
    "projector = Projector(generator, task, prior=prior, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16feeafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(np.concatenate((np.zeros((3, 256, 512)), np.ones((3, 256, 512))), axis =1))\n",
    "plt.imsave(\"mask.png\", mask.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74509d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor(plt.imread(\"mask.png\")[:, :, :3]).to(torch.uint8).permute(2, 0, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3afc519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.min()\n",
    "python project_latent.py --latent_space style --target_fname target2.png --out_dir out12 --prior_type l2 --lr 0.01 --num_img 18 --num_steps 1000 --prior_weight 0.0001 --mask_fname mask.png\n",
    "python project_latent.py --latent_space style --target_fname target2.png --out_dir out12 --prior_type l2 --lr 0.01 --num_img 18 --num_steps 1000 --prior_weight 0.00005 --mask_fname mask.png\n",
    "python project_latent.py --latent_space style --target_fname target2.png --out_dir out12 --prior_type l2 --lr 0.01 --num_img 18 --num_steps 1000 --prior_weight 0.00001 --mask_fname mask.png\n",
    "python project_latent.py --latent_space style --target_fname target2.png --out_dir out12 --prior_type l2 --lr 0.01 --num_img 18 --num_steps 1000 --prior_weight 0.000005 --mask_fname mask.png\n",
    "python project_latent.py --latent_space style --target_fname target2.png --out_dir out12 --prior_type l2 --lr 0.01 --num_img 18 --num_steps 1000 --prior_weight 0.000001 --mask_fname mask.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae96c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|██                                                                                                                                                                                                       | 1/100 [00:00<01:23,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:03<00:00, 31.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image out12/god\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.3\n",
    "num_img = 12\n",
    "\n",
    "init_t = 0.05\n",
    "d = 0\n",
    "mode = \"mini_end\"\n",
    "lamb = 2\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project_batched(\n",
    "    num_img,\n",
    "    6, # batch size -> depends on gpu\n",
    "    learning_rate=lr,\n",
    "    num_steps=100,\n",
    "    # diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    # diffusion_step_size = s,\n",
    "    mini_end_init_t = init_t,\n",
    "    diffusion_magnitude_lambda = lamb,\n",
    "    # optimizer_step = True,\n",
    "    # save_video=True,\n",
    "    # prior_loss_weight = 0,\n",
    ")\n",
    "\n",
    "save_image(projected_w_steps, projector, num_rows=4, outdir=\"out12\", name=\"god\")\n",
    "\n",
    "# z, bpd = prior.ode_likelihood(projected_w_steps.detach()[:, :, None, :], eps=1e-6)\n",
    "# idxs = np.argsort(bpd.cpu().numpy())\n",
    "# ll_w_steps = projected_w_steps[idxs]\n",
    "# save_image(ll_w_steps, projector, num_rows=5, outdir=\"out10\", name=\"cluster_prior1\")\n",
    "\n",
    "# create_video(projected_w_steps, projector, num_rows=4, outdir=\"out13\", name=f\"lol.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34997d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:49<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimization progress video out13/lol2.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.3\n",
    "num_img = 6\n",
    "\n",
    "init_t = 0.05\n",
    "d = 20\n",
    "mode = \"mini_end\"\n",
    "lamb = 2\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=num_img,\n",
    "    learning_rate=lr,\n",
    "    num_steps=200,\n",
    "    # diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    # diffusion_step_size = s,\n",
    "    mini_end_init_t = init_t,\n",
    "    diffusion_magnitude_lambda = lamb,\n",
    "    optimizer_step = True,\n",
    "    save_video=True,\n",
    "    # prior_loss_weight = 0,\n",
    ")\n",
    "\n",
    "# save_image(projected_w_steps, projector, num_rows=4, outdir=\"out12\", name=\"god\")\n",
    "\n",
    "# z, bpd = prior.ode_likelihood(projected_w_steps.detach()[:, :, None, :], eps=1e-6)\n",
    "# idxs = np.argsort(bpd.cpu().numpy())\n",
    "# ll_w_steps = projected_w_steps[idxs]\n",
    "# save_image(ll_w_steps, projector, num_rows=5, outdir=\"out10\", name=\"cluster_prior1\")\n",
    "\n",
    "create_video(projected_w_steps, projector, num_rows=3, outdir=\"out13\", name=f\"lol2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_t = 0.05\n",
    "lr = 0.01\n",
    "d = 10\n",
    "mode = \"mini_end\"\n",
    "num_img = 30\n",
    "lamb = 2\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project_batched(\n",
    "    num_img,\n",
    "    6, # batch size -> depends on gpu\n",
    "    learning_rate=lr,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    # diffusion_step_size = s,\n",
    "    mini_end_init_t = init_t,\n",
    "    diffusion_magnitude_lambda = lamb,\n",
    "    # optimizer_step = True,\n",
    "    # prior_loss_weight = 0.01,\n",
    ")\n",
    "\n",
    "save_image(projected_w_steps, projector, num_rows=5, outdir=\"out10\", name=\"diff_prior2\")\n",
    "\n",
    "# z, bpd = prior.ode_likelihood(projected_w_steps.detach()[:, :, None, :], eps=1e-6)\n",
    "# idxs = np.argsort(bpd.cpu().numpy())\n",
    "# ll_w_steps = projected_w_steps[idxs]\n",
    "# save_image(ll_w_steps, projector, num_rows=5, outdir=\"out10\", name=\"cluster_prior1\")\n",
    "\n",
    "# create_video(projected_w_steps, projector, num_rows=2, outdir=\"out6\", name=f\"diff11-initt{init_t}-lr{lr}-mode{mode}-innersteps{d}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9abde6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/oleksiiv/gan-regularization/training/dataset.py\", line 88, in __getitem__\n    image = self._load_raw_image(self._raw_idx[idx])\n  File \"/home/oleksiiv/gan-regularization/training/dataset.py\", line 231, in _load_raw_image\n    image = np.array(PIL.Image.open(f))\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/Image.py\", line 698, in __array__\n    new[\"data\"] = self.tobytes()\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/Image.py\", line 744, in tobytes\n    self.load()\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/ImageFile.py\", line 237, in load\n    s = read(self.decodermaxblock)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/PngImagePlugin.py\", line 894, in load_read\n    self.fp.read(4)  # CRC\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 922, in read\n    data = self._read1(n)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 1012, in _read1\n    self._update_crc(data)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 940, in _update_crc\n    raise BadZipFile(\"Bad CRC-32 for file %r\" % self.name)\nzipfile.BadZipFile: Bad CRC-32 for file '00000/img00000017.png'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2332729/3750812373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'training.dataset.ImageFolderDataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'./lsuncar200k.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'resolution'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use_labels'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xflip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_opts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetricOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_kid_projector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gan-regularization/metrics/kernel_inception_distance.py\u001b[0m in \u001b[0;36mcompute_kid_projector\u001b[0;34m(data_opts, projector_opts, max_real, num_gen, num_subsets, max_subset_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     real_features = metric_utils.compute_feature_stats_for_dataset(\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_opts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdetector_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetector_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/metrics/metric_utils.py\u001b[0m in \u001b[0;36mcompute_feature_stats_for_dataset\u001b[0;34m(opts, detector_url, detector_kwargs, rel_lo, rel_hi, batch_size, data_loader_kwargs, max_items, **stats_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     ]\n\u001b[0;32m--> 330\u001b[0;31m     for images, _labels in torch.utils.data.DataLoader(\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: Caught BadZipFile in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/oleksiiv/gan-regularization/training/dataset.py\", line 88, in __getitem__\n    image = self._load_raw_image(self._raw_idx[idx])\n  File \"/home/oleksiiv/gan-regularization/training/dataset.py\", line 231, in _load_raw_image\n    image = np.array(PIL.Image.open(f))\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/Image.py\", line 698, in __array__\n    new[\"data\"] = self.tobytes()\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/Image.py\", line 744, in tobytes\n    self.load()\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/ImageFile.py\", line 237, in load\n    s = read(self.decodermaxblock)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/PIL/PngImagePlugin.py\", line 894, in load_read\n    self.fp.read(4)  # CRC\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 922, in read\n    data = self._read1(n)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 1012, in _read1\n    self._update_crc(data)\n  File \"/home/oleksiiv/anaconda3/envs/ganenvem1/lib/python3.9/zipfile.py\", line 940, in _update_crc\n    raise BadZipFile(\"Bad CRC-32 for file %r\" % self.name)\nzipfile.BadZipFile: Bad CRC-32 for file '00000/img00000017.png'\n"
     ]
    }
   ],
   "source": [
    "proj_opts = MetricOptionsProjector(projector=projector, device=device)\n",
    "dataset_kwargs = {'class_name':'training.dataset.ImageFolderDataset','path':'./lsuncar200k.zip','resolution': 512, 'use_labels':False, 'xflip':False, 'max_size':None}\n",
    "data_opts = MetricOptions(G=generator.G, dataset_kwargs=dataset_kwargs, device=device)\n",
    "kid = compute_kid_projector(data_opts, proj_opts, None, 5000, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53c023cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81f05436a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6bc00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 2\n",
    "clusters = (generator.latent_to_image(prior.get_clusters())*127.5 + 128).detach().clamp(0, 255).to(torch.uint8)\n",
    "grid_image = einops.rearrange(clusters.permute(0, 2, 3, 1).cpu().numpy(), \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "plt.imsave(\"clusters2.png\", grid_image)\n",
    "# clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45baf340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:39<00:00, 31.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image out10/cluster_prior57\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a7c8c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.21it/s]\n",
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                              | 3/5 [01:33<01:02, 31.19s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "init_t = 0.05\n",
    "lr = 0.01\n",
    "d = 10\n",
    "mode = \"mini_end\"\n",
    "num_img = 30\n",
    "lamb = 2\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project_batched(\n",
    "    num_img,\n",
    "    6, # batch size -> depends on gpu\n",
    "    learning_rate=lr,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    # diffusion_step_size = s,\n",
    "    mini_end_init_t = init_t,\n",
    "    diffusion_magnitude_lambda = lamb,\n",
    "    # optimizer_step = True,\n",
    "    # prior_loss_weight = 0,\n",
    ")\n",
    "\n",
    "# save_image(projected_w_steps, projector, num_rows=5, outdir=\"out7\", name=\"diff_prior3\")\n",
    "\n",
    "z, bpd = prior.ode_likelihood(projected_w_steps.detach()[:, :, None, :], eps=1e-6)\n",
    "idxs = np.argsort(bpd.cpu().numpy())\n",
    "ll_w_steps = projected_w_steps[idxs]\n",
    "save_image(ll_w_steps, projector, num_rows=5, outdir=\"out9\", name=\"diff_prior2\")\n",
    "\n",
    "# create_video(projected_w_steps, projector, num_rows=2, outdir=\"out6\", name=f\"diff11-initt{init_t}-lr{lr}-mode{mode}-innersteps{d}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_t = 0.05\n",
    "lr = 0.001\n",
    "d = 10\n",
    "mode = \"mini_end\"\n",
    "num_img = 30\n",
    "lamb = 2\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project_batched(\n",
    "    num_img,\n",
    "    6, # batch size -> depends on gpu\n",
    "    learning_rate=lr,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    # diffusion_step_size = s,\n",
    "    mini_end_init_t = init_t,\n",
    "    diffusion_magnitude_lambda = lamb,\n",
    "    # optimizer_step = True,\n",
    "    # prior_loss_weight = 0,\n",
    ")\n",
    "\n",
    "# save_image(projected_w_steps, projector, num_rows=5, outdir=\"out7\", name=\"diff_prior3\")\n",
    "\n",
    "z, bpd = prior.ode_likelihood(projected_w_steps.detach()[:, :, None, :], eps=1e-6)\n",
    "idxs = np.argsort(bpd.cpu().numpy())\n",
    "ll_w_steps = projected_w_steps[idxs]\n",
    "save_image(ll_w_steps, projector, num_rows=5, outdir=\"out9\", name=\"diff_prior3\")\n",
    "\n",
    "# create_video(projected_w_steps, projector, num_rows=2, outdir=\"out6\", name=f\"diff11-initt{init_t}-lr{lr}-mode{mode}-innersteps{d}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edaede72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████████████████████████████████▍                                              | 51/100 [00:17<00:16,  2.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1562334/2747483427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m projected_w_steps = projector.project(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnum_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/projector.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(self, learning_rate, num_images, num_steps, diffusion_time_schedule, mini_end_init_t, num_diffusion_steps_per_step, diffusion_step_size, optimizer_step, diffusion_magnitude_lambda)\u001b[0m\n\u001b[1;32m    443\u001b[0m                                 )\n\u001b[1;32m    444\u001b[0m                             \u001b[0;32melif\u001b[0m \u001b[0mdiffusion_time_schedule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                                 new_latent = self.prior.step(\n\u001b[0m\u001b[1;32m    446\u001b[0m                                     \u001b[0mopt_latent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                                     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiffusion_step_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/projector.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, x, t, snr, eps, step_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mbatch_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Corrector step (Langevin MCMC)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_time_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mnoise_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gan-regularization/score.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_abs_string_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m_get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;34m\"\"\"Get the absolute index for the list of modules\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index {} is out of range'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ganenvem1/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0m_copy_to_script_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_t = 0.01\n",
    "s = 0.01\n",
    "lr = 0.1\n",
    "d = 10\n",
    "mode = \"constant\"\n",
    "num_img = 6\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    learning_rate=lr,\n",
    "    num_images=num_img,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=mode,\n",
    "    num_diffusion_steps_per_step = d,\n",
    "    diffusion_step_size = s,\n",
    "    # mini_end_init_t = init_t,\n",
    ")\n",
    "\n",
    "create_video(projected_w_steps, projector, num_rows=2, outdir=\"out5\", name=f\"diff12-initt{init_t}-lr{lr}-mode{mode}-innersteps{d}.mp4\")\n",
    "del(projected_w_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f853cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "step    1/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.32 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.91 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.13\n",
      "step    8/100: prior_loss 0.00  task_loss 9.69 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.13\n",
      "step   11/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.17\n",
      "step   18/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.59\n",
      "step   26/100: prior_loss 0.00  task_loss 10.60\n",
      "step   27/100: prior_loss 0.00  task_loss 10.35\n",
      "step   28/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   42/100: prior_loss 0.00  task_loss 10.01\n",
      "step   43/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   68/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   69/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.53 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.32 \n",
      "Elapsed: 65.4 s\n",
      "Saving optimization progress video out/diff-0.05-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.51 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.71 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.79 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.72\n",
      "step    6/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.67 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.88 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.19\n",
      "step   16/100: prior_loss 0.00  task_loss 10.03\n",
      "step   17/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.15\n",
      "step   21/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.17\n",
      "step   27/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.24\n",
      "step   34/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.16\n",
      "step   36/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.35\n",
      "step   39/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.16\n",
      "step   45/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.88 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.03 \n",
      "Elapsed: 171.4 s\n",
      "Saving optimization progress video out/diff-0.05-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    3/100: prior_loss 0.00  task_loss 10.48\n",
      "step    4/100: prior_loss 0.00  task_loss 9.95 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.03\n",
      "step    6/100: prior_loss 0.00  task_loss 9.87 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.39 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.12\n",
      "step   13/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.10\n",
      "step   15/100: prior_loss 0.00  task_loss 10.11\n",
      "step   16/100: prior_loss 0.00  task_loss 10.37\n",
      "step   17/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.01\n",
      "step   20/100: prior_loss 0.00  task_loss 10.64\n",
      "step   21/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.39\n",
      "step   24/100: prior_loss 0.00  task_loss 10.34\n",
      "step   25/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.07\n",
      "step   27/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.17\n",
      "step   30/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.10\n",
      "step   33/100: prior_loss 0.00  task_loss 10.08\n",
      "step   34/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.23\n",
      "step   36/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.00\n",
      "step   41/100: prior_loss 0.00  task_loss 10.00\n",
      "step   42/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.25\n",
      "step   47/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   50/100: prior_loss 0.00  task_loss 10.01\n",
      "step   51/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.10 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.84 \n",
      "Elapsed: 322.5 s\n",
      "Saving optimization progress video out/diff-0.05-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.87 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.03\n",
      "step    7/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.51 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.00\n",
      "step   15/100: prior_loss 0.00  task_loss 10.29\n",
      "step   16/100: prior_loss 0.00  task_loss 10.39\n",
      "step   17/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.33\n",
      "step   19/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.43\n",
      "step   23/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.14\n",
      "step   30/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.05\n",
      "step   35/100: prior_loss 0.00  task_loss 10.12\n",
      "step   36/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.56\n",
      "step   38/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   42/100: prior_loss 0.00  task_loss 10.24\n",
      "step   43/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.28\n",
      "step   46/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.84 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.16 \n",
      "Elapsed: 945.9 s\n",
      "Saving optimization progress video out/diff-0.05-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.77 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.67 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.52 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.16 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.12 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.97 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   34/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   35/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   36/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   37/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   38/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   39/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   40/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   41/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   42/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   43/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   44/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   45/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   46/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   47/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   48/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   49/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   50/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   51/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   52/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   53/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   54/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   55/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   56/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   57/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   58/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   59/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   60/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   61/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   62/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   63/100: prior_loss 0.00  task_loss 7.48 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   66/100: prior_loss 0.00  task_loss 7.36 \n",
      "step   67/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   68/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   69/100: prior_loss 0.00  task_loss 7.34 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.34 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.27 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.29 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.22 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.22 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.12 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.26 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.26 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.32 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.35 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.35 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.73 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.92 \n",
      "Elapsed: 1053.9 s\n",
      "Saving optimization progress video out/diff-0.05-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.65 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.53 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.57 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.51 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.64 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.70 \n",
      "Elapsed: 1156.5 s\n",
      "Saving optimization progress video out/diff-0.05-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.77 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.88 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   48/100: prior_loss 0.00  task_loss 10.02\n",
      "step   49/100: prior_loss 0.00  task_loss 10.06\n",
      "step   50/100: prior_loss 0.00  task_loss 10.09\n",
      "step   51/100: prior_loss 0.00  task_loss 10.11\n",
      "step   52/100: prior_loss 0.00  task_loss 10.11\n",
      "step   53/100: prior_loss 0.00  task_loss 10.12\n",
      "step   54/100: prior_loss 0.00  task_loss 10.15\n",
      "step   55/100: prior_loss 0.00  task_loss 10.15\n",
      "step   56/100: prior_loss 0.00  task_loss 10.17\n",
      "step   57/100: prior_loss 0.00  task_loss 10.18\n",
      "step   58/100: prior_loss 0.00  task_loss 10.18\n",
      "step   59/100: prior_loss 0.00  task_loss 10.19\n",
      "step   60/100: prior_loss 0.00  task_loss 10.17\n",
      "step   61/100: prior_loss 0.00  task_loss 10.19\n",
      "step   62/100: prior_loss 0.00  task_loss 10.20\n",
      "step   63/100: prior_loss 0.00  task_loss 10.19\n",
      "step   64/100: prior_loss 0.00  task_loss 10.18\n",
      "step   65/100: prior_loss 0.00  task_loss 10.20\n",
      "step   66/100: prior_loss 0.00  task_loss 10.20\n",
      "step   67/100: prior_loss 0.00  task_loss 10.17\n",
      "step   68/100: prior_loss 0.00  task_loss 10.19\n",
      "step   69/100: prior_loss 0.00  task_loss 10.19\n",
      "step   70/100: prior_loss 0.00  task_loss 10.20\n",
      "step   71/100: prior_loss 0.00  task_loss 10.21\n",
      "step   72/100: prior_loss 0.00  task_loss 10.20\n",
      "step   73/100: prior_loss 0.00  task_loss 10.20\n",
      "step   74/100: prior_loss 0.00  task_loss 10.20\n",
      "step   75/100: prior_loss 0.00  task_loss 10.22\n",
      "step   76/100: prior_loss 0.00  task_loss 10.19\n",
      "step   77/100: prior_loss 0.00  task_loss 10.20\n",
      "step   78/100: prior_loss 0.00  task_loss 10.21\n",
      "step   79/100: prior_loss 0.00  task_loss 10.22\n",
      "step   80/100: prior_loss 0.00  task_loss 10.21\n",
      "step   81/100: prior_loss 0.00  task_loss 10.20\n",
      "step   82/100: prior_loss 0.00  task_loss 10.22\n",
      "step   83/100: prior_loss 0.00  task_loss 10.21\n",
      "step   84/100: prior_loss 0.00  task_loss 10.19\n",
      "step   85/100: prior_loss 0.00  task_loss 10.21\n",
      "step   86/100: prior_loss 0.00  task_loss 10.21\n",
      "step   87/100: prior_loss 0.00  task_loss 10.20\n",
      "step   88/100: prior_loss 0.00  task_loss 10.20\n",
      "step   89/100: prior_loss 0.00  task_loss 10.19\n",
      "step   90/100: prior_loss 0.00  task_loss 10.22\n",
      "step   91/100: prior_loss 0.00  task_loss 10.21\n",
      "step   92/100: prior_loss 0.00  task_loss 10.20\n",
      "step   93/100: prior_loss 0.00  task_loss 10.21\n",
      "step   94/100: prior_loss 0.00  task_loss 10.19\n",
      "step   95/100: prior_loss 0.00  task_loss 10.18\n",
      "step   96/100: prior_loss 0.00  task_loss 10.21\n",
      "step   97/100: prior_loss 0.00  task_loss 10.20\n",
      "step   98/100: prior_loss 0.00  task_loss 10.21\n",
      "step   99/100: prior_loss 0.00  task_loss 10.18\n",
      "step  100/100: prior_loss 0.00  task_loss 10.19\n",
      "Elapsed: 1308.6 s\n",
      "Saving optimization progress video out/diff-0.05-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.43 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.07 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.95 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.93 \n",
      "Elapsed: 1941.1 s\n",
      "Saving optimization progress video out/diff-0.05-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.79 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.20\n",
      "step    5/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.46 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.18 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.10\n",
      "step   13/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.33\n",
      "step   15/100: prior_loss 0.00  task_loss 10.02\n",
      "step   16/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.05\n",
      "step   21/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.67\n",
      "step   25/100: prior_loss 0.00  task_loss 10.03\n",
      "step   26/100: prior_loss 0.00  task_loss 10.01\n",
      "step   27/100: prior_loss 0.00  task_loss 10.31\n",
      "step   28/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.24\n",
      "step   34/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   39/100: prior_loss 0.00  task_loss 10.19\n",
      "step   40/100: prior_loss 0.00  task_loss 10.30\n",
      "step   41/100: prior_loss 0.00  task_loss 10.41\n",
      "step   42/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.15\n",
      "step   44/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.01\n",
      "step   46/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   47/100: prior_loss 0.00  task_loss 10.04\n",
      "step   48/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   58/100: prior_loss 0.00  task_loss 10.13\n",
      "step   59/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   65/100: prior_loss 0.00  task_loss 10.53\n",
      "step   66/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   70/100: prior_loss 0.00  task_loss 10.61\n",
      "step   71/100: prior_loss 0.00  task_loss 10.27\n",
      "step   72/100: prior_loss 0.00  task_loss 10.32\n",
      "step   73/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.16\n",
      "step   75/100: prior_loss 0.00  task_loss 10.68\n",
      "step   76/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.41\n",
      "step   81/100: prior_loss 0.00  task_loss 10.11\n",
      "step   82/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   85/100: prior_loss 0.00  task_loss 10.08\n",
      "step   86/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   87/100: prior_loss 0.00  task_loss 10.18\n",
      "step   88/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   90/100: prior_loss 0.00  task_loss 10.15\n",
      "step   91/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.67 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.06 \n",
      "Elapsed: 2039.8 s\n",
      "Saving optimization progress video out/diff-0.05-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.94 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.33 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.29 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.20 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.22 \n",
      "Elapsed: 2155.2 s\n",
      "Saving optimization progress video out/diff-0.05-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.61 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.03 \n",
      "Elapsed: 2315.1 s\n",
      "Saving optimization progress video out/diff-0.05-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.29 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.31\n",
      "step   87/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.99 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.45 \n",
      "Elapsed: 2949.8 s\n",
      "Saving optimization progress video out/diff-0.05-mini-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.43 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.29 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.20 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.67 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.50 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.33 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.32 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   34/100: prior_loss 0.00  task_loss 7.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 7.05 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.89 \n",
      "step   37/100: prior_loss 0.00  task_loss 7.15 \n",
      "step   38/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   39/100: prior_loss 0.00  task_loss 7.35 \n",
      "step   40/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   41/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   42/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   43/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   44/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   45/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   46/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   47/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   48/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   49/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   50/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   51/100: prior_loss 0.00  task_loss 6.91 \n",
      "step   52/100: prior_loss 0.00  task_loss 6.95 \n",
      "step   53/100: prior_loss 0.00  task_loss 6.92 \n",
      "step   54/100: prior_loss 0.00  task_loss 7.10 \n",
      "step   55/100: prior_loss 0.00  task_loss 6.95 \n",
      "step   56/100: prior_loss 0.00  task_loss 6.81 \n",
      "step   57/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   58/100: prior_loss 0.00  task_loss 6.76 \n",
      "step   59/100: prior_loss 0.00  task_loss 6.74 \n",
      "step   60/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   61/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   62/100: prior_loss 0.00  task_loss 6.66 \n",
      "step   63/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   64/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   65/100: prior_loss 0.00  task_loss 6.58 \n",
      "step   66/100: prior_loss 0.00  task_loss 6.60 \n",
      "step   67/100: prior_loss 0.00  task_loss 6.47 \n",
      "step   68/100: prior_loss 0.00  task_loss 6.52 \n",
      "step   69/100: prior_loss 0.00  task_loss 6.46 \n",
      "step   70/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   71/100: prior_loss 0.00  task_loss 6.39 \n",
      "step   72/100: prior_loss 0.00  task_loss 6.33 \n",
      "step   73/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   74/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   75/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   76/100: prior_loss 0.00  task_loss 6.07 \n",
      "step   77/100: prior_loss 0.00  task_loss 6.14 \n",
      "step   78/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   79/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   80/100: prior_loss 0.00  task_loss 6.08 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 5.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 6.12 \n",
      "step   85/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   86/100: prior_loss 0.00  task_loss 6.02 \n",
      "step   87/100: prior_loss 0.00  task_loss 6.02 \n",
      "step   88/100: prior_loss 0.00  task_loss 6.07 \n",
      "step   89/100: prior_loss 0.00  task_loss 6.21 \n",
      "step   90/100: prior_loss 0.00  task_loss 6.32 \n",
      "step   91/100: prior_loss 0.00  task_loss 6.53 \n",
      "step   92/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   93/100: prior_loss 0.00  task_loss 6.93 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.26 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.67 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.14 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.30 \n",
      "Elapsed: 3055.2 s\n",
      "Saving optimization progress video out/diff-0.01-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.68 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.29 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.74 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.30 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.69 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.22 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.04\n",
      "step    8/100: prior_loss 0.00  task_loss 9.65 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.09\n",
      "step   11/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.49\n",
      "step   15/100: prior_loss 0.00  task_loss 10.08\n",
      "step   16/100: prior_loss 0.00  task_loss 10.31\n",
      "step   17/100: prior_loss 0.00  task_loss 10.36\n",
      "step   18/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.26\n",
      "step   22/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.51\n",
      "step   24/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.39\n",
      "step   30/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.30\n",
      "step   32/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.53\n",
      "step   36/100: prior_loss 0.00  task_loss 10.12\n",
      "step   37/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.17\n",
      "step   42/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.23\n",
      "step   47/100: prior_loss 0.00  task_loss 10.05\n",
      "step   48/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.72 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.91 \n",
      "Elapsed: 3156.8 s\n",
      "Saving optimization progress video out/diff-0.01-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.14 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.58 \n",
      "step    3/100: prior_loss 0.00  task_loss 10.42\n",
      "step    4/100: prior_loss 0.00  task_loss 9.76 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.89 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.15\n",
      "step    7/100: prior_loss 0.00  task_loss 9.59 \n",
      "step    8/100: prior_loss 0.00  task_loss 10.35\n",
      "step    9/100: prior_loss 0.00  task_loss 10.13\n",
      "step   10/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.25\n",
      "step   18/100: prior_loss 0.00  task_loss 10.27\n",
      "step   19/100: prior_loss 0.00  task_loss 10.23\n",
      "step   20/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.08\n",
      "step   25/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.21\n",
      "step   27/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.24\n",
      "step   29/100: prior_loss 0.00  task_loss 10.55\n",
      "step   30/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.50\n",
      "step   32/100: prior_loss 0.00  task_loss 10.29\n",
      "step   33/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.18\n",
      "step   36/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.07\n",
      "step   38/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.17\n",
      "step   47/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.22 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.26 \n",
      "Elapsed: 3315.9 s\n",
      "Saving optimization progress video out/diff-0.01-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 11.04\n",
      "step    2/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.41 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.07\n",
      "step    5/100: prior_loss 0.00  task_loss 10.15\n",
      "step    6/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.08\n",
      "step    8/100: prior_loss 0.00  task_loss 9.91 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   11/100: prior_loss 0.00  task_loss 10.25\n",
      "step   12/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.09\n",
      "step   15/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.63\n",
      "step   18/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.30\n",
      "step   21/100: prior_loss 0.00  task_loss 10.08\n",
      "step   22/100: prior_loss 0.00  task_loss 10.00\n",
      "step   23/100: prior_loss 0.00  task_loss 10.31\n",
      "step   24/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.04\n",
      "step   32/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.23\n",
      "step   38/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.10 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.47 \n",
      "Elapsed: 3954.4 s\n",
      "Saving optimization progress video out/diff-0.01-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.74 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.57 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.35 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.07 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.82 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.67 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.33 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.19 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.12 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.06 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.02 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.98 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.92 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.87 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.61 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.53 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.49 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.44 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.34 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.26 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.23 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.08 \n",
      "step   38/100: prior_loss 0.00  task_loss 6.02 \n",
      "step   39/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.92 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.87 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.83 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.80 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.75 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.58 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.54 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   91/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   92/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   93/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   94/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   95/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   96/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   97/100: prior_loss 0.00  task_loss 5.71 \n",
      "step   98/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.93 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.98 \n",
      "Elapsed: 4062.8 s\n",
      "Saving optimization progress video out/diff-0.01-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.70 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.65 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   62/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   63/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   66/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   67/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   68/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   69/100: prior_loss 0.00  task_loss 7.97 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.16 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.34 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.35 \n",
      "Elapsed: 4164.7 s\n",
      "Saving optimization progress video out/diff-0.01-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.67 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.54 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.16 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.12 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.12 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.55 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.54 \n",
      "Elapsed: 4315.1 s\n",
      "Saving optimization progress video out/diff-0.01-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.98 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.76 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.66 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.60 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.67 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.67 \n",
      "Elapsed: 4948.3 s\n",
      "Saving optimization progress video out/diff-0.01-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.21 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.26 \n",
      "step    3/100: prior_loss 0.00  task_loss 7.96 \n",
      "step    4/100: prior_loss 0.00  task_loss 7.90 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.96 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.94 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.92 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.87 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.13\n",
      "step   42/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   45/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.07\n",
      "step   52/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   58/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   68/100: prior_loss 0.00  task_loss 10.03\n",
      "step   69/100: prior_loss 0.00  task_loss 10.01\n",
      "step   70/100: prior_loss 0.00  task_loss 10.07\n",
      "step   71/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.45\n",
      "step   75/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   76/100: prior_loss 0.00  task_loss 10.34\n",
      "step   77/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   78/100: prior_loss 0.00  task_loss 10.02\n",
      "step   79/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   84/100: prior_loss 0.00  task_loss 10.13\n",
      "step   85/100: prior_loss 0.00  task_loss 10.41\n",
      "step   86/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   90/100: prior_loss 0.00  task_loss 10.05\n",
      "step   91/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.56 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.67 \n",
      "Elapsed: 5048.9 s\n",
      "Saving optimization progress video out/diff-0.01-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.05\n",
      "step    3/100: prior_loss 0.00  task_loss 9.57 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.22\n",
      "step    5/100: prior_loss 0.00  task_loss 9.78 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.91 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.61 \n",
      "step    8/100: prior_loss 0.00  task_loss 10.00\n",
      "step    9/100: prior_loss 0.00  task_loss 10.18\n",
      "step   10/100: prior_loss 0.00  task_loss 10.19\n",
      "step   11/100: prior_loss 0.00  task_loss 10.06\n",
      "step   12/100: prior_loss 0.00  task_loss 10.45\n",
      "step   13/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.33\n",
      "step   16/100: prior_loss 0.00  task_loss 10.50\n",
      "step   17/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.18\n",
      "step   23/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.43\n",
      "step   28/100: prior_loss 0.00  task_loss 10.52\n",
      "step   29/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.02\n",
      "step   34/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.72\n",
      "step   37/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.38\n",
      "step   44/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.08\n",
      "step   47/100: prior_loss 0.00  task_loss 10.65\n",
      "step   48/100: prior_loss 0.00  task_loss 10.35\n",
      "step   49/100: prior_loss 0.00  task_loss 10.30\n",
      "step   50/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   58/100: prior_loss 0.00  task_loss 10.11\n",
      "step   59/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   62/100: prior_loss 0.00  task_loss 10.16\n",
      "step   63/100: prior_loss 0.00  task_loss 10.07\n",
      "step   64/100: prior_loss 0.00  task_loss 10.44\n",
      "step   65/100: prior_loss 0.00  task_loss 10.06\n",
      "step   66/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   67/100: prior_loss 0.00  task_loss 10.24\n",
      "step   68/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.21\n",
      "step   75/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   79/100: prior_loss 0.00  task_loss 10.62\n",
      "step   80/100: prior_loss 0.00  task_loss 10.33\n",
      "step   81/100: prior_loss 0.00  task_loss 10.04\n",
      "step   82/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   83/100: prior_loss 0.00  task_loss 10.27\n",
      "step   84/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.03\n",
      "step   87/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   88/100: prior_loss 0.00  task_loss 10.21\n",
      "step   89/100: prior_loss 0.00  task_loss 10.23\n",
      "step   90/100: prior_loss 0.00  task_loss 10.23\n",
      "step   91/100: prior_loss 0.00  task_loss 10.44\n",
      "step   92/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   96/100: prior_loss 0.00  task_loss 10.08\n",
      "step   97/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   98/100: prior_loss 0.00  task_loss 10.13\n",
      "step   99/100: prior_loss 0.00  task_loss 10.02\n",
      "step  100/100: prior_loss 0.00  task_loss 10.28\n",
      "Elapsed: 5161.4 s\n",
      "Saving optimization progress video out/diff-0.01-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.10 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.98 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.08 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.15 \n",
      "Elapsed: 5323.3 s\n",
      "Saving optimization progress video out/diff-0.01-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.90 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.04 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.81 \n",
      "Elapsed: 5959.3 s\n",
      "Saving optimization progress video out/diff-0.01-mini-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.24\n",
      "step    2/100: prior_loss 0.00  task_loss 10.30\n",
      "step    3/100: prior_loss 0.00  task_loss 10.34\n",
      "step    4/100: prior_loss 0.00  task_loss 10.28\n",
      "step    5/100: prior_loss 0.00  task_loss 10.30\n",
      "step    6/100: prior_loss 0.00  task_loss 10.02\n",
      "step    7/100: prior_loss 0.00  task_loss 9.83 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.58 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.27 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.98 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.62 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.41 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.14 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.89 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.88 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.63 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.40 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.34 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.53 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.46 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.26 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.22 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.21 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.22 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.09 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.49 \n",
      "Elapsed: 6065.4 s\n",
      "Saving optimization progress video out/diff-0.005-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.17\n",
      "step    3/100: prior_loss 0.00  task_loss 9.60 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.85 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.76 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.46 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.14\n",
      "step    8/100: prior_loss 0.00  task_loss 9.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.65\n",
      "step   11/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 10.21\n",
      "step   17/100: prior_loss 0.00  task_loss 10.02\n",
      "step   18/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.02\n",
      "step   21/100: prior_loss 0.00  task_loss 10.05\n",
      "step   22/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.08\n",
      "step   26/100: prior_loss 0.00  task_loss 10.23\n",
      "step   27/100: prior_loss 0.00  task_loss 10.18\n",
      "step   28/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.07\n",
      "step   34/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.13\n",
      "step   36/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.65 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.70 \n",
      "Elapsed: 6164.2 s\n",
      "Saving optimization progress video out/diff-0.005-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.38 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.79 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.34 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.50 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.47 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.29\n",
      "step    8/100: prior_loss 0.00  task_loss 10.57\n",
      "step    9/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.03\n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   16/100: prior_loss 0.00  task_loss 10.57\n",
      "step   17/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.20\n",
      "step   22/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.82\n",
      "step   24/100: prior_loss 0.00  task_loss 10.22\n",
      "step   25/100: prior_loss 0.00  task_loss 10.29\n",
      "step   26/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.20\n",
      "step   28/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.02 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.09 \n",
      "Elapsed: 6318.0 s\n",
      "Saving optimization progress video out/diff-0.005-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.08\n",
      "step    2/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.56 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.67 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.78 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.31 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.51\n",
      "step   13/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.45\n",
      "step   15/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.08\n",
      "step   22/100: prior_loss 0.00  task_loss 10.00\n",
      "step   23/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.16\n",
      "step   25/100: prior_loss 0.00  task_loss 10.12\n",
      "step   26/100: prior_loss 0.00  task_loss 10.37\n",
      "step   27/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.27\n",
      "step   29/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.53\n",
      "step   32/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.29\n",
      "step   34/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.39\n",
      "step   37/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.31\n",
      "step   45/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   56/100: prior_loss 0.00  task_loss 10.05\n",
      "step   57/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.11 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.13 \n",
      "Elapsed: 6936.4 s\n",
      "Saving optimization progress video out/diff-0.005-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.41 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.06 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.69 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.43 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.97 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.80 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.31 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.03 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.77 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.44 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.00 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.81 \n",
      "step   64/100: prior_loss 0.00  task_loss 3.81 \n",
      "step   65/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   66/100: prior_loss 0.00  task_loss 3.75 \n",
      "step   67/100: prior_loss 0.00  task_loss 3.72 \n",
      "step   68/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   69/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   70/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   71/100: prior_loss 0.00  task_loss 3.61 \n",
      "step   72/100: prior_loss 0.00  task_loss 3.58 \n",
      "step   73/100: prior_loss 0.00  task_loss 3.57 \n",
      "step   74/100: prior_loss 0.00  task_loss 3.51 \n",
      "step   75/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   76/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   77/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   78/100: prior_loss 0.00  task_loss 3.50 \n",
      "step   79/100: prior_loss 0.00  task_loss 3.50 \n",
      "step   80/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   81/100: prior_loss 0.00  task_loss 3.42 \n",
      "step   82/100: prior_loss 0.00  task_loss 3.43 \n",
      "step   83/100: prior_loss 0.00  task_loss 3.43 \n",
      "step   84/100: prior_loss 0.00  task_loss 3.41 \n",
      "step   85/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   86/100: prior_loss 0.00  task_loss 3.43 \n",
      "step   87/100: prior_loss 0.00  task_loss 3.49 \n",
      "step   88/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   89/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   90/100: prior_loss 0.00  task_loss 3.74 \n",
      "step   91/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   92/100: prior_loss 0.00  task_loss 3.99 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.13 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.28 \n",
      "Elapsed: 7041.8 s\n",
      "Saving optimization progress video out/diff-0.005-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.61 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.49 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.43 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.16 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   34/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   35/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   36/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   37/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   38/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   39/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   40/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   41/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   42/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   43/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   44/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   45/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   46/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   47/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   48/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   49/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   50/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   51/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   52/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   53/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   54/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   55/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   56/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   57/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   58/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   59/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   60/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   61/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   62/100: prior_loss 0.00  task_loss 7.67 \n",
      "step   63/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   66/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   67/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   68/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   69/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.94 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.01 \n",
      "Elapsed: 7141.8 s\n",
      "Saving optimization progress video out/diff-0.005-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.90 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.63 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.67 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.66 \n",
      "Elapsed: 7284.9 s\n",
      "Saving optimization progress video out/diff-0.005-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.21 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.76 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.64 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.67 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.26 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.27 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.26 \n",
      "Elapsed: 7866.8 s\n",
      "Saving optimization progress video out/diff-0.005-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.61 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.39 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.47 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.60 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.34 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   57/100: prior_loss 0.00  task_loss 10.23\n",
      "step   58/100: prior_loss 0.00  task_loss 10.02\n",
      "step   59/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   62/100: prior_loss 0.00  task_loss 10.15\n",
      "step   63/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   64/100: prior_loss 0.00  task_loss 10.37\n",
      "step   65/100: prior_loss 0.00  task_loss 10.43\n",
      "step   66/100: prior_loss 0.00  task_loss 10.19\n",
      "step   67/100: prior_loss 0.00  task_loss 10.02\n",
      "step   68/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   99/100: prior_loss 0.00  task_loss 10.59\n",
      "step  100/100: prior_loss 0.00  task_loss 10.03\n",
      "Elapsed: 7965.4 s\n",
      "Saving optimization progress video out/diff-0.005-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.55 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.33 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.75 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.27\n",
      "step    7/100: prior_loss 0.00  task_loss 10.71\n",
      "step    8/100: prior_loss 0.00  task_loss 10.22\n",
      "step    9/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.35\n",
      "step   16/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.05\n",
      "step   20/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.34\n",
      "step   22/100: prior_loss 0.00  task_loss 10.67\n",
      "step   23/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.63\n",
      "step   27/100: prior_loss 0.00  task_loss 10.20\n",
      "step   28/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.27\n",
      "step   35/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.47\n",
      "step   37/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   39/100: prior_loss 0.00  task_loss 10.13\n",
      "step   40/100: prior_loss 0.00  task_loss 10.38\n",
      "step   41/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   42/100: prior_loss 0.00  task_loss 10.08\n",
      "step   43/100: prior_loss 0.00  task_loss 10.13\n",
      "step   44/100: prior_loss 0.00  task_loss 10.13\n",
      "step   45/100: prior_loss 0.00  task_loss 10.51\n",
      "step   46/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.75\n",
      "step   52/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   53/100: prior_loss 0.00  task_loss 10.05\n",
      "step   54/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   55/100: prior_loss 0.00  task_loss 10.58\n",
      "step   56/100: prior_loss 0.00  task_loss 10.34\n",
      "step   57/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.02\n",
      "step   60/100: prior_loss 0.00  task_loss 10.13\n",
      "step   61/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   63/100: prior_loss 0.00  task_loss 10.41\n",
      "step   64/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   68/100: prior_loss 0.00  task_loss 10.07\n",
      "step   69/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   75/100: prior_loss 0.00  task_loss 10.43\n",
      "step   76/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   87/100: prior_loss 0.00  task_loss 10.36\n",
      "step   88/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   89/100: prior_loss 0.00  task_loss 10.48\n",
      "step   90/100: prior_loss 0.00  task_loss 10.24\n",
      "step   91/100: prior_loss 0.00  task_loss 10.20\n",
      "step   92/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.68 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.82 \n",
      "Elapsed: 8072.1 s\n",
      "Saving optimization progress video out/diff-0.005-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.56 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.17 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.52 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.92 \n",
      "Elapsed: 8225.7 s\n",
      "Saving optimization progress video out/diff-0.005-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.87 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.23 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.13 \n",
      "Elapsed: 8813.6 s\n",
      "Saving optimization progress video out/diff-0.005-mini-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.65 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.60 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.63 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.50 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.31 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.05 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.88 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.46 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.21 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.06 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.92 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.87 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.75 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.45 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.26 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.14 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.03 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.93 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.54 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.45 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.54 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.49 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.46 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.27 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.22 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.21 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.06 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.02 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.03 \n",
      "step  100/100: prior_loss 0.00  task_loss 4.03 \n",
      "Elapsed: 8916.0 s\n",
      "Saving optimization progress video out/diff-0.001-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.14 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.92 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.35 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.97 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.11\n",
      "step    6/100: prior_loss 0.00  task_loss 9.70 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.51 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.14 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.22\n",
      "step   13/100: prior_loss 0.00  task_loss 10.58\n",
      "step   14/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.62\n",
      "step   16/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.32\n",
      "step   32/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.97 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.40 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.46 \n",
      "Elapsed: 9012.8 s\n",
      "Saving optimization progress video out/diff-0.001-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.42 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.17\n",
      "step    3/100: prior_loss 0.00  task_loss 9.35 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.57\n",
      "step    5/100: prior_loss 0.00  task_loss 8.72 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.35 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.39\n",
      "step   11/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.00\n",
      "step   13/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   16/100: prior_loss 0.00  task_loss 10.11\n",
      "step   17/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.20\n",
      "step   22/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.16\n",
      "step   28/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.16\n",
      "step   35/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.29\n",
      "step   38/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.15\n",
      "step   47/100: prior_loss 0.00  task_loss 10.50\n",
      "step   48/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.86 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.73 \n",
      "Elapsed: 9162.2 s\n",
      "Saving optimization progress video out/diff-0.001-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.74 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.96 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.28\n",
      "step    6/100: prior_loss 0.00  task_loss 8.66 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.40 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    9/100: prior_loss 0.00  task_loss 10.38\n",
      "step   10/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.58\n",
      "step   14/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.58\n",
      "step   18/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.37\n",
      "step   20/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.03\n",
      "step   22/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.09\n",
      "step   28/100: prior_loss 0.00  task_loss 10.27\n",
      "step   29/100: prior_loss 0.00  task_loss 10.60\n",
      "step   30/100: prior_loss 0.00  task_loss 11.44\n",
      "step   31/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.38\n",
      "step   33/100: prior_loss 0.00  task_loss 10.00\n",
      "step   34/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.13\n",
      "step   39/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.19\n",
      "step   41/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   42/100: prior_loss 0.00  task_loss 10.37\n",
      "step   43/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   49/100: prior_loss 0.00  task_loss 10.35\n",
      "step   50/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.99 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.00 \n",
      "Elapsed: 9749.6 s\n",
      "Saving optimization progress video out/diff-0.001-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.41 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.96 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.51 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.10 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.80 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.52 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.35 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.20 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.77 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   32/100: prior_loss 0.00  task_loss 3.98 \n",
      "step   33/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.77 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.70 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.65 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.54 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.38 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.33 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.02 \n",
      "step   45/100: prior_loss 0.00  task_loss 2.92 \n",
      "step   46/100: prior_loss 0.00  task_loss 2.87 \n",
      "step   47/100: prior_loss 0.00  task_loss 2.82 \n",
      "step   48/100: prior_loss 0.00  task_loss 2.77 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.68 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.60 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.52 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.47 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.35 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.28 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.23 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.11 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.04 \n",
      "step   61/100: prior_loss 0.00  task_loss 1.98 \n",
      "step   62/100: prior_loss 0.00  task_loss 1.93 \n",
      "step   63/100: prior_loss 0.00  task_loss 1.90 \n",
      "step   64/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   65/100: prior_loss 0.00  task_loss 1.87 \n",
      "step   66/100: prior_loss 0.00  task_loss 1.72 \n",
      "step   67/100: prior_loss 0.00  task_loss 1.69 \n",
      "step   68/100: prior_loss 0.00  task_loss 1.62 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.60 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.59 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.53 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.53 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.46 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.42 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.40 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.37 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.45 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.34 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.34 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.34 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.30 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.29 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.28 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.28 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.30 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.32 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.37 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.40 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.49 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.55 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.89 \n",
      "step   97/100: prior_loss 0.00  task_loss 2.17 \n",
      "step   98/100: prior_loss 0.00  task_loss 2.55 \n",
      "step   99/100: prior_loss 0.00  task_loss 2.96 \n",
      "step  100/100: prior_loss 0.00  task_loss 3.45 \n",
      "Elapsed: 9853.6 s\n",
      "Saving optimization progress video out/diff-0.001-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.92 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.56 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.31 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.97 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.32 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.20 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.12 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.01 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.95 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.93 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.79 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.75 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.58 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.53 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.47 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   38/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   39/100: prior_loss 0.00  task_loss 6.23 \n",
      "step   40/100: prior_loss 0.00  task_loss 6.20 \n",
      "step   41/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   42/100: prior_loss 0.00  task_loss 6.13 \n",
      "step   43/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   44/100: prior_loss 0.00  task_loss 6.08 \n",
      "step   45/100: prior_loss 0.00  task_loss 6.03 \n",
      "step   46/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   47/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   48/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.97 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.96 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.87 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.54 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.45 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.45 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.40 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   70/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   71/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   74/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   75/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   89/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   90/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   91/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   92/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   93/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 5.33 \n",
      "step   95/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   96/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   97/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   98/100: prior_loss 0.00  task_loss 5.67 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.77 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.90 \n",
      "Elapsed: 9955.8 s\n",
      "Saving optimization progress video out/diff-0.001-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.14 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.85 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.82 \n",
      "Elapsed: 10099.2 s\n",
      "Saving optimization progress video out/diff-0.001-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.54 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.55 \n",
      "Elapsed: 10679.8 s\n",
      "Saving optimization progress video out/diff-0.001-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.81 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.45 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.19 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.05 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.78 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.30 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.03 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.79 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.84 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.89 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.58 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.47 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.41 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.39 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.28 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.03 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.99 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   38/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   40/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   41/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   42/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   44/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   45/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   46/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   47/100: prior_loss 0.00  task_loss 6.09 \n",
      "step   48/100: prior_loss 0.00  task_loss 6.23 \n",
      "step   49/100: prior_loss 0.00  task_loss 6.76 \n",
      "step   50/100: prior_loss 0.00  task_loss 6.64 \n",
      "step   51/100: prior_loss 0.00  task_loss 6.33 \n",
      "step   52/100: prior_loss 0.00  task_loss 6.26 \n",
      "step   53/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   54/100: prior_loss 0.00  task_loss 6.43 \n",
      "step   55/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   56/100: prior_loss 0.00  task_loss 6.88 \n",
      "step   57/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   58/100: prior_loss 0.00  task_loss 6.86 \n",
      "step   59/100: prior_loss 0.00  task_loss 7.17 \n",
      "step   60/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   61/100: prior_loss 0.00  task_loss 7.05 \n",
      "step   62/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   63/100: prior_loss 0.00  task_loss 7.11 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.22 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.30 \n",
      "step   66/100: prior_loss 0.00  task_loss 6.76 \n",
      "step   67/100: prior_loss 0.00  task_loss 6.37 \n",
      "step   68/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   69/100: prior_loss 0.00  task_loss 6.41 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.01 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.30 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.84 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.30 \n",
      "Elapsed: 10775.5 s\n",
      "Saving optimization progress video out/diff-0.001-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.63 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.45 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.72 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.54 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.59 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.50 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.09\n",
      "step   34/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.00\n",
      "step   38/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.60\n",
      "step   41/100: prior_loss 0.00  task_loss 10.52\n",
      "step   42/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.20\n",
      "step   47/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   49/100: prior_loss 0.00  task_loss 10.04\n",
      "step   50/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   53/100: prior_loss 0.00  task_loss 10.24\n",
      "step   54/100: prior_loss 0.00  task_loss 10.32\n",
      "step   55/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   82/100: prior_loss 0.00  task_loss 10.23\n",
      "step   83/100: prior_loss 0.00  task_loss 10.94\n",
      "step   84/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   87/100: prior_loss 0.00  task_loss 10.21\n",
      "step   88/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   90/100: prior_loss 0.00  task_loss 10.17\n",
      "step   91/100: prior_loss 0.00  task_loss 10.20\n",
      "step   92/100: prior_loss 0.00  task_loss 10.52\n",
      "step   93/100: prior_loss 0.00  task_loss 10.39\n",
      "step   94/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.76 \n",
      "step  100/100: prior_loss 0.00  task_loss 10.04\n",
      "Elapsed: 10879.0 s\n",
      "Saving optimization progress video out/diff-0.001-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.18\n",
      "step    2/100: prior_loss 0.00  task_loss 9.92 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.70 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.30 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.78 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.74 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.17\n",
      "step   16/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.31\n",
      "step   19/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.13\n",
      "step   29/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.17\n",
      "step   31/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.02\n",
      "step   46/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   79/100: prior_loss 0.00  task_loss 10.10\n",
      "step   80/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   82/100: prior_loss 0.00  task_loss 10.16\n",
      "step   83/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.01\n",
      "step   87/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   92/100: prior_loss 0.00  task_loss 10.35\n",
      "step   93/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.50 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.27 \n",
      "Elapsed: 11031.1 s\n",
      "Saving optimization progress video out/diff-0.001-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.89 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.96 \n",
      "Elapsed: 11619.6 s\n",
      "Saving optimization progress video out/diff-0.001-mini-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.55 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.57 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.48 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.29 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.03 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.84 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.60 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.37 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.07 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.90 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.43 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.26 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.07 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.88 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.80 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.68 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.58 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.20 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.54 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.99 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.95 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.86 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.83 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.82 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.81 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.77 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.75 \n",
      "step   57/100: prior_loss 0.00  task_loss 3.73 \n",
      "step   58/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   59/100: prior_loss 0.00  task_loss 3.69 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.67 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.64 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   64/100: prior_loss 0.00  task_loss 3.60 \n",
      "step   65/100: prior_loss 0.00  task_loss 3.59 \n",
      "step   66/100: prior_loss 0.00  task_loss 3.58 \n",
      "step   67/100: prior_loss 0.00  task_loss 3.57 \n",
      "step   68/100: prior_loss 0.00  task_loss 3.55 \n",
      "step   69/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   70/100: prior_loss 0.00  task_loss 3.51 \n",
      "step   71/100: prior_loss 0.00  task_loss 3.50 \n",
      "step   72/100: prior_loss 0.00  task_loss 3.49 \n",
      "step   73/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   74/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   75/100: prior_loss 0.00  task_loss 3.45 \n",
      "step   76/100: prior_loss 0.00  task_loss 3.43 \n",
      "step   77/100: prior_loss 0.00  task_loss 3.42 \n",
      "step   78/100: prior_loss 0.00  task_loss 3.41 \n",
      "step   79/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   80/100: prior_loss 0.00  task_loss 3.39 \n",
      "step   81/100: prior_loss 0.00  task_loss 3.38 \n",
      "step   82/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   83/100: prior_loss 0.00  task_loss 3.35 \n",
      "step   84/100: prior_loss 0.00  task_loss 3.34 \n",
      "step   85/100: prior_loss 0.00  task_loss 3.33 \n",
      "step   86/100: prior_loss 0.00  task_loss 3.32 \n",
      "step   87/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   88/100: prior_loss 0.00  task_loss 3.30 \n",
      "step   89/100: prior_loss 0.00  task_loss 3.29 \n",
      "step   90/100: prior_loss 0.00  task_loss 3.28 \n",
      "step   91/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   92/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   93/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 3.26 \n",
      "step   95/100: prior_loss 0.00  task_loss 3.26 \n",
      "step   96/100: prior_loss 0.00  task_loss 3.26 \n",
      "step   97/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   98/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   99/100: prior_loss 0.00  task_loss 3.25 \n",
      "step  100/100: prior_loss 0.00  task_loss 3.26 \n",
      "Elapsed: 11722.3 s\n",
      "Saving optimization progress video out/diff-0.0005-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.36\n",
      "step    2/100: prior_loss 0.00  task_loss 9.91 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.68 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.29\n",
      "step    5/100: prior_loss 0.00  task_loss 10.09\n",
      "step    6/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.78 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.98 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.58\n",
      "step   14/100: prior_loss 0.00  task_loss 10.69\n",
      "step   15/100: prior_loss 0.00  task_loss 10.00\n",
      "step   16/100: prior_loss 0.00  task_loss 10.06\n",
      "step   17/100: prior_loss 0.00  task_loss 10.43\n",
      "step   18/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.53\n",
      "step   20/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.40\n",
      "step   22/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.17\n",
      "step   31/100: prior_loss 0.00  task_loss 10.03\n",
      "step   32/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.03\n",
      "step   37/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.22 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.23 \n",
      "Elapsed: 11817.4 s\n",
      "Saving optimization progress video out/diff-0.0005-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.33 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.07\n",
      "step    3/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.58 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.70 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.97 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.19\n",
      "step   11/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.24\n",
      "step   16/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.51\n",
      "step   22/100: prior_loss 0.00  task_loss 10.07\n",
      "step   23/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.61\n",
      "step   26/100: prior_loss 0.00  task_loss 10.13\n",
      "step   27/100: prior_loss 0.00  task_loss 10.77\n",
      "step   28/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.16\n",
      "step   31/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.02\n",
      "step   33/100: prior_loss 0.00  task_loss 10.26\n",
      "step   34/100: prior_loss 0.00  task_loss 10.23\n",
      "step   35/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.04\n",
      "step   41/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   49/100: prior_loss 0.00  task_loss 10.02\n",
      "step   50/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.02 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.01 \n",
      "Elapsed: 11968.6 s\n",
      "Saving optimization progress video out/diff-0.0005-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.78 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.18\n",
      "step    5/100: prior_loss 0.00  task_loss 9.77 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.27\n",
      "step    8/100: prior_loss 0.00  task_loss 9.53 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.06\n",
      "step   13/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.09\n",
      "step   26/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.07\n",
      "step   29/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.01\n",
      "step   31/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.39\n",
      "step   35/100: prior_loss 0.00  task_loss 10.63\n",
      "step   36/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.14\n",
      "step   39/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.76 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.89 \n",
      "Elapsed: 12570.4 s\n",
      "Saving optimization progress video out/diff-0.0005-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.17 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.37 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.93 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.53 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.11 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.76 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.32 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   12/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   20/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.39 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   32/100: prior_loss 0.00  task_loss 3.93 \n",
      "step   33/100: prior_loss 0.00  task_loss 3.83 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.75 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.55 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.47 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.24 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.09 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   44/100: prior_loss 0.00  task_loss 2.98 \n",
      "step   45/100: prior_loss 0.00  task_loss 2.96 \n",
      "step   46/100: prior_loss 0.00  task_loss 2.88 \n",
      "step   47/100: prior_loss 0.00  task_loss 2.79 \n",
      "step   48/100: prior_loss 0.00  task_loss 2.77 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.79 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.59 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.62 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.46 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.46 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.38 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.32 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.25 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.20 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.15 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.11 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.04 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.01 \n",
      "step   63/100: prior_loss 0.00  task_loss 1.95 \n",
      "step   64/100: prior_loss 0.00  task_loss 1.92 \n",
      "step   65/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   66/100: prior_loss 0.00  task_loss 1.80 \n",
      "step   67/100: prior_loss 0.00  task_loss 1.78 \n",
      "step   68/100: prior_loss 0.00  task_loss 1.73 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.68 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.77 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.69 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.63 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.67 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.58 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.56 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.57 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.52 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.51 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.52 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.49 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.46 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.45 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.51 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.53 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.56 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.62 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.70 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   98/100: prior_loss 0.00  task_loss 2.09 \n",
      "step   99/100: prior_loss 0.00  task_loss 2.38 \n",
      "step  100/100: prior_loss 0.00  task_loss 2.74 \n",
      "Elapsed: 12678.6 s\n",
      "Saving optimization progress video out/diff-0.0005-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.02 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.81 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.67 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.52 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.36 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.20 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.53 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.36 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.18 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.11 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.04 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.97 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.91 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.86 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.81 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.69 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.54 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.52 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.45 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.41 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   38/100: prior_loss 0.00  task_loss 6.31 \n",
      "step   39/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   40/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   41/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   42/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 6.12 \n",
      "step   44/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   45/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   46/100: prior_loss 0.00  task_loss 6.02 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.96 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.92 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.81 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.75 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.69 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.47 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.34 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   70/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   71/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   74/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   93/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   94/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   95/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   96/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   97/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   98/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.61 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.77 \n",
      "Elapsed: 12782.1 s\n",
      "Saving optimization progress video out/diff-0.0005-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.90 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.92 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.99 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.98 \n",
      "Elapsed: 12931.3 s\n",
      "Saving optimization progress video out/diff-0.0005-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.18 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.55 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.41 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.48 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.47 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.62 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.62 \n",
      "Elapsed: 13559.1 s\n",
      "Saving optimization progress video out/diff-0.0005-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.04 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.04 \n",
      "step    3/100: prior_loss 0.00  task_loss 7.99 \n",
      "step    4/100: prior_loss 0.00  task_loss 7.95 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.84 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.62 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.26 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.23 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.30 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.20 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.16 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.20 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.33 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.21 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   74/100: prior_loss 0.00  task_loss 5.86 \n",
      "step   75/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   76/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   77/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   78/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   79/100: prior_loss 0.00  task_loss 5.71 \n",
      "step   80/100: prior_loss 0.00  task_loss 6.09 \n",
      "step   81/100: prior_loss 0.00  task_loss 6.26 \n",
      "step   82/100: prior_loss 0.00  task_loss 6.50 \n",
      "step   83/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.05 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.09 \n",
      "step   86/100: prior_loss 0.00  task_loss 6.94 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.19 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.72 \n",
      "Elapsed: 13657.6 s\n",
      "Saving optimization progress video out/diff-0.0005-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.68 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.41 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.13 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.94 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.67 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.27 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.45\n",
      "step   31/100: prior_loss 0.00  task_loss 10.51\n",
      "step   32/100: prior_loss 0.00  task_loss 10.50\n",
      "step   33/100: prior_loss 0.00  task_loss 10.69\n",
      "step   34/100: prior_loss 0.00  task_loss 10.30\n",
      "step   35/100: prior_loss 0.00  task_loss 10.17\n",
      "step   36/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.41\n",
      "step   38/100: prior_loss 0.00  task_loss 10.29\n",
      "step   39/100: prior_loss 0.00  task_loss 10.05\n",
      "step   40/100: prior_loss 0.00  task_loss 10.46\n",
      "step   41/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.22\n",
      "step   44/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   49/100: prior_loss 0.00  task_loss 10.07\n",
      "step   50/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.50\n",
      "step   52/100: prior_loss 0.00  task_loss 10.24\n",
      "step   53/100: prior_loss 0.00  task_loss 10.42\n",
      "step   54/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   55/100: prior_loss 0.00  task_loss 10.17\n",
      "step   56/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   61/100: prior_loss 0.00  task_loss 10.39\n",
      "step   62/100: prior_loss 0.00  task_loss 10.17\n",
      "step   63/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   65/100: prior_loss 0.00  task_loss 10.04\n",
      "step   66/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   68/100: prior_loss 0.00  task_loss 10.17\n",
      "step   69/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   70/100: prior_loss 0.00  task_loss 10.04\n",
      "step   71/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   78/100: prior_loss 0.00  task_loss 10.31\n",
      "step   79/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.55\n",
      "step   81/100: prior_loss 0.00  task_loss 10.03\n",
      "step   82/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   85/100: prior_loss 0.00  task_loss 10.17\n",
      "step   86/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   89/100: prior_loss 0.00  task_loss 10.22\n",
      "step   90/100: prior_loss 0.00  task_loss 10.10\n",
      "step   91/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   98/100: prior_loss 0.00  task_loss 10.22\n",
      "step   99/100: prior_loss 0.00  task_loss 9.00 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.57 \n",
      "Elapsed: 13761.9 s\n",
      "Saving optimization progress video out/diff-0.0005-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.71\n",
      "step    2/100: prior_loss 0.00  task_loss 9.71 \n",
      "step    3/100: prior_loss 0.00  task_loss 10.06\n",
      "step    4/100: prior_loss 0.00  task_loss 9.63 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.19\n",
      "step    7/100: prior_loss 0.00  task_loss 9.55 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.40 \n",
      "step    9/100: prior_loss 0.00  task_loss 10.12\n",
      "step   10/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.00\n",
      "step   16/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.13\n",
      "step   18/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.10\n",
      "step   20/100: prior_loss 0.00  task_loss 10.37\n",
      "step   21/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.30\n",
      "step   24/100: prior_loss 0.00  task_loss 10.01\n",
      "step   25/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.19\n",
      "step   29/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.25\n",
      "step   34/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.13\n",
      "step   37/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.30\n",
      "step   41/100: prior_loss 0.00  task_loss 10.25\n",
      "step   42/100: prior_loss 0.00  task_loss 10.07\n",
      "step   43/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   53/100: prior_loss 0.00  task_loss 10.15\n",
      "step   54/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   56/100: prior_loss 0.00  task_loss 10.10\n",
      "step   57/100: prior_loss 0.00  task_loss 10.01\n",
      "step   58/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.11\n",
      "step   60/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   65/100: prior_loss 0.00  task_loss 10.04\n",
      "step   66/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   68/100: prior_loss 0.00  task_loss 10.13\n",
      "step   69/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   70/100: prior_loss 0.00  task_loss 10.32\n",
      "step   71/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.19\n",
      "step   75/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   81/100: prior_loss 0.00  task_loss 10.04\n",
      "step   82/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   84/100: prior_loss 0.00  task_loss 10.63\n",
      "step   85/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   95/100: prior_loss 0.00  task_loss 10.22\n",
      "step   96/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   98/100: prior_loss 0.00  task_loss 10.07\n",
      "step   99/100: prior_loss 0.00  task_loss 9.97 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.44 \n",
      "Elapsed: 13921.7 s\n",
      "Saving optimization progress video out/diff-0.0005-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.31 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.24 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.08 \n",
      "Elapsed: 14559.6 s\n",
      "Saving optimization progress video out/diff-0.0005-mini-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.52 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.60 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.67 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.77 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.35 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.10 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.57 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.48 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.29 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.19 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.10 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.98 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.90 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.80 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.71 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.61 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.42 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.33 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.13 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.01 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.94 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.89 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.85 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.76 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.71 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.67 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.63 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.29 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.25 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.71 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.69 \n",
      "step  100/100: prior_loss 0.00  task_loss 4.70 \n",
      "Elapsed: 14666.0 s\n",
      "Saving optimization progress video out/diff-0.0001-linear-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.66 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.77 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.50 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.32 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.04\n",
      "step   16/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.16\n",
      "step   18/100: prior_loss 0.00  task_loss 10.12\n",
      "step   19/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.23\n",
      "step   22/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.14\n",
      "step   37/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.12 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.41 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.46 \n",
      "Elapsed: 14764.5 s\n",
      "Saving optimization progress video out/diff-0.0001-linear-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.40 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.43 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.98 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.57 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.40 \n",
      "step    8/100: prior_loss 0.00  task_loss 10.63\n",
      "step    9/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.46\n",
      "step   11/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.15\n",
      "step   15/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   16/100: prior_loss 0.00  task_loss 10.01\n",
      "step   17/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.04\n",
      "step   19/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.59\n",
      "step   21/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.00\n",
      "step   25/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.29\n",
      "step   27/100: prior_loss 0.00  task_loss 10.01\n",
      "step   28/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.34\n",
      "step   31/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.05\n",
      "step   34/100: prior_loss 0.00  task_loss 10.04\n",
      "step   35/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.15\n",
      "step   37/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.29\n",
      "step   39/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.10\n",
      "step   41/100: prior_loss 0.00  task_loss 10.06\n",
      "step   42/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.17\n",
      "step   45/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.82 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.78 \n",
      "Elapsed: 14921.9 s\n",
      "Saving optimization progress video out/diff-0.0001-linear-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.91\n",
      "step    2/100: prior_loss 0.00  task_loss 10.12\n",
      "step    3/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.66 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.37\n",
      "step    6/100: prior_loss 0.00  task_loss 9.84 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.03\n",
      "step    8/100: prior_loss 0.00  task_loss 9.84 \n",
      "step    9/100: prior_loss 0.00  task_loss 10.37\n",
      "step   10/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   11/100: prior_loss 0.00  task_loss 10.08\n",
      "step   12/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.12\n",
      "step   14/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   15/100: prior_loss 0.00  task_loss 10.28\n",
      "step   16/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.07\n",
      "step   22/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.31\n",
      "step   26/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.26\n",
      "step   28/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.15\n",
      "step   30/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   35/100: prior_loss 0.00  task_loss 10.10\n",
      "step   36/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.12\n",
      "step   41/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.01\n",
      "step   46/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   56/100: prior_loss 0.00  task_loss 10.12\n",
      "step   57/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.91 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.13 \n",
      "Elapsed: 15558.4 s\n",
      "Saving optimization progress video out/diff-0.0001-linear-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.87 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.45 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.00 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.57 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.20 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.90 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.66 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.44 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.87 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.25 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   19/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   20/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.44 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.99 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.84 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.74 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.65 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.60 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.56 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.51 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.30 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.23 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.20 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.11 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.05 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.97 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.91 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.85 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.78 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.52 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.45 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.33 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.28 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.24 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.21 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.17 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.09 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.04 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.01 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.91 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.83 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.75 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.66 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.66 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.59 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.53 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.41 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.38 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.31 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.26 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.23 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.20 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.19 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.16 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.15 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.14 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.14 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.16 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.18 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.20 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.24 \n",
      "Elapsed: 15667.0 s\n",
      "Saving optimization progress video out/diff-0.0001-constant-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.60 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.22 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.83 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.55 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.11 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.95 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.60 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.41 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.34 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.21 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.93 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.86 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.81 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.68 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.20 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.49 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.93 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.86 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.77 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   64/100: prior_loss 0.00  task_loss 3.65 \n",
      "step   65/100: prior_loss 0.00  task_loss 3.60 \n",
      "step   66/100: prior_loss 0.00  task_loss 3.54 \n",
      "step   67/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   68/100: prior_loss 0.00  task_loss 3.41 \n",
      "step   69/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   70/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   71/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   72/100: prior_loss 0.00  task_loss 3.20 \n",
      "step   73/100: prior_loss 0.00  task_loss 3.13 \n",
      "step   74/100: prior_loss 0.00  task_loss 3.07 \n",
      "step   75/100: prior_loss 0.00  task_loss 3.02 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.97 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.89 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.84 \n",
      "step   79/100: prior_loss 0.00  task_loss 2.80 \n",
      "step   80/100: prior_loss 0.00  task_loss 2.84 \n",
      "step   81/100: prior_loss 0.00  task_loss 2.81 \n",
      "step   82/100: prior_loss 0.00  task_loss 2.79 \n",
      "step   83/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   84/100: prior_loss 0.00  task_loss 2.69 \n",
      "step   85/100: prior_loss 0.00  task_loss 2.66 \n",
      "step   86/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   87/100: prior_loss 0.00  task_loss 2.61 \n",
      "step   88/100: prior_loss 0.00  task_loss 2.60 \n",
      "step   89/100: prior_loss 0.00  task_loss 2.66 \n",
      "step   90/100: prior_loss 0.00  task_loss 2.68 \n",
      "step   91/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   92/100: prior_loss 0.00  task_loss 2.95 \n",
      "step   93/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 3.59 \n",
      "step   95/100: prior_loss 0.00  task_loss 3.95 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   98/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   99/100: prior_loss 0.00  task_loss 5.42 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.70 \n",
      "Elapsed: 15769.5 s\n",
      "Saving optimization progress video out/diff-0.0001-constant-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.53 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.48 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.38 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.11 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.14 \n",
      "Elapsed: 15920.1 s\n",
      "Saving optimization progress video out/diff-0.0001-constant-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.87 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.15 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.36 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.57 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.53 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.41 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.42 \n",
      "Elapsed: 16547.9 s\n",
      "Saving optimization progress video out/diff-0.0001-constant-1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.15\n",
      "step    2/100: prior_loss 0.00  task_loss 10.11\n",
      "step    3/100: prior_loss 0.00  task_loss 9.99 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.84 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.59 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.39 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.23 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.92 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.53 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.67 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.23 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.03 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.96 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.71 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.52 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.08 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.80 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.80 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.67 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.57 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.34 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.25 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   71/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   80/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   83/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   84/100: prior_loss 0.00  task_loss 5.29 \n",
      "step   85/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   86/100: prior_loss 0.00  task_loss 5.34 \n",
      "step   87/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   88/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   89/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   90/100: prior_loss 0.00  task_loss 5.87 \n",
      "step   91/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   92/100: prior_loss 0.00  task_loss 6.88 \n",
      "step   93/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.04 \n",
      "step   95/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.03 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.62 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.00 \n",
      "Elapsed: 16647.6 s\n",
      "Saving optimization progress video out/diff-0.0001-mini-1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.21\n",
      "step    2/100: prior_loss 0.00  task_loss 10.22\n",
      "step    3/100: prior_loss 0.00  task_loss 10.28\n",
      "step    4/100: prior_loss 0.00  task_loss 10.14\n",
      "step    5/100: prior_loss 0.00  task_loss 10.09\n",
      "step    6/100: prior_loss 0.00  task_loss 9.95 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.76 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.53 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   23/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   25/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.67 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.12\n",
      "step   42/100: prior_loss 0.00  task_loss 10.25\n",
      "step   43/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.38\n",
      "step   46/100: prior_loss 0.00  task_loss 10.03\n",
      "step   47/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   61/100: prior_loss 0.00  task_loss 10.74\n",
      "step   62/100: prior_loss 0.00  task_loss 10.49\n",
      "step   63/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.16\n",
      "step   75/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   76/100: prior_loss 0.00  task_loss 10.04\n",
      "step   77/100: prior_loss 0.00  task_loss 10.67\n",
      "step   78/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.18\n",
      "step   81/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   82/100: prior_loss 0.00  task_loss 10.48\n",
      "step   83/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   89/100: prior_loss 0.00  task_loss 10.46\n",
      "step   90/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.64 \n",
      "step  100/100: prior_loss 0.00  task_loss 10.40\n",
      "Elapsed: 16750.2 s\n",
      "Saving optimization progress video out/diff-0.0001-mini-10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.30\n",
      "step    2/100: prior_loss 0.00  task_loss 10.29\n",
      "step    3/100: prior_loss 0.00  task_loss 9.33 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.28\n",
      "step    6/100: prior_loss 0.00  task_loss 9.45 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.88 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.50 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   11/100: prior_loss 0.00  task_loss 10.02\n",
      "step   12/100: prior_loss 0.00  task_loss 10.35\n",
      "step   13/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.09\n",
      "step   15/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.38\n",
      "step   22/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.14\n",
      "step   24/100: prior_loss 0.00  task_loss 10.04\n",
      "step   25/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.14\n",
      "step   29/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.04\n",
      "step   32/100: prior_loss 0.00  task_loss 10.41\n",
      "step   33/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.08\n",
      "step   35/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.12\n",
      "step   38/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.38\n",
      "step   41/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.31\n",
      "step   45/100: prior_loss 0.00  task_loss 10.05\n",
      "step   46/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   47/100: prior_loss 0.00  task_loss 10.30\n",
      "step   48/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   52/100: prior_loss 0.00  task_loss 10.64\n",
      "step   53/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.02\n",
      "step   60/100: prior_loss 0.00  task_loss 10.18\n",
      "step   61/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   62/100: prior_loss 0.00  task_loss 10.17\n",
      "step   63/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   65/100: prior_loss 0.00  task_loss 10.04\n",
      "step   66/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   68/100: prior_loss 0.00  task_loss 10.44\n",
      "step   69/100: prior_loss 0.00  task_loss 10.50\n",
      "step   70/100: prior_loss 0.00  task_loss 10.41\n",
      "step   71/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   72/100: prior_loss 0.00  task_loss 10.11\n",
      "step   73/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.02\n",
      "step   75/100: prior_loss 0.00  task_loss 10.15\n",
      "step   76/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   77/100: prior_loss 0.00  task_loss 10.43\n",
      "step   78/100: prior_loss 0.00  task_loss 10.67\n",
      "step   79/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.04\n",
      "step   81/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   84/100: prior_loss 0.00  task_loss 10.11\n",
      "step   85/100: prior_loss 0.00  task_loss 10.18\n",
      "step   86/100: prior_loss 0.00  task_loss 10.94\n",
      "step   87/100: prior_loss 0.00  task_loss 10.19\n",
      "step   88/100: prior_loss 0.00  task_loss 10.32\n",
      "step   89/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   90/100: prior_loss 0.00  task_loss 10.37\n",
      "step   91/100: prior_loss 0.00  task_loss 10.02\n",
      "step   92/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   94/100: prior_loss 0.00  task_loss 10.20\n",
      "step   95/100: prior_loss 0.00  task_loss 10.13\n",
      "step   96/100: prior_loss 0.00  task_loss 10.00\n",
      "step   97/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.99 \n",
      "step  100/100: prior_loss 0.00  task_loss 10.19\n",
      "Elapsed: 16908.1 s\n",
      "Saving optimization progress video out/diff-0.0001-mini-100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.18 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.90 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.57 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.34 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.17 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.13 \n",
      "Elapsed: 17539.9 s\n",
      "Saving optimization progress video out/diff-0.0001-mini-1000.mp4\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "stepsizes = [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "time_schedules = [\"linear\", \"constant\", \"mini\"]\n",
    "num_d_steps = [1, 10, 100, 1000]\n",
    "\n",
    "for s in stepsizes:\n",
    "    for mode in time_schedules:\n",
    "        for d in num_d_steps:\n",
    "            # later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "            projected_w_steps = projector.project(\n",
    "                num_images=12,\n",
    "                num_steps=100,\n",
    "                diffusion_time_schedule=mode,\n",
    "                num_diffusion_steps_per_step = d,\n",
    "                diffusion_step_size = s\n",
    "            )\n",
    "\n",
    "            print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
    "            create_video(projected_w_steps, projector, num_rows=3, outdir=\"out\", name=f\"diff-dfsteps{s}-mode{mode}-innersteps{d}.mp4\")\n",
    "            del(projected_w_steps)\n",
    "\n",
    "start_time = perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992a9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/100: prior_loss 0.00  task_loss 7.91 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.08 \n",
      "step    3/100: prior_loss 0.00  task_loss 7.83 \n",
      "step    4/100: prior_loss 0.00  task_loss 7.52 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.37 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.08 \n",
      "step    7/100: prior_loss 0.00  task_loss 6.90 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.70 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.42 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.30 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.93 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.69 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.58 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.21 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.06 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.02 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.00 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.98 \n",
      "step   57/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   58/100: prior_loss 0.00  task_loss 3.96 \n",
      "step   59/100: prior_loss 0.00  task_loss 3.95 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.93 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.92 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.90 \n",
      "step   64/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   65/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   66/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   67/100: prior_loss 0.00  task_loss 3.86 \n",
      "step   68/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   69/100: prior_loss 0.00  task_loss 3.84 \n",
      "step   70/100: prior_loss 0.00  task_loss 3.83 \n",
      "step   71/100: prior_loss 0.00  task_loss 3.82 \n",
      "step   72/100: prior_loss 0.00  task_loss 3.82 \n",
      "step   73/100: prior_loss 0.00  task_loss 3.80 \n",
      "step   74/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   75/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   76/100: prior_loss 0.00  task_loss 3.78 \n",
      "step   77/100: prior_loss 0.00  task_loss 3.77 \n",
      "step   78/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   79/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   80/100: prior_loss 0.00  task_loss 3.75 \n",
      "step   81/100: prior_loss 0.00  task_loss 3.74 \n",
      "step   82/100: prior_loss 0.00  task_loss 3.73 \n",
      "step   83/100: prior_loss 0.00  task_loss 3.73 \n",
      "step   84/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   85/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   86/100: prior_loss 0.00  task_loss 3.70 \n",
      "step   87/100: prior_loss 0.00  task_loss 3.69 \n",
      "step   88/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   89/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   90/100: prior_loss 0.00  task_loss 3.67 \n",
      "step   91/100: prior_loss 0.00  task_loss 3.67 \n",
      "step   92/100: prior_loss 0.00  task_loss 3.67 \n",
      "step   93/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   94/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   95/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   96/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   97/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   98/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   99/100: prior_loss 0.00  task_loss 3.65 \n",
      "step  100/100: prior_loss 0.00  task_loss 3.66 \n",
      "Elapsed: 65.6 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modelinear-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 7.93 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.21 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.37 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.10 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.30 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.38 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.19 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.05\n",
      "step   26/100: prior_loss 0.00  task_loss 10.24\n",
      "step   27/100: prior_loss 0.00  task_loss 10.02\n",
      "step   28/100: prior_loss 0.00  task_loss 10.19\n",
      "step   29/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.29\n",
      "step   31/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.20\n",
      "step   34/100: prior_loss 0.00  task_loss 10.04\n",
      "step   35/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.27\n",
      "step   38/100: prior_loss 0.00  task_loss 10.07\n",
      "step   39/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.39 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.18 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.21 \n",
      "Elapsed: 162.0 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modelinear-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.69 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.61 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.07 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.66 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.07 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.83 \n",
      "step    8/100: prior_loss 0.00  task_loss 10.35\n",
      "step    9/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   11/100: prior_loss 0.00  task_loss 10.78\n",
      "step   12/100: prior_loss 0.00  task_loss 10.14\n",
      "step   13/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.28\n",
      "step   15/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.06\n",
      "step   22/100: prior_loss 0.00  task_loss 10.20\n",
      "step   23/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.02\n",
      "step   27/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.07\n",
      "step   30/100: prior_loss 0.00  task_loss 10.10\n",
      "step   31/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.10\n",
      "step   35/100: prior_loss 0.00  task_loss 10.03\n",
      "step   36/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.08\n",
      "step   38/100: prior_loss 0.00  task_loss 10.09\n",
      "step   39/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   48/100: prior_loss 0.00  task_loss 10.01\n",
      "step   49/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.89 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.85 \n",
      "Elapsed: 315.4 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modelinear-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.71 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.15 \n",
      "step    3/100: prior_loss 0.00  task_loss 10.30\n",
      "step    4/100: prior_loss 0.00  task_loss 9.74 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.71 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.18 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.31 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.43 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.21\n",
      "step   14/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.46\n",
      "step   25/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.40\n",
      "step   27/100: prior_loss 0.00  task_loss 10.28\n",
      "step   28/100: prior_loss 0.00  task_loss 10.43\n",
      "step   29/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.33\n",
      "step   33/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.26\n",
      "step   38/100: prior_loss 0.00  task_loss 10.02\n",
      "step   39/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   48/100: prior_loss 0.00  task_loss 10.22\n",
      "step   49/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.04 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.64 \n",
      "Elapsed: 931.0 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modelinear-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.40 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.90 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.45 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.10 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.49 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.85 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.29 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.17 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   19/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   20/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.26 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.21 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.02 \n",
      "step   33/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.82 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.75 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.63 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.55 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.49 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.41 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.32 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.22 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.13 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.05 \n",
      "step   46/100: prior_loss 0.00  task_loss 2.98 \n",
      "step   47/100: prior_loss 0.00  task_loss 2.92 \n",
      "step   48/100: prior_loss 0.00  task_loss 2.89 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.82 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.66 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.60 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.55 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.51 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.45 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.38 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.34 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.31 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.27 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.24 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.20 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.17 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.12 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.11 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.06 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.05 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.01 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.99 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.95 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.92 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.87 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.65 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.64 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.61 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.57 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.55 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.52 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.49 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.46 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.43 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.41 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.39 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.38 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.36 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.36 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.34 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.34 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.36 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.37 \n",
      "Elapsed: 1038.7 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modeconstant-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.88 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.86 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.74 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.51 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.21 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.87 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.53 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.28 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.11 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.96 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.70 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.48 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.17 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.08 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.00 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.81 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.58 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.31 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.06 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.82 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.74 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.60 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.49 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.35 \n",
      "step   57/100: prior_loss 0.00  task_loss 3.34 \n",
      "step   58/100: prior_loss 0.00  task_loss 3.27 \n",
      "step   59/100: prior_loss 0.00  task_loss 3.21 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.13 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.04 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.99 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.97 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.91 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.86 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.81 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.65 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.59 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.55 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 2.41 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.41 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.38 \n",
      "step   79/100: prior_loss 0.00  task_loss 2.34 \n",
      "step   80/100: prior_loss 0.00  task_loss 2.29 \n",
      "step   81/100: prior_loss 0.00  task_loss 2.25 \n",
      "step   82/100: prior_loss 0.00  task_loss 2.24 \n",
      "step   83/100: prior_loss 0.00  task_loss 2.19 \n",
      "step   84/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   85/100: prior_loss 0.00  task_loss 2.15 \n",
      "step   86/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   87/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   88/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   89/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   90/100: prior_loss 0.00  task_loss 2.21 \n",
      "step   91/100: prior_loss 0.00  task_loss 2.28 \n",
      "step   92/100: prior_loss 0.00  task_loss 2.34 \n",
      "step   93/100: prior_loss 0.00  task_loss 2.44 \n",
      "step   94/100: prior_loss 0.00  task_loss 2.70 \n",
      "step   95/100: prior_loss 0.00  task_loss 3.04 \n",
      "step   96/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   97/100: prior_loss 0.00  task_loss 3.83 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.72 \n",
      "step  100/100: prior_loss 0.00  task_loss 5.07 \n",
      "Elapsed: 1140.5 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modeconstant-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.92 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.77 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.66 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.54 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.41 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   34/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   35/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   36/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   37/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   41/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   44/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.28 \n",
      "step   48/100: prior_loss 0.00  task_loss 8.31 \n",
      "step   49/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   50/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   51/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.22 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.23 \n",
      "Elapsed: 1290.1 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modeconstant-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.15 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.42 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.68 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.47 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.36 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.36 \n",
      "Elapsed: 1881.8 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modeconstant-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.19 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.08 \n",
      "step    3/100: prior_loss 0.00  task_loss 7.98 \n",
      "step    4/100: prior_loss 0.00  task_loss 7.93 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.80 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.52 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.32 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.24 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.10 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.93 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.74 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.09 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.96 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.83 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.68 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.96 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.12 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.33 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.21 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.21 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.69 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.85 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.89 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.25 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   70/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   74/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   75/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   77/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   78/100: prior_loss 0.00  task_loss 5.45 \n",
      "step   79/100: prior_loss 0.00  task_loss 5.54 \n",
      "step   80/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   83/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   84/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   85/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   86/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   87/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   88/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   89/100: prior_loss 0.00  task_loss 5.29 \n",
      "step   90/100: prior_loss 0.00  task_loss 5.97 \n",
      "step   91/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   92/100: prior_loss 0.00  task_loss 6.86 \n",
      "step   93/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   94/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   95/100: prior_loss 0.00  task_loss 6.84 \n",
      "step   96/100: prior_loss 0.00  task_loss 6.84 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.65 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.28 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.45 \n",
      "Elapsed: 1978.4 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modemini-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.14 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.56 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.40 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.25 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.46 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.02 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.56 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.32\n",
      "step   46/100: prior_loss 0.00  task_loss 10.24\n",
      "step   47/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   48/100: prior_loss 0.00  task_loss 10.05\n",
      "step   49/100: prior_loss 0.00  task_loss 10.40\n",
      "step   50/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   55/100: prior_loss 0.00  task_loss 10.16\n",
      "step   56/100: prior_loss 0.00  task_loss 10.01\n",
      "step   57/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   62/100: prior_loss 0.00  task_loss 10.02\n",
      "step   63/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   64/100: prior_loss 0.00  task_loss 10.04\n",
      "step   65/100: prior_loss 0.00  task_loss 10.54\n",
      "step   66/100: prior_loss 0.00  task_loss 10.27\n",
      "step   67/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   70/100: prior_loss 0.00  task_loss 10.43\n",
      "step   71/100: prior_loss 0.00  task_loss 10.25\n",
      "step   72/100: prior_loss 0.00  task_loss 10.03\n",
      "step   73/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   77/100: prior_loss 0.00  task_loss 10.32\n",
      "step   78/100: prior_loss 0.00  task_loss 10.25\n",
      "step   79/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   90/100: prior_loss 0.00  task_loss 10.30\n",
      "step   91/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   95/100: prior_loss 0.00  task_loss 10.71\n",
      "step   96/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.06 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.14 \n",
      "Elapsed: 2078.3 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modemini-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.56 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    3/100: prior_loss 0.00  task_loss 10.45\n",
      "step    4/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.93 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    8/100: prior_loss 0.00  task_loss 10.13\n",
      "step    9/100: prior_loss 0.00  task_loss 10.21\n",
      "step   10/100: prior_loss 0.00  task_loss 10.03\n",
      "step   11/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.66\n",
      "step   14/100: prior_loss 0.00  task_loss 10.09\n",
      "step   15/100: prior_loss 0.00  task_loss 10.28\n",
      "step   16/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.43\n",
      "step   18/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   19/100: prior_loss 0.00  task_loss 10.24\n",
      "step   20/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.01\n",
      "step   23/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.88\n",
      "step   25/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.04\n",
      "step   29/100: prior_loss 0.00  task_loss 10.00\n",
      "step   30/100: prior_loss 0.00  task_loss 10.15\n",
      "step   31/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.23\n",
      "step   33/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.34\n",
      "step   35/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.45\n",
      "step   39/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.02\n",
      "step   41/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.52\n",
      "step   44/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   48/100: prior_loss 0.00  task_loss 10.00\n",
      "step   49/100: prior_loss 0.00  task_loss 10.35\n",
      "step   50/100: prior_loss 0.00  task_loss 10.26\n",
      "step   51/100: prior_loss 0.00  task_loss 10.06\n",
      "step   52/100: prior_loss 0.00  task_loss 10.14\n",
      "step   53/100: prior_loss 0.00  task_loss 10.25\n",
      "step   54/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.00\n",
      "step   60/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   70/100: prior_loss 0.00  task_loss 10.01\n",
      "step   71/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   72/100: prior_loss 0.00  task_loss 10.20\n",
      "step   73/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   77/100: prior_loss 0.00  task_loss 10.13\n",
      "step   78/100: prior_loss 0.00  task_loss 10.34\n",
      "step   79/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   82/100: prior_loss 0.00  task_loss 10.00\n",
      "step   83/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.21\n",
      "step   87/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   96/100: prior_loss 0.00  task_loss 10.10\n",
      "step   97/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.70 \n",
      "step  100/100: prior_loss 0.00  task_loss 10.21\n",
      "Elapsed: 2229.0 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modemini-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.98 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   24/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.38 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.39 \n",
      "Elapsed: 2818.1 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-05-modemini-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.31 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.31 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.35 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.23 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.01 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.90 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.57 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.40 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.26 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.18 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.05 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.98 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.88 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.78 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.61 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.49 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.39 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.35 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.12 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.03 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.57 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.51 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.30 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.57 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.54 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.49 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.21 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.19 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.06 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.03 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.00 \n",
      "step   79/100: prior_loss 0.00  task_loss 3.99 \n",
      "step   80/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   81/100: prior_loss 0.00  task_loss 3.95 \n",
      "step   82/100: prior_loss 0.00  task_loss 3.95 \n",
      "step   83/100: prior_loss 0.00  task_loss 3.93 \n",
      "step   84/100: prior_loss 0.00  task_loss 3.92 \n",
      "step   85/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   86/100: prior_loss 0.00  task_loss 3.90 \n",
      "step   87/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   88/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   89/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   90/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   92/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   93/100: prior_loss 0.00  task_loss 3.86 \n",
      "step   94/100: prior_loss 0.00  task_loss 3.86 \n",
      "step   95/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   96/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   97/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   98/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   99/100: prior_loss 0.00  task_loss 3.85 \n",
      "step  100/100: prior_loss 0.00  task_loss 3.85 \n",
      "Elapsed: 2920.9 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modelinear-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.11 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.50 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.45 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.49 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.06 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.57\n",
      "step   19/100: prior_loss 0.00  task_loss 10.14\n",
      "step   20/100: prior_loss 0.00  task_loss 10.01\n",
      "step   21/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.30\n",
      "step   24/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   25/100: prior_loss 0.00  task_loss 10.18\n",
      "step   26/100: prior_loss 0.00  task_loss 10.31\n",
      "step   27/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.11\n",
      "step   30/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.49 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.48 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.80 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.86 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.89 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.89 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.92 \n",
      "Elapsed: 3016.6 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modelinear-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.78 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.56 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.56\n",
      "step    6/100: prior_loss 0.00  task_loss 9.63 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.65 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.82 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.25\n",
      "step   11/100: prior_loss 0.00  task_loss 10.11\n",
      "step   12/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.31\n",
      "step   21/100: prior_loss 0.00  task_loss 10.75\n",
      "step   22/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.66\n",
      "step   27/100: prior_loss 0.00  task_loss 10.16\n",
      "step   28/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.52\n",
      "step   33/100: prior_loss 0.00  task_loss 10.12\n",
      "step   34/100: prior_loss 0.00  task_loss 10.03\n",
      "step   35/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.09\n",
      "step   42/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.05\n",
      "step   44/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.83 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.83 \n",
      "Elapsed: 3165.1 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modelinear-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.33\n",
      "step    2/100: prior_loss 0.00  task_loss 8.74 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.70 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.47 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.75 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.22 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.32\n",
      "step   13/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.08\n",
      "step   15/100: prior_loss 0.00  task_loss 10.69\n",
      "step   16/100: prior_loss 0.00  task_loss 10.33\n",
      "step   17/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.04\n",
      "step   19/100: prior_loss 0.00  task_loss 10.47\n",
      "step   20/100: prior_loss 0.00  task_loss 10.72\n",
      "step   21/100: prior_loss 0.00  task_loss 10.12\n",
      "step   22/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.27\n",
      "step   24/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.08\n",
      "step   35/100: prior_loss 0.00  task_loss 10.15\n",
      "step   36/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   38/100: prior_loss 0.00  task_loss 10.36\n",
      "step   39/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   58/100: prior_loss 0.00  task_loss 10.01\n",
      "step   59/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.52 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.88 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.71 \n",
      "Elapsed: 3753.3 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modelinear-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.23 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.44 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.94 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.54 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.20 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.98 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.78 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.60 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.31 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.53 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.87 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.84 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.73 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.65 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.59 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.56 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.50 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.44 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.39 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.33 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.19 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.12 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.06 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.00 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.96 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.90 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.87 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.81 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.61 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.57 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.52 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.50 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.44 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.36 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.30 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.26 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.21 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.12 \n",
      "step   75/100: prior_loss 0.00  task_loss 2.09 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.06 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.03 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.00 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.98 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.95 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.93 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.91 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.86 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.82 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.79 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.78 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.78 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.76 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.75 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.75 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.73 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.74 \n",
      "Elapsed: 3857.4 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modeconstant-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.68 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.69 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.47 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.13 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.82 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.47 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.13 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.35 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.93 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.27 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.19 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.00 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.78 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.72 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.64 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.58 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.49 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.43 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.33 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.26 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.19 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.09 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.01 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.96 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.86 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.86 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.83 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.78 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.76 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.71 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.69 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.60 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.56 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.53 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.50 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.48 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.45 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.44 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.40 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.40 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.38 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.35 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.34 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.33 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.29 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.28 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.26 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.25 \n",
      "step   75/100: prior_loss 0.00  task_loss 2.23 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.22 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.20 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.19 \n",
      "step   79/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   80/100: prior_loss 0.00  task_loss 2.17 \n",
      "step   81/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   82/100: prior_loss 0.00  task_loss 2.15 \n",
      "step   83/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   84/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   85/100: prior_loss 0.00  task_loss 2.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 2.13 \n",
      "step   87/100: prior_loss 0.00  task_loss 2.13 \n",
      "step   88/100: prior_loss 0.00  task_loss 2.13 \n",
      "step   89/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   90/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   91/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   92/100: prior_loss 0.00  task_loss 2.20 \n",
      "step   93/100: prior_loss 0.00  task_loss 2.24 \n",
      "step   94/100: prior_loss 0.00  task_loss 2.29 \n",
      "step   95/100: prior_loss 0.00  task_loss 2.37 \n",
      "step   96/100: prior_loss 0.00  task_loss 2.44 \n",
      "step   97/100: prior_loss 0.00  task_loss 2.58 \n",
      "step   98/100: prior_loss 0.00  task_loss 2.78 \n",
      "step   99/100: prior_loss 0.00  task_loss 3.06 \n",
      "step  100/100: prior_loss 0.00  task_loss 3.43 \n",
      "Elapsed: 3955.5 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modeconstant-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.31 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.14 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.84 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.72 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.46 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.41 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.25 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.96 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.66 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   34/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   35/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   36/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   37/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   38/100: prior_loss 0.00  task_loss 7.50 \n",
      "step   39/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   40/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   41/100: prior_loss 0.00  task_loss 7.46 \n",
      "step   42/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   43/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   44/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   45/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   46/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   47/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   48/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   49/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   50/100: prior_loss 0.00  task_loss 7.39 \n",
      "step   51/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   52/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   53/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   54/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   55/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   56/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   57/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   58/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   59/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   60/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   61/100: prior_loss 0.00  task_loss 7.42 \n",
      "step   62/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   63/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   64/100: prior_loss 0.00  task_loss 7.50 \n",
      "step   65/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   66/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   67/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   68/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   69/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   70/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   72/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   74/100: prior_loss 0.00  task_loss 7.68 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.71 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.90 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.00 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.18 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.35 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.45 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.50 \n",
      "Elapsed: 4099.9 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modeconstant-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.28 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.25 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.22 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.00 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.18 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.29 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.29 \n",
      "Elapsed: 4679.0 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modeconstant-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.63 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.62 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.35 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.73 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.18 \n",
      "step    7/100: prior_loss 0.00  task_loss 6.93 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.77 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.78 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.16 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.91 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.97 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.85 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.83 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.76 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.78 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.67 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.71 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.40 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.47 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   59/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   65/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   66/100: prior_loss 0.00  task_loss 5.33 \n",
      "step   67/100: prior_loss 0.00  task_loss 5.47 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   70/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   71/100: prior_loss 0.00  task_loss 5.58 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.81 \n",
      "step   73/100: prior_loss 0.00  task_loss 6.00 \n",
      "step   74/100: prior_loss 0.00  task_loss 5.71 \n",
      "step   75/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   76/100: prior_loss 0.00  task_loss 5.75 \n",
      "step   77/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   78/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   79/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   80/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.83 \n",
      "step   83/100: prior_loss 0.00  task_loss 6.12 \n",
      "step   84/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   85/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   86/100: prior_loss 0.00  task_loss 6.71 \n",
      "step   87/100: prior_loss 0.00  task_loss 5.89 \n",
      "step   88/100: prior_loss 0.00  task_loss 6.84 \n",
      "step   89/100: prior_loss 0.00  task_loss 6.74 \n",
      "step   90/100: prior_loss 0.00  task_loss 6.35 \n",
      "step   91/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   92/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   93/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   94/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   95/100: prior_loss 0.00  task_loss 6.98 \n",
      "step   96/100: prior_loss 0.00  task_loss 6.88 \n",
      "step   97/100: prior_loss 0.00  task_loss 6.79 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.01 \n",
      "step   99/100: prior_loss 0.00  task_loss 6.95 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.39 \n",
      "Elapsed: 4775.4 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modemini-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.94 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.19\n",
      "step    3/100: prior_loss 0.00  task_loss 9.94 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.99 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.10 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.80 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.60 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.00 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.27 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.75 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.80 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.90 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.19 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.35 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.46 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.03 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.12 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.32 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.54 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.33 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.67 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.02\n",
      "step   42/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.06\n",
      "step   45/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   47/100: prior_loss 0.00  task_loss 10.30\n",
      "step   48/100: prior_loss 0.00  task_loss 10.14\n",
      "step   49/100: prior_loss 0.00  task_loss 10.11\n",
      "step   50/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   58/100: prior_loss 0.00  task_loss 10.88\n",
      "step   59/100: prior_loss 0.00  task_loss 10.46\n",
      "step   60/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   61/100: prior_loss 0.00  task_loss 10.53\n",
      "step   62/100: prior_loss 0.00  task_loss 10.89\n",
      "step   63/100: prior_loss 0.00  task_loss 10.06\n",
      "step   64/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   69/100: prior_loss 0.00  task_loss 10.15\n",
      "step   70/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.18\n",
      "step   81/100: prior_loss 0.00  task_loss 10.20\n",
      "step   82/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   89/100: prior_loss 0.00  task_loss 10.34\n",
      "step   90/100: prior_loss 0.00  task_loss 11.25\n",
      "step   91/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   94/100: prior_loss 0.00  task_loss 10.15\n",
      "step   95/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   97/100: prior_loss 0.00  task_loss 10.48\n",
      "step   98/100: prior_loss 0.00  task_loss 10.49\n",
      "step   99/100: prior_loss 0.00  task_loss 9.99 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.70 \n",
      "Elapsed: 4875.1 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modemini-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.71 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.40 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.81 \n",
      "step    5/100: prior_loss 0.00  task_loss 10.00\n",
      "step    6/100: prior_loss 0.00  task_loss 10.34\n",
      "step    7/100: prior_loss 0.00  task_loss 10.12\n",
      "step    8/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    9/100: prior_loss 0.00  task_loss 10.07\n",
      "step   10/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   17/100: prior_loss 0.00  task_loss 10.18\n",
      "step   18/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.34\n",
      "step   21/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   30/100: prior_loss 0.00  task_loss 10.72\n",
      "step   31/100: prior_loss 0.00  task_loss 10.65\n",
      "step   32/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.58\n",
      "step   35/100: prior_loss 0.00  task_loss 10.27\n",
      "step   36/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.21\n",
      "step   38/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   39/100: prior_loss 0.00  task_loss 10.63\n",
      "step   40/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.43\n",
      "step   42/100: prior_loss 0.00  task_loss 10.60\n",
      "step   43/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.10\n",
      "step   46/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   47/100: prior_loss 0.00  task_loss 10.68\n",
      "step   48/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.02\n",
      "step   52/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   53/100: prior_loss 0.00  task_loss 10.14\n",
      "step   54/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.00\n",
      "step   60/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   62/100: prior_loss 0.00  task_loss 10.15\n",
      "step   63/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   64/100: prior_loss 0.00  task_loss 10.27\n",
      "step   65/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   66/100: prior_loss 0.00  task_loss 10.26\n",
      "step   67/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   69/100: prior_loss 0.00  task_loss 10.22\n",
      "step   70/100: prior_loss 0.00  task_loss 10.02\n",
      "step   71/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   75/100: prior_loss 0.00  task_loss 10.17\n",
      "step   76/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   78/100: prior_loss 0.00  task_loss 10.40\n",
      "step   79/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   82/100: prior_loss 0.00  task_loss 10.02\n",
      "step   83/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   85/100: prior_loss 0.00  task_loss 10.07\n",
      "step   86/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   87/100: prior_loss 0.00  task_loss 10.41\n",
      "step   88/100: prior_loss 0.00  task_loss 10.56\n",
      "step   89/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   91/100: prior_loss 0.00  task_loss 10.35\n",
      "step   92/100: prior_loss 0.00  task_loss 10.27\n",
      "step   93/100: prior_loss 0.00  task_loss 10.05\n",
      "step   94/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   95/100: prior_loss 0.00  task_loss 10.32\n",
      "step   96/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.44 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.45 \n",
      "Elapsed: 5026.2 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modemini-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.20 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.88 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.88 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   19/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   20/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.66 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.71 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.46 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.80 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.67 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.04 \n",
      "Elapsed: 5615.1 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-05-modemini-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.13 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.20 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.38 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.14 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.03 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.53 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.26 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.15 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.07 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.89 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.46 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.29 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.20 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.16 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.00 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.95 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.89 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.85 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.80 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.66 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.45 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.38 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.31 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.25 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.19 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.47 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.27 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.23 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.22 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.11 \n",
      "step  100/100: prior_loss 0.00  task_loss 4.11 \n",
      "Elapsed: 5717.8 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modelinear-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.43 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.08 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.35 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.55 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.39 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.14 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.15 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.73 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.42\n",
      "step   22/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.16\n",
      "step   24/100: prior_loss 0.00  task_loss 10.01\n",
      "step   25/100: prior_loss 0.00  task_loss 10.14\n",
      "step   26/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.21\n",
      "step   28/100: prior_loss 0.00  task_loss 10.33\n",
      "step   29/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   34/100: prior_loss 0.00  task_loss 10.18\n",
      "step   35/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   46/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   52/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.85 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.47 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   66/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   67/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.06 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   71/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.09 \n",
      "step   73/100: prior_loss 0.00  task_loss 7.95 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   75/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   76/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   77/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   78/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   79/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   80/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.48 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.48 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.51 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.46 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.34 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.43 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.55 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.76 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.73 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.76 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.80 \n",
      "Elapsed: 5813.5 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modelinear-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.13\n",
      "step    2/100: prior_loss 0.00  task_loss 9.99 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.39 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.68 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.61 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.51\n",
      "step    7/100: prior_loss 0.00  task_loss 8.52 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.81 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.07\n",
      "step   11/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.34\n",
      "step   21/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   22/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.29\n",
      "step   27/100: prior_loss 0.00  task_loss 10.05\n",
      "step   28/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.04\n",
      "step   30/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.74\n",
      "step   32/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.43\n",
      "step   37/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.06\n",
      "step   44/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.96 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.31 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.27 \n",
      "Elapsed: 5961.1 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modelinear-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.42 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.68 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.67 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.55 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.54 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.41\n",
      "step    8/100: prior_loss 0.00  task_loss 10.28\n",
      "step    9/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   10/100: prior_loss 0.00  task_loss 10.04\n",
      "step   11/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.30\n",
      "step   15/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.13\n",
      "step   21/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.42\n",
      "step   23/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.11\n",
      "step   25/100: prior_loss 0.00  task_loss 10.87\n",
      "step   26/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.09\n",
      "step   32/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.12\n",
      "step   37/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   47/100: prior_loss 0.00  task_loss 10.03\n",
      "step   48/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.72 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.61 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.32 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.70 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.74 \n",
      "Elapsed: 6549.3 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modelinear-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.31 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.30 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.48 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.97 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.58 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.27 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.03 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.86 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.66 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.44 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.13 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.92 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.76 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.07 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.46 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   33/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.91 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.33 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.26 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.18 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.00 \n",
      "step   47/100: prior_loss 0.00  task_loss 2.88 \n",
      "step   48/100: prior_loss 0.00  task_loss 2.81 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.57 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.48 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.41 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.35 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.29 \n",
      "step   56/100: prior_loss 0.00  task_loss 2.24 \n",
      "step   57/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   58/100: prior_loss 0.00  task_loss 2.11 \n",
      "step   59/100: prior_loss 0.00  task_loss 2.05 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.02 \n",
      "step   61/100: prior_loss 0.00  task_loss 1.94 \n",
      "step   62/100: prior_loss 0.00  task_loss 1.89 \n",
      "step   63/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   64/100: prior_loss 0.00  task_loss 1.78 \n",
      "step   65/100: prior_loss 0.00  task_loss 1.74 \n",
      "step   66/100: prior_loss 0.00  task_loss 1.68 \n",
      "step   67/100: prior_loss 0.00  task_loss 1.63 \n",
      "step   68/100: prior_loss 0.00  task_loss 1.57 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.54 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.49 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.45 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.40 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.37 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.29 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.24 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.19 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.19 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.17 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.13 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.09 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.05 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.04 \n",
      "step   84/100: prior_loss 0.00  task_loss 0.97 \n",
      "step   85/100: prior_loss 0.00  task_loss 0.94 \n",
      "step   86/100: prior_loss 0.00  task_loss 0.93 \n",
      "step   87/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   88/100: prior_loss 0.00  task_loss 0.87 \n",
      "step   89/100: prior_loss 0.00  task_loss 0.85 \n",
      "step   90/100: prior_loss 0.00  task_loss 0.84 \n",
      "step   91/100: prior_loss 0.00  task_loss 0.83 \n",
      "step   92/100: prior_loss 0.00  task_loss 0.81 \n",
      "step   93/100: prior_loss 0.00  task_loss 0.80 \n",
      "step   94/100: prior_loss 0.00  task_loss 0.79 \n",
      "step   95/100: prior_loss 0.00  task_loss 0.78 \n",
      "step   96/100: prior_loss 0.00  task_loss 0.78 \n",
      "step   97/100: prior_loss 0.00  task_loss 0.78 \n",
      "step   98/100: prior_loss 0.00  task_loss 0.77 \n",
      "step   99/100: prior_loss 0.00  task_loss 0.77 \n",
      "step  100/100: prior_loss 0.00  task_loss 0.78 \n",
      "Elapsed: 6652.8 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modeconstant-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.82 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.30 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.82 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.41 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.07 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.80 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.21 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.06 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.87 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.62 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.42 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.32 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.13 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.91 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   33/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   34/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   35/100: prior_loss 0.00  task_loss 3.81 \n",
      "step   36/100: prior_loss 0.00  task_loss 3.72 \n",
      "step   37/100: prior_loss 0.00  task_loss 3.64 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.44 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.07 \n",
      "step   44/100: prior_loss 0.00  task_loss 2.98 \n",
      "step   45/100: prior_loss 0.00  task_loss 2.88 \n",
      "step   46/100: prior_loss 0.00  task_loss 2.79 \n",
      "step   47/100: prior_loss 0.00  task_loss 2.70 \n",
      "step   48/100: prior_loss 0.00  task_loss 2.60 \n",
      "step   49/100: prior_loss 0.00  task_loss 2.48 \n",
      "step   50/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   51/100: prior_loss 0.00  task_loss 2.28 \n",
      "step   52/100: prior_loss 0.00  task_loss 2.20 \n",
      "step   53/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   54/100: prior_loss 0.00  task_loss 2.06 \n",
      "step   55/100: prior_loss 0.00  task_loss 2.01 \n",
      "step   56/100: prior_loss 0.00  task_loss 1.94 \n",
      "step   57/100: prior_loss 0.00  task_loss 1.87 \n",
      "step   58/100: prior_loss 0.00  task_loss 1.85 \n",
      "step   59/100: prior_loss 0.00  task_loss 1.83 \n",
      "step   60/100: prior_loss 0.00  task_loss 1.77 \n",
      "step   61/100: prior_loss 0.00  task_loss 1.75 \n",
      "step   62/100: prior_loss 0.00  task_loss 1.68 \n",
      "step   63/100: prior_loss 0.00  task_loss 1.69 \n",
      "step   64/100: prior_loss 0.00  task_loss 1.62 \n",
      "step   65/100: prior_loss 0.00  task_loss 1.61 \n",
      "step   66/100: prior_loss 0.00  task_loss 1.59 \n",
      "step   67/100: prior_loss 0.00  task_loss 1.58 \n",
      "step   68/100: prior_loss 0.00  task_loss 1.55 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.47 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.44 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.41 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.39 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.37 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.30 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.28 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.24 \n",
      "step   77/100: prior_loss 0.00  task_loss 1.21 \n",
      "step   78/100: prior_loss 0.00  task_loss 1.18 \n",
      "step   79/100: prior_loss 0.00  task_loss 1.14 \n",
      "step   80/100: prior_loss 0.00  task_loss 1.11 \n",
      "step   81/100: prior_loss 0.00  task_loss 1.03 \n",
      "step   82/100: prior_loss 0.00  task_loss 1.00 \n",
      "step   83/100: prior_loss 0.00  task_loss 0.97 \n",
      "step   84/100: prior_loss 0.00  task_loss 0.96 \n",
      "step   85/100: prior_loss 0.00  task_loss 0.92 \n",
      "step   86/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   87/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   88/100: prior_loss 0.00  task_loss 0.87 \n",
      "step   89/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   90/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   91/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   92/100: prior_loss 0.00  task_loss 0.88 \n",
      "step   93/100: prior_loss 0.00  task_loss 0.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 0.90 \n",
      "step   95/100: prior_loss 0.00  task_loss 0.94 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.01 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.09 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.23 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.43 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.77 \n",
      "Elapsed: 6752.6 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modeconstant-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.96 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.91 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.83 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.70 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.58 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.40 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.24 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.99 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.88 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.83 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.79 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.74 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.69 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.64 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.59 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.54 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.38 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.34 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.21 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   30/100: prior_loss 0.00  task_loss 7.14 \n",
      "step   31/100: prior_loss 0.00  task_loss 7.10 \n",
      "step   32/100: prior_loss 0.00  task_loss 7.07 \n",
      "step   33/100: prior_loss 0.00  task_loss 7.04 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.95 \n",
      "step   37/100: prior_loss 0.00  task_loss 6.94 \n",
      "step   38/100: prior_loss 0.00  task_loss 6.92 \n",
      "step   39/100: prior_loss 0.00  task_loss 6.89 \n",
      "step   40/100: prior_loss 0.00  task_loss 6.89 \n",
      "step   41/100: prior_loss 0.00  task_loss 6.84 \n",
      "step   42/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   43/100: prior_loss 0.00  task_loss 6.81 \n",
      "step   44/100: prior_loss 0.00  task_loss 6.80 \n",
      "step   45/100: prior_loss 0.00  task_loss 6.75 \n",
      "step   46/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   47/100: prior_loss 0.00  task_loss 6.76 \n",
      "step   48/100: prior_loss 0.00  task_loss 6.74 \n",
      "step   49/100: prior_loss 0.00  task_loss 6.72 \n",
      "step   50/100: prior_loss 0.00  task_loss 6.69 \n",
      "step   51/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   52/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   53/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   54/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   55/100: prior_loss 0.00  task_loss 6.66 \n",
      "step   56/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   57/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   58/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   59/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   60/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   61/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   62/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   63/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   64/100: prior_loss 0.00  task_loss 6.62 \n",
      "step   65/100: prior_loss 0.00  task_loss 6.61 \n",
      "step   66/100: prior_loss 0.00  task_loss 6.60 \n",
      "step   67/100: prior_loss 0.00  task_loss 6.59 \n",
      "step   68/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   69/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   70/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   71/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   72/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   73/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   74/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   75/100: prior_loss 0.00  task_loss 6.54 \n",
      "step   76/100: prior_loss 0.00  task_loss 6.53 \n",
      "step   77/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   78/100: prior_loss 0.00  task_loss 6.52 \n",
      "step   79/100: prior_loss 0.00  task_loss 6.54 \n",
      "step   80/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   81/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   82/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   83/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   84/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   85/100: prior_loss 0.00  task_loss 6.66 \n",
      "step   86/100: prior_loss 0.00  task_loss 6.70 \n",
      "step   87/100: prior_loss 0.00  task_loss 6.75 \n",
      "step   88/100: prior_loss 0.00  task_loss 6.79 \n",
      "step   89/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   90/100: prior_loss 0.00  task_loss 6.93 \n",
      "step   91/100: prior_loss 0.00  task_loss 7.00 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.08 \n",
      "step   93/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.24 \n",
      "step   95/100: prior_loss 0.00  task_loss 7.35 \n",
      "step   96/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.58 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.80 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.87 \n",
      "Elapsed: 6896.7 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modeconstant-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.95 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.90 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.99 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.14 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.14 \n",
      "Elapsed: 7468.9 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modeconstant-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.89 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.07\n",
      "step    3/100: prior_loss 0.00  task_loss 10.09\n",
      "step    4/100: prior_loss 0.00  task_loss 9.94 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.75 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.62 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.17 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.85 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.68 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.24 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.04 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.50 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.28 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.21 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.03 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.02 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.93 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.91 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.63 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.58 \n",
      "step   31/100: prior_loss 0.00  task_loss 6.51 \n",
      "step   32/100: prior_loss 0.00  task_loss 6.42 \n",
      "step   33/100: prior_loss 0.00  task_loss 6.32 \n",
      "step   34/100: prior_loss 0.00  task_loss 6.24 \n",
      "step   35/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   36/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.98 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.92 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.74 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.78 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.33 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   53/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   54/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   55/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   56/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   57/100: prior_loss 0.00  task_loss 5.15 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   60/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   61/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   63/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   64/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.77 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.95 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   83/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   87/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   88/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   89/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   90/100: prior_loss 0.00  task_loss 5.24 \n",
      "step   91/100: prior_loss 0.00  task_loss 5.53 \n",
      "step   92/100: prior_loss 0.00  task_loss 5.68 \n",
      "step   93/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   94/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   95/100: prior_loss 0.00  task_loss 5.67 \n",
      "step   96/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   97/100: prior_loss 0.00  task_loss 6.25 \n",
      "step   98/100: prior_loss 0.00  task_loss 6.33 \n",
      "step   99/100: prior_loss 0.00  task_loss 6.53 \n",
      "step  100/100: prior_loss 0.00  task_loss 6.68 \n",
      "Elapsed: 7564.9 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modemini-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.10 \n",
      "step    2/100: prior_loss 0.00  task_loss 7.89 \n",
      "step    3/100: prior_loss 0.00  task_loss 7.83 \n",
      "step    4/100: prior_loss 0.00  task_loss 7.99 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.50 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.29 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.14 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.03 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.68 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.64 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.69 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.62 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.57 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.45 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.67 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.71 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.64 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.92 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.85 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.83 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.16 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   28/100: prior_loss 0.00  task_loss 8.42 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.01 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.10 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.75 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   40/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.62 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.29\n",
      "step   52/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   53/100: prior_loss 0.00  task_loss 10.53\n",
      "step   54/100: prior_loss 0.00  task_loss 10.65\n",
      "step   55/100: prior_loss 0.00  task_loss 10.23\n",
      "step   56/100: prior_loss 0.00  task_loss 11.09\n",
      "step   57/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   58/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.03\n",
      "step   60/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.51 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.34 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.13 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   84/100: prior_loss 0.00  task_loss 10.24\n",
      "step   85/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.33\n",
      "step   87/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   89/100: prior_loss 0.00  task_loss 10.68\n",
      "step   90/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.09 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.93 \n",
      "Elapsed: 7663.2 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modemini-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.97 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.45 \n",
      "step    4/100: prior_loss 0.00  task_loss 10.27\n",
      "step    5/100: prior_loss 0.00  task_loss 10.15\n",
      "step    6/100: prior_loss 0.00  task_loss 10.00\n",
      "step    7/100: prior_loss 0.00  task_loss 9.61 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.04 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.52 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.21\n",
      "step   13/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   14/100: prior_loss 0.00  task_loss 10.01\n",
      "step   15/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.34\n",
      "step   19/100: prior_loss 0.00  task_loss 10.64\n",
      "step   20/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.12\n",
      "step   23/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.55\n",
      "step   25/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   28/100: prior_loss 0.00  task_loss 10.07\n",
      "step   29/100: prior_loss 0.00  task_loss 10.31\n",
      "step   30/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.14\n",
      "step   34/100: prior_loss 0.00  task_loss 10.60\n",
      "step   35/100: prior_loss 0.00  task_loss 10.28\n",
      "step   36/100: prior_loss 0.00  task_loss 10.19\n",
      "step   37/100: prior_loss 0.00  task_loss 10.25\n",
      "step   38/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.91 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   45/100: prior_loss 0.00  task_loss 10.16\n",
      "step   46/100: prior_loss 0.00  task_loss 10.28\n",
      "step   47/100: prior_loss 0.00  task_loss 10.26\n",
      "step   48/100: prior_loss 0.00  task_loss 9.50 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   51/100: prior_loss 0.00  task_loss 10.12\n",
      "step   52/100: prior_loss 0.00  task_loss 10.11\n",
      "step   53/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   57/100: prior_loss 0.00  task_loss 10.16\n",
      "step   58/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   64/100: prior_loss 0.00  task_loss 10.06\n",
      "step   65/100: prior_loss 0.00  task_loss 10.06\n",
      "step   66/100: prior_loss 0.00  task_loss 10.06\n",
      "step   67/100: prior_loss 0.00  task_loss 10.14\n",
      "step   68/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   69/100: prior_loss 0.00  task_loss 10.43\n",
      "step   70/100: prior_loss 0.00  task_loss 9.60 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   73/100: prior_loss 0.00  task_loss 10.26\n",
      "step   74/100: prior_loss 0.00  task_loss 10.66\n",
      "step   75/100: prior_loss 0.00  task_loss 9.87 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   78/100: prior_loss 0.00  task_loss 10.08\n",
      "step   79/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.29\n",
      "step   81/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   86/100: prior_loss 0.00  task_loss 10.13\n",
      "step   87/100: prior_loss 0.00  task_loss 10.45\n",
      "step   88/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.98 \n",
      "step   90/100: prior_loss 0.00  task_loss 11.17\n",
      "step   91/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   94/100: prior_loss 0.00  task_loss 10.28\n",
      "step   95/100: prior_loss 0.00  task_loss 10.39\n",
      "step   96/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   98/100: prior_loss 0.00  task_loss 10.03\n",
      "step   99/100: prior_loss 0.00  task_loss 10.35\n",
      "step  100/100: prior_loss 0.00  task_loss 9.20 \n",
      "Elapsed: 7813.3 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modemini-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.23 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.20 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.10 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.81 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.98 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.29 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.25 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   21/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   27/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   85/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.13 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.35 \n",
      "Elapsed: 8396.2 s\n",
      "Saving optimization progress video out/diff-dfsteps5e-06-modemini-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 10.05\n",
      "step    2/100: prior_loss 0.00  task_loss 9.93 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.88 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.71 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.51 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.01 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.79 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.57 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.43 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.30 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.19 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.08 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.70 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.61 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.49 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.15 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.00 \n",
      "step   22/100: prior_loss 0.00  task_loss 6.87 \n",
      "step   23/100: prior_loss 0.00  task_loss 6.74 \n",
      "step   24/100: prior_loss 0.00  task_loss 6.62 \n",
      "step   25/100: prior_loss 0.00  task_loss 6.53 \n",
      "step   26/100: prior_loss 0.00  task_loss 6.46 \n",
      "step   27/100: prior_loss 0.00  task_loss 6.31 \n",
      "step   28/100: prior_loss 0.00  task_loss 6.19 \n",
      "step   29/100: prior_loss 0.00  task_loss 6.11 \n",
      "step   30/100: prior_loss 0.00  task_loss 6.00 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.76 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.70 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.46 \n",
      "step   40/100: prior_loss 0.00  task_loss 5.40 \n",
      "step   41/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   42/100: prior_loss 0.00  task_loss 5.31 \n",
      "step   43/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   44/100: prior_loss 0.00  task_loss 5.22 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   46/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   47/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   48/100: prior_loss 0.00  task_loss 5.08 \n",
      "step   49/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   50/100: prior_loss 0.00  task_loss 5.03 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.70 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.39 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.27 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.25 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.22 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.17 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.12 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.11 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.07 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.06 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.07 \n",
      "step  100/100: prior_loss 0.00  task_loss 4.06 \n",
      "Elapsed: 8498.6 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modelinear-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.40 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.27 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.55 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.48 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.66 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.36 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.59 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.57 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.91 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.27 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.87 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.24\n",
      "step   19/100: prior_loss 0.00  task_loss 10.66\n",
      "step   20/100: prior_loss 0.00  task_loss 10.34\n",
      "step   21/100: prior_loss 0.00  task_loss 10.11\n",
      "step   22/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   23/100: prior_loss 0.00  task_loss 10.25\n",
      "step   24/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   26/100: prior_loss 0.00  task_loss 10.37\n",
      "step   27/100: prior_loss 0.00  task_loss 10.02\n",
      "step   28/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.18\n",
      "step   33/100: prior_loss 0.00  task_loss 10.33\n",
      "step   34/100: prior_loss 0.00  task_loss 10.15\n",
      "step   35/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   43/100: prior_loss 0.00  task_loss 10.10\n",
      "step   44/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.93 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   54/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   59/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   64/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   65/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.60 \n",
      "step   67/100: prior_loss 0.00  task_loss 8.59 \n",
      "step   68/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.45 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.40 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.50 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.44 \n",
      "step   74/100: prior_loss 0.00  task_loss 8.63 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.36 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.37 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.03 \n",
      "step   80/100: prior_loss 0.00  task_loss 8.11 \n",
      "step   81/100: prior_loss 0.00  task_loss 7.93 \n",
      "step   82/100: prior_loss 0.00  task_loss 7.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 7.85 \n",
      "step   84/100: prior_loss 0.00  task_loss 7.82 \n",
      "step   85/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   86/100: prior_loss 0.00  task_loss 7.77 \n",
      "step   87/100: prior_loss 0.00  task_loss 7.81 \n",
      "step   88/100: prior_loss 0.00  task_loss 7.75 \n",
      "step   89/100: prior_loss 0.00  task_loss 7.84 \n",
      "step   90/100: prior_loss 0.00  task_loss 7.98 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.05 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.07 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.22 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.17 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.23 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.29 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.33 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.35 \n",
      "Elapsed: 8594.7 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modelinear-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.58 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.20 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    7/100: prior_loss 0.00  task_loss 10.25\n",
      "step    8/100: prior_loss 0.00  task_loss 9.72 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   12/100: prior_loss 0.00  task_loss 9.30 \n",
      "step   13/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.78 \n",
      "step   15/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   16/100: prior_loss 0.00  task_loss 10.32\n",
      "step   17/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   18/100: prior_loss 0.00  task_loss 10.21\n",
      "step   19/100: prior_loss 0.00  task_loss 9.89 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   22/100: prior_loss 0.00  task_loss 10.63\n",
      "step   23/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   29/100: prior_loss 0.00  task_loss 10.00\n",
      "step   30/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   32/100: prior_loss 0.00  task_loss 10.15\n",
      "step   33/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   36/100: prior_loss 0.00  task_loss 10.16\n",
      "step   37/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   39/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.44\n",
      "step   41/100: prior_loss 0.00  task_loss 10.26\n",
      "step   42/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.67 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.79 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.71 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.34 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.38 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.31 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   58/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.45 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   72/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   78/100: prior_loss 0.00  task_loss 8.69 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   83/100: prior_loss 0.00  task_loss 8.65 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   87/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.81 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   98/100: prior_loss 0.00  task_loss 8.74 \n",
      "step   99/100: prior_loss 0.00  task_loss 8.77 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.85 \n",
      "Elapsed: 8741.9 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modelinear-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.67 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.85 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.64 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.05\n",
      "step    7/100: prior_loss 0.00  task_loss 10.30\n",
      "step    8/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    9/100: prior_loss 0.00  task_loss 10.71\n",
      "step   10/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.92 \n",
      "step   12/100: prior_loss 0.00  task_loss 10.14\n",
      "step   13/100: prior_loss 0.00  task_loss 10.52\n",
      "step   14/100: prior_loss 0.00  task_loss 10.23\n",
      "step   15/100: prior_loss 0.00  task_loss 10.01\n",
      "step   16/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.88 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   20/100: prior_loss 0.00  task_loss 10.71\n",
      "step   21/100: prior_loss 0.00  task_loss 10.62\n",
      "step   22/100: prior_loss 0.00  task_loss 9.47 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   24/100: prior_loss 0.00  task_loss 10.32\n",
      "step   25/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.17\n",
      "step   28/100: prior_loss 0.00  task_loss 9.69 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.02\n",
      "step   32/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.59\n",
      "step   34/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.67\n",
      "step   38/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   40/100: prior_loss 0.00  task_loss 10.09\n",
      "step   41/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.04\n",
      "step   47/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.41 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   56/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.96 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.42 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.18 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.28 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   75/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   76/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.22 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   81/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   82/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.78 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   86/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   88/100: prior_loss 0.00  task_loss 8.79 \n",
      "step   89/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.76 \n",
      "step   92/100: prior_loss 0.00  task_loss 8.55 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   94/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   95/100: prior_loss 0.00  task_loss 8.70 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.13 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.08 \n",
      "Elapsed: 9322.6 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modelinear-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.89 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.52 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.13 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.77 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.46 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.21 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.00 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.77 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.55 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.39 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.22 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.07 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.92 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.76 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   19/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.27 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.10 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.81 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.67 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.16 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.09 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.98 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.93 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.90 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.84 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.79 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.72 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.69 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.66 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.57 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.52 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.35 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.30 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.20 \n",
      "step   57/100: prior_loss 0.00  task_loss 3.14 \n",
      "step   58/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   59/100: prior_loss 0.00  task_loss 3.03 \n",
      "step   60/100: prior_loss 0.00  task_loss 2.99 \n",
      "step   61/100: prior_loss 0.00  task_loss 2.93 \n",
      "step   62/100: prior_loss 0.00  task_loss 2.89 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.82 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.77 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.72 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.62 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.57 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.51 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.46 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.43 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.38 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.34 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.29 \n",
      "step   75/100: prior_loss 0.00  task_loss 2.25 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.22 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.17 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.13 \n",
      "step   79/100: prior_loss 0.00  task_loss 2.09 \n",
      "step   80/100: prior_loss 0.00  task_loss 2.06 \n",
      "step   81/100: prior_loss 0.00  task_loss 2.03 \n",
      "step   82/100: prior_loss 0.00  task_loss 2.01 \n",
      "step   83/100: prior_loss 0.00  task_loss 1.96 \n",
      "step   84/100: prior_loss 0.00  task_loss 1.92 \n",
      "step   85/100: prior_loss 0.00  task_loss 1.87 \n",
      "step   86/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   87/100: prior_loss 0.00  task_loss 1.79 \n",
      "step   88/100: prior_loss 0.00  task_loss 1.75 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.73 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.69 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.67 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.66 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.66 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.65 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.64 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.63 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.63 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.63 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.63 \n",
      "Elapsed: 9426.1 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modeconstant-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.03 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.72 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.24 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.80 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.45 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.16 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.90 \n",
      "step    9/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.38 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.13 \n",
      "step   12/100: prior_loss 0.00  task_loss 5.90 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.72 \n",
      "step   14/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   15/100: prior_loss 0.00  task_loss 5.39 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   17/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   18/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   19/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   20/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   23/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   27/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   28/100: prior_loss 0.00  task_loss 4.44 \n",
      "step   29/100: prior_loss 0.00  task_loss 4.39 \n",
      "step   30/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   31/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.26 \n",
      "step   33/100: prior_loss 0.00  task_loss 4.21 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.18 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.15 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.10 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.05 \n",
      "step   38/100: prior_loss 0.00  task_loss 3.99 \n",
      "step   39/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   40/100: prior_loss 0.00  task_loss 3.90 \n",
      "step   41/100: prior_loss 0.00  task_loss 3.85 \n",
      "step   42/100: prior_loss 0.00  task_loss 3.80 \n",
      "step   43/100: prior_loss 0.00  task_loss 3.76 \n",
      "step   44/100: prior_loss 0.00  task_loss 3.72 \n",
      "step   45/100: prior_loss 0.00  task_loss 3.69 \n",
      "step   46/100: prior_loss 0.00  task_loss 3.65 \n",
      "step   47/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   48/100: prior_loss 0.00  task_loss 3.58 \n",
      "step   49/100: prior_loss 0.00  task_loss 3.53 \n",
      "step   50/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   51/100: prior_loss 0.00  task_loss 3.63 \n",
      "step   52/100: prior_loss 0.00  task_loss 3.40 \n",
      "step   53/100: prior_loss 0.00  task_loss 3.37 \n",
      "step   54/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   55/100: prior_loss 0.00  task_loss 3.29 \n",
      "step   56/100: prior_loss 0.00  task_loss 3.24 \n",
      "step   57/100: prior_loss 0.00  task_loss 3.20 \n",
      "step   58/100: prior_loss 0.00  task_loss 3.35 \n",
      "step   59/100: prior_loss 0.00  task_loss 3.14 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.09 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.06 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.02 \n",
      "step   63/100: prior_loss 0.00  task_loss 2.99 \n",
      "step   64/100: prior_loss 0.00  task_loss 2.95 \n",
      "step   65/100: prior_loss 0.00  task_loss 2.91 \n",
      "step   66/100: prior_loss 0.00  task_loss 2.87 \n",
      "step   67/100: prior_loss 0.00  task_loss 2.83 \n",
      "step   68/100: prior_loss 0.00  task_loss 2.80 \n",
      "step   69/100: prior_loss 0.00  task_loss 2.75 \n",
      "step   70/100: prior_loss 0.00  task_loss 2.71 \n",
      "step   71/100: prior_loss 0.00  task_loss 2.65 \n",
      "step   72/100: prior_loss 0.00  task_loss 2.55 \n",
      "step   73/100: prior_loss 0.00  task_loss 2.54 \n",
      "step   74/100: prior_loss 0.00  task_loss 2.49 \n",
      "step   75/100: prior_loss 0.00  task_loss 2.48 \n",
      "step   76/100: prior_loss 0.00  task_loss 2.43 \n",
      "step   77/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   78/100: prior_loss 0.00  task_loss 2.37 \n",
      "step   79/100: prior_loss 0.00  task_loss 2.36 \n",
      "step   80/100: prior_loss 0.00  task_loss 2.30 \n",
      "step   81/100: prior_loss 0.00  task_loss 2.32 \n",
      "step   82/100: prior_loss 0.00  task_loss 2.21 \n",
      "step   83/100: prior_loss 0.00  task_loss 2.22 \n",
      "step   84/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   85/100: prior_loss 0.00  task_loss 2.12 \n",
      "step   86/100: prior_loss 0.00  task_loss 2.05 \n",
      "step   87/100: prior_loss 0.00  task_loss 2.00 \n",
      "step   88/100: prior_loss 0.00  task_loss 2.00 \n",
      "step   89/100: prior_loss 0.00  task_loss 1.95 \n",
      "step   90/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   91/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   92/100: prior_loss 0.00  task_loss 1.85 \n",
      "step   93/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   94/100: prior_loss 0.00  task_loss 1.82 \n",
      "step   95/100: prior_loss 0.00  task_loss 1.81 \n",
      "step   96/100: prior_loss 0.00  task_loss 1.80 \n",
      "step   97/100: prior_loss 0.00  task_loss 1.82 \n",
      "step   98/100: prior_loss 0.00  task_loss 1.82 \n",
      "step   99/100: prior_loss 0.00  task_loss 1.84 \n",
      "step  100/100: prior_loss 0.00  task_loss 1.88 \n",
      "Elapsed: 9523.9 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modeconstant-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.94 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.97 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.88 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.76 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.53 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.16 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.74 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.44 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.19 \n",
      "step   10/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   11/100: prior_loss 0.00  task_loss 6.82 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.65 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.54 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.44 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.36 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.28 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.21 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.15 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.10 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.04 \n",
      "step   21/100: prior_loss 0.00  task_loss 5.99 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.94 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.88 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.82 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.79 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.69 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.54 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.41 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.29 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   36/100: prior_loss 0.00  task_loss 5.17 \n",
      "step   37/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   38/100: prior_loss 0.00  task_loss 5.04 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.87 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.64 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.60 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.52 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.49 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.39 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.26 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.20 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.13 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.08 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.04 \n",
      "step   60/100: prior_loss 0.00  task_loss 3.98 \n",
      "step   61/100: prior_loss 0.00  task_loss 3.94 \n",
      "step   62/100: prior_loss 0.00  task_loss 3.88 \n",
      "step   63/100: prior_loss 0.00  task_loss 3.84 \n",
      "step   64/100: prior_loss 0.00  task_loss 3.78 \n",
      "step   65/100: prior_loss 0.00  task_loss 3.71 \n",
      "step   66/100: prior_loss 0.00  task_loss 3.67 \n",
      "step   67/100: prior_loss 0.00  task_loss 3.62 \n",
      "step   68/100: prior_loss 0.00  task_loss 3.61 \n",
      "step   69/100: prior_loss 0.00  task_loss 3.55 \n",
      "step   70/100: prior_loss 0.00  task_loss 3.54 \n",
      "step   71/100: prior_loss 0.00  task_loss 3.50 \n",
      "step   72/100: prior_loss 0.00  task_loss 3.47 \n",
      "step   73/100: prior_loss 0.00  task_loss 3.42 \n",
      "step   74/100: prior_loss 0.00  task_loss 3.38 \n",
      "step   75/100: prior_loss 0.00  task_loss 3.34 \n",
      "step   76/100: prior_loss 0.00  task_loss 3.28 \n",
      "step   77/100: prior_loss 0.00  task_loss 3.25 \n",
      "step   78/100: prior_loss 0.00  task_loss 3.21 \n",
      "step   79/100: prior_loss 0.00  task_loss 3.17 \n",
      "step   80/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   81/100: prior_loss 0.00  task_loss 3.14 \n",
      "step   82/100: prior_loss 0.00  task_loss 3.12 \n",
      "step   83/100: prior_loss 0.00  task_loss 3.11 \n",
      "step   84/100: prior_loss 0.00  task_loss 3.12 \n",
      "step   85/100: prior_loss 0.00  task_loss 3.09 \n",
      "step   86/100: prior_loss 0.00  task_loss 3.12 \n",
      "step   87/100: prior_loss 0.00  task_loss 3.16 \n",
      "step   88/100: prior_loss 0.00  task_loss 3.23 \n",
      "step   89/100: prior_loss 0.00  task_loss 3.31 \n",
      "step   90/100: prior_loss 0.00  task_loss 3.46 \n",
      "step   91/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   92/100: prior_loss 0.00  task_loss 3.97 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   95/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   96/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   97/100: prior_loss 0.00  task_loss 6.14 \n",
      "step   98/100: prior_loss 0.00  task_loss 6.54 \n",
      "step   99/100: prior_loss 0.00  task_loss 6.82 \n",
      "step  100/100: prior_loss 0.00  task_loss 7.07 \n",
      "Elapsed: 9665.9 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modeconstant-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.19 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.08 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.07 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.09 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.05 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   11/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   33/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   34/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.20 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   55/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   61/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   71/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.09 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.09 \n",
      "Elapsed: 10239.5 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modeconstant-innersteps1000.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 8.56 \n",
      "step    2/100: prior_loss 0.00  task_loss 8.64 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.63 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.65 \n",
      "step    5/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.23 \n",
      "step    7/100: prior_loss 0.00  task_loss 7.90 \n",
      "step    8/100: prior_loss 0.00  task_loss 7.65 \n",
      "step    9/100: prior_loss 0.00  task_loss 7.52 \n",
      "step   10/100: prior_loss 0.00  task_loss 7.26 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.06 \n",
      "step   12/100: prior_loss 0.00  task_loss 6.91 \n",
      "step   13/100: prior_loss 0.00  task_loss 6.73 \n",
      "step   14/100: prior_loss 0.00  task_loss 6.56 \n",
      "step   15/100: prior_loss 0.00  task_loss 6.44 \n",
      "step   16/100: prior_loss 0.00  task_loss 6.34 \n",
      "step   17/100: prior_loss 0.00  task_loss 6.40 \n",
      "step   18/100: prior_loss 0.00  task_loss 6.30 \n",
      "step   19/100: prior_loss 0.00  task_loss 6.27 \n",
      "step   20/100: prior_loss 0.00  task_loss 6.18 \n",
      "step   21/100: prior_loss 0.00  task_loss 6.05 \n",
      "step   22/100: prior_loss 0.00  task_loss 5.84 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.73 \n",
      "step   24/100: prior_loss 0.00  task_loss 5.64 \n",
      "step   25/100: prior_loss 0.00  task_loss 5.55 \n",
      "step   26/100: prior_loss 0.00  task_loss 5.48 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.60 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.59 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.65 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   32/100: prior_loss 0.00  task_loss 5.56 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   34/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   35/100: prior_loss 0.00  task_loss 5.09 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   39/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.69 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   45/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.94 \n",
      "step   51/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   52/100: prior_loss 0.00  task_loss 5.06 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.73 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.80 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.72 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   58/100: prior_loss 0.00  task_loss 5.00 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   62/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.79 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   68/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   69/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   70/100: prior_loss 0.00  task_loss 5.35 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.75 \n",
      "step   72/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   73/100: prior_loss 0.00  task_loss 5.16 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   75/100: prior_loss 0.00  task_loss 5.12 \n",
      "step   76/100: prior_loss 0.00  task_loss 5.14 \n",
      "step   77/100: prior_loss 0.00  task_loss 5.23 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.83 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   81/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   82/100: prior_loss 0.00  task_loss 5.69 \n",
      "step   83/100: prior_loss 0.00  task_loss 5.50 \n",
      "step   84/100: prior_loss 0.00  task_loss 5.37 \n",
      "step   85/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   86/100: prior_loss 0.00  task_loss 5.44 \n",
      "step   87/100: prior_loss 0.00  task_loss 5.49 \n",
      "step   88/100: prior_loss 0.00  task_loss 5.43 \n",
      "step   89/100: prior_loss 0.00  task_loss 5.81 \n",
      "step   90/100: prior_loss 0.00  task_loss 6.46 \n",
      "step   91/100: prior_loss 0.00  task_loss 6.43 \n",
      "step   92/100: prior_loss 0.00  task_loss 7.00 \n",
      "step   93/100: prior_loss 0.00  task_loss 6.43 \n",
      "step   94/100: prior_loss 0.00  task_loss 7.25 \n",
      "step   95/100: prior_loss 0.00  task_loss 6.97 \n",
      "step   96/100: prior_loss 0.00  task_loss 6.99 \n",
      "step   97/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   98/100: prior_loss 0.00  task_loss 7.78 \n",
      "step   99/100: prior_loss 0.00  task_loss 7.89 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.40 \n",
      "Elapsed: 10334.8 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modemini-innersteps1.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.51 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.49 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.77 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.55 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.12 \n",
      "step    6/100: prior_loss 0.00  task_loss 8.87 \n",
      "step    7/100: prior_loss 0.00  task_loss 8.42 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.14 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.21 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.20 \n",
      "step   11/100: prior_loss 0.00  task_loss 7.92 \n",
      "step   12/100: prior_loss 0.00  task_loss 7.60 \n",
      "step   13/100: prior_loss 0.00  task_loss 7.41 \n",
      "step   14/100: prior_loss 0.00  task_loss 7.36 \n",
      "step   15/100: prior_loss 0.00  task_loss 7.47 \n",
      "step   16/100: prior_loss 0.00  task_loss 7.36 \n",
      "step   17/100: prior_loss 0.00  task_loss 7.31 \n",
      "step   18/100: prior_loss 0.00  task_loss 7.56 \n",
      "step   19/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   20/100: prior_loss 0.00  task_loss 7.44 \n",
      "step   21/100: prior_loss 0.00  task_loss 7.63 \n",
      "step   22/100: prior_loss 0.00  task_loss 7.72 \n",
      "step   23/100: prior_loss 0.00  task_loss 7.21 \n",
      "step   24/100: prior_loss 0.00  task_loss 7.22 \n",
      "step   25/100: prior_loss 0.00  task_loss 7.62 \n",
      "step   26/100: prior_loss 0.00  task_loss 7.33 \n",
      "step   27/100: prior_loss 0.00  task_loss 7.37 \n",
      "step   28/100: prior_loss 0.00  task_loss 7.45 \n",
      "step   29/100: prior_loss 0.00  task_loss 7.40 \n",
      "step   30/100: prior_loss 0.00  task_loss 8.64 \n",
      "step   31/100: prior_loss 0.00  task_loss 9.53 \n",
      "step   32/100: prior_loss 0.00  task_loss 8.15 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.48 \n",
      "step   35/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   36/100: prior_loss 0.00  task_loss 8.58 \n",
      "step   37/100: prior_loss 0.00  task_loss 9.27 \n",
      "step   38/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.43 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   42/100: prior_loss 0.00  task_loss 9.81 \n",
      "step   43/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.44 \n",
      "step   45/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   47/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.32 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   53/100: prior_loss 0.00  task_loss 9.76 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.14 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.36 \n",
      "step   57/100: prior_loss 0.00  task_loss 9.33 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   60/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   62/100: prior_loss 0.00  task_loss 8.82 \n",
      "step   63/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   66/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   69/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   70/100: prior_loss 0.00  task_loss 8.73 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   73/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.85 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   77/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   81/100: prior_loss 0.00  task_loss 10.15\n",
      "step   82/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   84/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.55 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.68 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   91/100: prior_loss 0.00  task_loss 9.90 \n",
      "step   92/100: prior_loss 0.00  task_loss 10.89\n",
      "step   93/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   96/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   97/100: prior_loss 0.00  task_loss 10.10\n",
      "step   98/100: prior_loss 0.00  task_loss 10.39\n",
      "step   99/100: prior_loss 0.00  task_loss 10.12\n",
      "step  100/100: prior_loss 0.00  task_loss 9.98 \n",
      "Elapsed: 10433.7 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modemini-innersteps10.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.54 \n",
      "step    2/100: prior_loss 0.00  task_loss 10.21\n",
      "step    3/100: prior_loss 0.00  task_loss 10.64\n",
      "step    4/100: prior_loss 0.00  task_loss 9.77 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.46 \n",
      "step    6/100: prior_loss 0.00  task_loss 10.04\n",
      "step    7/100: prior_loss 0.00  task_loss 9.58 \n",
      "step    8/100: prior_loss 0.00  task_loss 9.37 \n",
      "step    9/100: prior_loss 0.00  task_loss 9.83 \n",
      "step   10/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   11/100: prior_loss 0.00  task_loss 10.11\n",
      "step   12/100: prior_loss 0.00  task_loss 9.94 \n",
      "step   13/100: prior_loss 0.00  task_loss 10.10\n",
      "step   14/100: prior_loss 0.00  task_loss 10.21\n",
      "step   15/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   16/100: prior_loss 0.00  task_loss 9.62 \n",
      "step   17/100: prior_loss 0.00  task_loss 9.58 \n",
      "step   18/100: prior_loss 0.00  task_loss 9.82 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.70 \n",
      "step   21/100: prior_loss 0.00  task_loss 10.08\n",
      "step   22/100: prior_loss 0.00  task_loss 10.07\n",
      "step   23/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.24 \n",
      "step   26/100: prior_loss 0.00  task_loss 9.75 \n",
      "step   27/100: prior_loss 0.00  task_loss 10.33\n",
      "step   28/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   29/100: prior_loss 0.00  task_loss 9.59 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   31/100: prior_loss 0.00  task_loss 10.09\n",
      "step   32/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   33/100: prior_loss 0.00  task_loss 10.30\n",
      "step   34/100: prior_loss 0.00  task_loss 10.01\n",
      "step   35/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.48 \n",
      "step   37/100: prior_loss 0.00  task_loss 10.32\n",
      "step   38/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.73 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.74 \n",
      "step   41/100: prior_loss 0.00  task_loss 10.21\n",
      "step   42/100: prior_loss 0.00  task_loss 10.01\n",
      "step   43/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   44/100: prior_loss 0.00  task_loss 10.24\n",
      "step   45/100: prior_loss 0.00  task_loss 9.19 \n",
      "step   46/100: prior_loss 0.00  task_loss 10.22\n",
      "step   47/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   49/100: prior_loss 0.00  task_loss 10.20\n",
      "step   50/100: prior_loss 0.00  task_loss 9.51 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   52/100: prior_loss 0.00  task_loss 10.17\n",
      "step   53/100: prior_loss 0.00  task_loss 9.99 \n",
      "step   54/100: prior_loss 0.00  task_loss 10.38\n",
      "step   55/100: prior_loss 0.00  task_loss 9.64 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.56 \n",
      "step   57/100: prior_loss 0.00  task_loss 10.13\n",
      "step   58/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   59/100: prior_loss 0.00  task_loss 10.00\n",
      "step   60/100: prior_loss 0.00  task_loss 10.29\n",
      "step   61/100: prior_loss 0.00  task_loss 10.28\n",
      "step   62/100: prior_loss 0.00  task_loss 9.40 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.95 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.65 \n",
      "step   65/100: prior_loss 0.00  task_loss 10.55\n",
      "step   66/100: prior_loss 0.00  task_loss 9.72 \n",
      "step   67/100: prior_loss 0.00  task_loss 10.41\n",
      "step   68/100: prior_loss 0.00  task_loss 9.63 \n",
      "step   69/100: prior_loss 0.00  task_loss 10.19\n",
      "step   70/100: prior_loss 0.00  task_loss 10.03\n",
      "step   71/100: prior_loss 0.00  task_loss 10.72\n",
      "step   72/100: prior_loss 0.00  task_loss 10.07\n",
      "step   73/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   74/100: prior_loss 0.00  task_loss 10.21\n",
      "step   75/100: prior_loss 0.00  task_loss 10.51\n",
      "step   76/100: prior_loss 0.00  task_loss 10.20\n",
      "step   77/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.84 \n",
      "step   79/100: prior_loss 0.00  task_loss 9.57 \n",
      "step   80/100: prior_loss 0.00  task_loss 10.52\n",
      "step   81/100: prior_loss 0.00  task_loss 10.28\n",
      "step   82/100: prior_loss 0.00  task_loss 10.43\n",
      "step   83/100: prior_loss 0.00  task_loss 9.49 \n",
      "step   84/100: prior_loss 0.00  task_loss 11.02\n",
      "step   85/100: prior_loss 0.00  task_loss 10.01\n",
      "step   86/100: prior_loss 0.00  task_loss 10.23\n",
      "step   87/100: prior_loss 0.00  task_loss 9.54 \n",
      "step   88/100: prior_loss 0.00  task_loss 10.35\n",
      "step   89/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   90/100: prior_loss 0.00  task_loss 9.77 \n",
      "step   91/100: prior_loss 0.00  task_loss 10.03\n",
      "step   92/100: prior_loss 0.00  task_loss 9.66 \n",
      "step   93/100: prior_loss 0.00  task_loss 9.80 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.97 \n",
      "step   95/100: prior_loss 0.00  task_loss 10.30\n",
      "step   96/100: prior_loss 0.00  task_loss 9.86 \n",
      "step   97/100: prior_loss 0.00  task_loss 9.37 \n",
      "step   98/100: prior_loss 0.00  task_loss 10.12\n",
      "step   99/100: prior_loss 0.00  task_loss 9.44 \n",
      "step  100/100: prior_loss 0.00  task_loss 9.91 \n",
      "Elapsed: 10583.1 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modemini-innersteps100.mp4\n",
      "step    1/100: prior_loss 0.00  task_loss 9.24 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    3/100: prior_loss 0.00  task_loss 9.16 \n",
      "step    4/100: prior_loss 0.00  task_loss 9.21 \n",
      "step    5/100: prior_loss 0.00  task_loss 9.13 \n",
      "step    6/100: prior_loss 0.00  task_loss 9.23 \n",
      "step    7/100: prior_loss 0.00  task_loss 9.06 \n",
      "step    8/100: prior_loss 0.00  task_loss 8.93 \n",
      "step    9/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   10/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   11/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   12/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   13/100: prior_loss 0.00  task_loss 8.98 \n",
      "step   14/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   15/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   16/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   17/100: prior_loss 0.00  task_loss 8.84 \n",
      "step   18/100: prior_loss 0.00  task_loss 8.77 \n",
      "step   19/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   20/100: prior_loss 0.00  task_loss 9.39 \n",
      "step   21/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   22/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   23/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   24/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   25/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   26/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   27/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   28/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   29/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   30/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   31/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   32/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   33/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   34/100: prior_loss 0.00  task_loss 8.91 \n",
      "step   35/100: prior_loss 0.00  task_loss 9.14 \n",
      "step   36/100: prior_loss 0.00  task_loss 9.35 \n",
      "step   37/100: prior_loss 0.00  task_loss 8.61 \n",
      "step   38/100: prior_loss 0.00  task_loss 8.87 \n",
      "step   39/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   40/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   41/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   42/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   43/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   44/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   45/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   46/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   47/100: prior_loss 0.00  task_loss 8.93 \n",
      "step   48/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   49/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   50/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   51/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   52/100: prior_loss 0.00  task_loss 9.12 \n",
      "step   53/100: prior_loss 0.00  task_loss 8.86 \n",
      "step   54/100: prior_loss 0.00  task_loss 9.17 \n",
      "step   55/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   56/100: prior_loss 0.00  task_loss 9.02 \n",
      "step   57/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   58/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   59/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   60/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   61/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   62/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   63/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   64/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   65/100: prior_loss 0.00  task_loss 9.01 \n",
      "step   66/100: prior_loss 0.00  task_loss 8.92 \n",
      "step   67/100: prior_loss 0.00  task_loss 9.21 \n",
      "step   68/100: prior_loss 0.00  task_loss 9.06 \n",
      "step   69/100: prior_loss 0.00  task_loss 8.94 \n",
      "step   70/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   71/100: prior_loss 0.00  task_loss 8.90 \n",
      "step   72/100: prior_loss 0.00  task_loss 9.15 \n",
      "step   73/100: prior_loss 0.00  task_loss 8.95 \n",
      "step   74/100: prior_loss 0.00  task_loss 9.00 \n",
      "step   75/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   76/100: prior_loss 0.00  task_loss 9.09 \n",
      "step   77/100: prior_loss 0.00  task_loss 9.23 \n",
      "step   78/100: prior_loss 0.00  task_loss 9.04 \n",
      "step   79/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   80/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   81/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   82/100: prior_loss 0.00  task_loss 9.08 \n",
      "step   83/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   84/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   85/100: prior_loss 0.00  task_loss 9.11 \n",
      "step   86/100: prior_loss 0.00  task_loss 9.03 \n",
      "step   87/100: prior_loss 0.00  task_loss 9.13 \n",
      "step   88/100: prior_loss 0.00  task_loss 9.07 \n",
      "step   89/100: prior_loss 0.00  task_loss 9.10 \n",
      "step   90/100: prior_loss 0.00  task_loss 8.97 \n",
      "step   91/100: prior_loss 0.00  task_loss 8.89 \n",
      "step   92/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   93/100: prior_loss 0.00  task_loss 8.88 \n",
      "step   94/100: prior_loss 0.00  task_loss 9.16 \n",
      "step   95/100: prior_loss 0.00  task_loss 9.26 \n",
      "step   96/100: prior_loss 0.00  task_loss 8.99 \n",
      "step   97/100: prior_loss 0.00  task_loss 8.83 \n",
      "step   98/100: prior_loss 0.00  task_loss 9.05 \n",
      "step   99/100: prior_loss 0.00  task_loss 9.00 \n",
      "step  100/100: prior_loss 0.00  task_loss 8.94 \n",
      "Elapsed: 11165.3 s\n",
      "Saving optimization progress video out/diff-dfsteps1e-06-modemini-innersteps1000.mp4\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "stepsizes = [0.00005, 0.00001, 0.000005, 0.000001]\n",
    "time_schedules = [\"linear\", \"constant\", \"mini\"]\n",
    "num_d_steps = [1, 10, 100, 1000]\n",
    "\n",
    "for s in stepsizes:\n",
    "    for mode in time_schedules:\n",
    "        for d in num_d_steps:\n",
    "            # later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "            projected_w_steps = projector.project(\n",
    "                num_images=12,\n",
    "                num_steps=100,\n",
    "                diffusion_time_schedule=mode,\n",
    "                num_diffusion_steps_per_step = d,\n",
    "                diffusion_step_size = s\n",
    "            )\n",
    "\n",
    "            print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
    "            create_video(projected_w_steps, projector, num_rows=3, outdir=\"out\", name=f\"diff-dfsteps{s}-mode{mode}-innersteps{d}.mp4\")\n",
    "            del(projected_w_steps)\n",
    "\n",
    "start_time = perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "273b9cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/100: prior_loss 0.00  task_loss 4.57 \n",
      "step    2/100: prior_loss 0.00  task_loss 4.58 \n",
      "step    3/100: prior_loss 0.00  task_loss 4.43 \n",
      "step    4/100: prior_loss 0.00  task_loss 4.18 \n",
      "step    5/100: prior_loss 0.00  task_loss 3.94 \n",
      "step    6/100: prior_loss 0.00  task_loss 3.74 \n",
      "step    7/100: prior_loss 0.00  task_loss 3.61 \n",
      "step    8/100: prior_loss 0.00  task_loss 3.51 \n",
      "step    9/100: prior_loss 0.00  task_loss 3.42 \n",
      "step   10/100: prior_loss 0.00  task_loss 3.35 \n",
      "step   11/100: prior_loss 0.00  task_loss 3.28 \n",
      "step   12/100: prior_loss 0.00  task_loss 3.21 \n",
      "step   13/100: prior_loss 0.00  task_loss 3.15 \n",
      "step   14/100: prior_loss 0.00  task_loss 3.08 \n",
      "step   15/100: prior_loss 0.00  task_loss 2.99 \n",
      "step   16/100: prior_loss 0.00  task_loss 2.89 \n",
      "step   17/100: prior_loss 0.00  task_loss 2.78 \n",
      "step   18/100: prior_loss 0.00  task_loss 2.69 \n",
      "step   19/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   20/100: prior_loss 0.00  task_loss 2.66 \n",
      "step   21/100: prior_loss 0.00  task_loss 2.63 \n",
      "step   22/100: prior_loss 0.00  task_loss 2.57 \n",
      "step   23/100: prior_loss 0.00  task_loss 2.51 \n",
      "step   24/100: prior_loss 0.00  task_loss 2.46 \n",
      "step   25/100: prior_loss 0.00  task_loss 2.42 \n",
      "step   26/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   27/100: prior_loss 0.00  task_loss 2.35 \n",
      "step   28/100: prior_loss 0.00  task_loss 2.30 \n",
      "step   29/100: prior_loss 0.00  task_loss 2.27 \n",
      "step   30/100: prior_loss 0.00  task_loss 2.22 \n",
      "step   31/100: prior_loss 0.00  task_loss 2.18 \n",
      "step   32/100: prior_loss 0.00  task_loss 2.14 \n",
      "step   33/100: prior_loss 0.00  task_loss 2.11 \n",
      "step   34/100: prior_loss 0.00  task_loss 2.07 \n",
      "step   35/100: prior_loss 0.00  task_loss 2.02 \n",
      "step   36/100: prior_loss 0.00  task_loss 1.99 \n",
      "step   37/100: prior_loss 0.00  task_loss 1.96 \n",
      "step   38/100: prior_loss 0.00  task_loss 1.93 \n",
      "step   39/100: prior_loss 0.00  task_loss 1.90 \n",
      "step   40/100: prior_loss 0.00  task_loss 1.88 \n",
      "step   41/100: prior_loss 0.00  task_loss 1.84 \n",
      "step   42/100: prior_loss 0.00  task_loss 1.82 \n",
      "step   43/100: prior_loss 0.00  task_loss 1.79 \n",
      "step   44/100: prior_loss 0.00  task_loss 1.76 \n",
      "step   45/100: prior_loss 0.00  task_loss 1.73 \n",
      "step   46/100: prior_loss 0.00  task_loss 1.71 \n",
      "step   47/100: prior_loss 0.00  task_loss 1.68 \n",
      "step   48/100: prior_loss 0.00  task_loss 1.66 \n",
      "step   49/100: prior_loss 0.00  task_loss 1.63 \n",
      "step   50/100: prior_loss 0.00  task_loss 1.61 \n",
      "step   51/100: prior_loss 0.00  task_loss 1.58 \n",
      "step   52/100: prior_loss 0.00  task_loss 1.56 \n",
      "step   53/100: prior_loss 0.00  task_loss 1.53 \n",
      "step   54/100: prior_loss 0.00  task_loss 1.51 \n",
      "step   55/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   56/100: prior_loss 0.00  task_loss 1.46 \n",
      "step   57/100: prior_loss 0.00  task_loss 1.43 \n",
      "step   58/100: prior_loss 0.00  task_loss 1.41 \n",
      "step   59/100: prior_loss 0.00  task_loss 1.38 \n",
      "step   60/100: prior_loss 0.00  task_loss 1.35 \n",
      "step   61/100: prior_loss 0.00  task_loss 1.33 \n",
      "step   62/100: prior_loss 0.00  task_loss 1.30 \n",
      "step   63/100: prior_loss 0.00  task_loss 1.27 \n",
      "step   64/100: prior_loss 0.00  task_loss 1.26 \n",
      "step   65/100: prior_loss 0.00  task_loss 1.23 \n",
      "step   66/100: prior_loss 0.00  task_loss 1.19 \n",
      "step   67/100: prior_loss 0.00  task_loss 1.19 \n",
      "step   68/100: prior_loss 0.00  task_loss 1.17 \n",
      "step   69/100: prior_loss 0.00  task_loss 1.14 \n",
      "step   70/100: prior_loss 0.00  task_loss 1.10 \n",
      "step   71/100: prior_loss 0.00  task_loss 1.09 \n",
      "step   72/100: prior_loss 0.00  task_loss 1.04 \n",
      "step   73/100: prior_loss 0.00  task_loss 1.04 \n",
      "step   74/100: prior_loss 0.00  task_loss 1.03 \n",
      "step   75/100: prior_loss 0.00  task_loss 1.02 \n",
      "step   76/100: prior_loss 0.00  task_loss 1.00 \n",
      "step   77/100: prior_loss 0.00  task_loss 0.98 \n",
      "step   78/100: prior_loss 0.00  task_loss 0.97 \n",
      "step   79/100: prior_loss 0.00  task_loss 0.96 \n",
      "step   80/100: prior_loss 0.00  task_loss 0.94 \n",
      "step   81/100: prior_loss 0.00  task_loss 0.93 \n",
      "step   82/100: prior_loss 0.00  task_loss 0.92 \n",
      "step   83/100: prior_loss 0.00  task_loss 0.90 \n",
      "step   84/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   85/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   86/100: prior_loss 0.00  task_loss 0.88 \n",
      "step   87/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   88/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   89/100: prior_loss 0.00  task_loss 0.84 \n",
      "step   90/100: prior_loss 0.00  task_loss 0.84 \n",
      "step   91/100: prior_loss 0.00  task_loss 0.83 \n",
      "step   92/100: prior_loss 0.00  task_loss 0.83 \n",
      "step   93/100: prior_loss 0.00  task_loss 0.83 \n",
      "step   94/100: prior_loss 0.00  task_loss 0.82 \n",
      "step   95/100: prior_loss 0.00  task_loss 0.82 \n",
      "step   96/100: prior_loss 0.00  task_loss 0.82 \n",
      "step   97/100: prior_loss 0.00  task_loss 0.82 \n",
      "step   98/100: prior_loss 0.00  task_loss 0.81 \n",
      "step   99/100: prior_loss 0.00  task_loss 0.81 \n",
      "step  100/100: prior_loss 0.00  task_loss 0.82 \n",
      "Elapsed: 34.2 s\n",
      "Saving optimization progress video out/diff_constant_100.mp4\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=6,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=\"constant\",\n",
    "    num_diffusion_steps_per_step = 100,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
    "\n",
    "create_video(projected_w_steps, projector, num_rows=3, outdir=\"out\", name=\"diff_constant_100.mp4\")\n",
    "del(projected_w_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc04513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/100: prior_loss 0.00  task_loss 4.34 \n",
      "step    2/100: prior_loss 0.00  task_loss 4.12 \n",
      "step    3/100: prior_loss 0.00  task_loss 4.80 \n",
      "step    4/100: prior_loss 0.00  task_loss 4.78 \n",
      "step    5/100: prior_loss 0.00  task_loss 5.26 \n",
      "step    6/100: prior_loss 0.00  task_loss 4.40 \n",
      "step    7/100: prior_loss 0.00  task_loss 3.57 \n",
      "step    8/100: prior_loss 0.00  task_loss 5.01 \n",
      "step    9/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   10/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   11/100: prior_loss 0.00  task_loss 4.99 \n",
      "step   12/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   13/100: prior_loss 0.00  task_loss 5.05 \n",
      "step   14/100: prior_loss 0.00  task_loss 4.92 \n",
      "step   15/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   16/100: prior_loss 0.00  task_loss 5.20 \n",
      "step   17/100: prior_loss 0.00  task_loss 4.98 \n",
      "step   18/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   19/100: prior_loss 0.00  task_loss 4.63 \n",
      "step   20/100: prior_loss 0.00  task_loss 5.11 \n",
      "step   21/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   22/100: prior_loss 0.00  task_loss 4.88 \n",
      "step   23/100: prior_loss 0.00  task_loss 5.52 \n",
      "step   24/100: prior_loss 0.00  task_loss 4.93 \n",
      "step   25/100: prior_loss 0.00  task_loss 4.85 \n",
      "step   26/100: prior_loss 0.00  task_loss 4.96 \n",
      "step   27/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   28/100: prior_loss 0.00  task_loss 5.28 \n",
      "step   29/100: prior_loss 0.00  task_loss 5.02 \n",
      "step   30/100: prior_loss 0.00  task_loss 5.18 \n",
      "step   31/100: prior_loss 0.00  task_loss 5.31 \n",
      "step   32/100: prior_loss 0.00  task_loss 4.51 \n",
      "step   33/100: prior_loss 0.00  task_loss 5.26 \n",
      "step   34/100: prior_loss 0.00  task_loss 4.65 \n",
      "step   35/100: prior_loss 0.00  task_loss 4.82 \n",
      "step   36/100: prior_loss 0.00  task_loss 4.86 \n",
      "step   37/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   38/100: prior_loss 0.00  task_loss 4.89 \n",
      "step   39/100: prior_loss 0.00  task_loss 5.01 \n",
      "step   40/100: prior_loss 0.00  task_loss 4.84 \n",
      "step   41/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   42/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   43/100: prior_loss 0.00  task_loss 4.90 \n",
      "step   44/100: prior_loss 0.00  task_loss 4.66 \n",
      "step   45/100: prior_loss 0.00  task_loss 4.61 \n",
      "step   46/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   47/100: prior_loss 0.00  task_loss 4.59 \n",
      "step   48/100: prior_loss 0.00  task_loss 4.57 \n",
      "step   49/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   50/100: prior_loss 0.00  task_loss 4.68 \n",
      "step   51/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   52/100: prior_loss 0.00  task_loss 4.44 \n",
      "step   53/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   54/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   55/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   56/100: prior_loss 0.00  task_loss 4.76 \n",
      "step   57/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   58/100: prior_loss 0.00  task_loss 4.62 \n",
      "step   59/100: prior_loss 0.00  task_loss 4.54 \n",
      "step   60/100: prior_loss 0.00  task_loss 4.74 \n",
      "step   61/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   62/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   63/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   64/100: prior_loss 0.00  task_loss 4.50 \n",
      "step   65/100: prior_loss 0.00  task_loss 4.49 \n",
      "step   66/100: prior_loss 0.00  task_loss 4.48 \n",
      "step   67/100: prior_loss 0.00  task_loss 4.24 \n",
      "step   68/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   69/100: prior_loss 0.00  task_loss 4.58 \n",
      "step   70/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   71/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   72/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   73/100: prior_loss 0.00  task_loss 4.45 \n",
      "step   74/100: prior_loss 0.00  task_loss 4.43 \n",
      "step   75/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   76/100: prior_loss 0.00  task_loss 4.28 \n",
      "step   77/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   78/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   79/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   80/100: prior_loss 0.00  task_loss 4.39 \n",
      "step   81/100: prior_loss 0.00  task_loss 4.34 \n",
      "step   82/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   83/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   84/100: prior_loss 0.00  task_loss 4.31 \n",
      "step   85/100: prior_loss 0.00  task_loss 4.37 \n",
      "step   86/100: prior_loss 0.00  task_loss 4.40 \n",
      "step   87/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   88/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   89/100: prior_loss 0.00  task_loss 4.32 \n",
      "step   90/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   91/100: prior_loss 0.00  task_loss 4.36 \n",
      "step   92/100: prior_loss 0.00  task_loss 4.33 \n",
      "step   93/100: prior_loss 0.00  task_loss 4.29 \n",
      "step   94/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   95/100: prior_loss 0.00  task_loss 4.30 \n",
      "step   96/100: prior_loss 0.00  task_loss 4.35 \n",
      "step   97/100: prior_loss 0.00  task_loss 4.38 \n",
      "step   98/100: prior_loss 0.00  task_loss 4.42 \n",
      "step   99/100: prior_loss 0.00  task_loss 4.38 \n",
      "step  100/100: prior_loss 0.00  task_loss 4.34 \n",
      "Elapsed: 39.0 s\n",
      "Saving optimization progress video out/diff_linear_10.mp4\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=6,\n",
    "    num_steps=100,\n",
    "    diffusion_time_schedule=\"linear\",\n",
    "    num_diffusion_steps_per_step=10,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
    "\n",
    "create_video(projected_w_steps, projector, num_rows=3, outdir=\"out\", name=\"diff_linear_10.mp4\")\n",
    "del(projected_w_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70304bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oleksiiv/anaconda3/envs/ganenv8/lib/python3.9/site-packages/torch/nn/modules/module.py:1051: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "step    1/100: prior_loss 1669.74 task_loss 4.70 \n",
      "step    2/100: prior_loss 1669.74 task_loss 4.70 \n",
      "step    3/100: prior_loss 1617.49 task_loss 4.69 \n",
      "step    4/100: prior_loss 1518.80 task_loss 4.69 \n",
      "step    5/100: prior_loss 1384.58 task_loss 4.69 \n",
      "step    6/100: prior_loss 1228.99 task_loss 4.68 \n",
      "step    7/100: prior_loss 1066.99 task_loss 4.66 \n",
      "step    8/100: prior_loss 934.77 task_loss 4.67 \n",
      "step    9/100: prior_loss 825.99 task_loss 4.66 \n",
      "step   10/100: prior_loss 735.29 task_loss 4.66 \n",
      "step   11/100: prior_loss 658.44 task_loss 4.67 \n",
      "step   12/100: prior_loss 592.26 task_loss 4.66 \n",
      "step   13/100: prior_loss 534.34 task_loss 4.66 \n",
      "step   14/100: prior_loss 483.01 task_loss 4.66 \n",
      "step   15/100: prior_loss 437.08 task_loss 4.65 \n",
      "step   16/100: prior_loss 395.74 task_loss 4.63 \n",
      "step   17/100: prior_loss 358.43 task_loss 4.60 \n",
      "step   18/100: prior_loss 324.71 task_loss 4.57 \n",
      "step   19/100: prior_loss 294.24 task_loss 4.54 \n",
      "step   20/100: prior_loss 266.70 task_loss 4.51 \n",
      "step   21/100: prior_loss 241.81 task_loss 4.47 \n",
      "step   22/100: prior_loss 219.36 task_loss 4.45 \n",
      "step   23/100: prior_loss 199.10 task_loss 4.41 \n",
      "step   24/100: prior_loss 180.82 task_loss 4.38 \n",
      "step   25/100: prior_loss 164.31 task_loss 4.35 \n",
      "step   26/100: prior_loss 149.36 task_loss 4.32 \n",
      "step   27/100: prior_loss 135.77 task_loss 4.31 \n",
      "step   28/100: prior_loss 123.40 task_loss 4.30 \n",
      "step   29/100: prior_loss 112.12 task_loss 4.29 \n",
      "step   30/100: prior_loss 101.82 task_loss 4.28 \n",
      "step   31/100: prior_loss 92.41 task_loss 4.29 \n",
      "step   32/100: prior_loss 83.82 task_loss 4.29 \n",
      "step   33/100: prior_loss 75.99 task_loss 4.29 \n",
      "step   34/100: prior_loss 68.86 task_loss 4.29 \n",
      "step   35/100: prior_loss 62.41 task_loss 4.29 \n",
      "step   36/100: prior_loss 56.57 task_loss 4.30 \n",
      "step   37/100: prior_loss 51.31 task_loss 4.30 \n",
      "step   38/100: prior_loss 46.57 task_loss 4.30 \n",
      "step   39/100: prior_loss 42.29 task_loss 4.29 \n",
      "step   40/100: prior_loss 38.44 task_loss 4.29 \n",
      "step   41/100: prior_loss 34.95 task_loss 4.29 \n",
      "step   42/100: prior_loss 31.79 task_loss 4.29 \n",
      "step   43/100: prior_loss 28.92 task_loss 4.29 \n",
      "step   44/100: prior_loss 26.30 task_loss 4.29 \n",
      "step   45/100: prior_loss 23.92 task_loss 4.29 \n",
      "step   46/100: prior_loss 21.75 task_loss 4.29 \n",
      "step   47/100: prior_loss 19.78 task_loss 4.29 \n",
      "step   48/100: prior_loss 18.00 task_loss 4.30 \n",
      "step   49/100: prior_loss 16.38 task_loss 4.30 \n",
      "step   50/100: prior_loss 14.92 task_loss 4.30 \n",
      "step   51/100: prior_loss 13.59 task_loss 4.30 \n",
      "step   52/100: prior_loss 12.39 task_loss 4.30 \n",
      "step   53/100: prior_loss 11.29 task_loss 4.29 \n",
      "step   54/100: prior_loss 10.30 task_loss 4.30 \n",
      "step   55/100: prior_loss 9.39  task_loss 4.29 \n",
      "step   56/100: prior_loss 8.55  task_loss 4.31 \n",
      "step   57/100: prior_loss 7.80  task_loss 4.30 \n",
      "step   58/100: prior_loss 7.11  task_loss 4.30 \n",
      "step   59/100: prior_loss 6.48  task_loss 4.29 \n",
      "step   60/100: prior_loss 5.92  task_loss 4.29 \n",
      "step   61/100: prior_loss 5.40  task_loss 4.30 \n",
      "step   62/100: prior_loss 4.94  task_loss 4.29 \n",
      "step   63/100: prior_loss 4.52  task_loss 4.30 \n",
      "step   64/100: prior_loss 4.14  task_loss 4.29 \n",
      "step   65/100: prior_loss 3.79  task_loss 4.29 \n",
      "step   66/100: prior_loss 3.47  task_loss 4.28 \n",
      "step   67/100: prior_loss 3.19  task_loss 4.29 \n",
      "step   68/100: prior_loss 2.92  task_loss 4.29 \n",
      "step   69/100: prior_loss 2.69  task_loss 4.29 \n",
      "step   70/100: prior_loss 2.47  task_loss 4.30 \n",
      "step   71/100: prior_loss 2.27  task_loss 4.29 \n",
      "step   72/100: prior_loss 2.09  task_loss 4.29 \n",
      "step   73/100: prior_loss 1.93  task_loss 4.29 \n",
      "step   74/100: prior_loss 1.78  task_loss 4.29 \n",
      "step   75/100: prior_loss 1.64  task_loss 4.28 \n",
      "step   76/100: prior_loss 1.51  task_loss 4.28 \n",
      "step   77/100: prior_loss 1.39  task_loss 4.28 \n",
      "step   78/100: prior_loss 1.28  task_loss 4.28 \n",
      "step   79/100: prior_loss 1.18  task_loss 4.28 \n",
      "step   80/100: prior_loss 1.09  task_loss 4.29 \n",
      "step   81/100: prior_loss 1.01  task_loss 4.29 \n",
      "step   82/100: prior_loss 0.94  task_loss 4.29 \n",
      "step   83/100: prior_loss 0.87  task_loss 4.29 \n",
      "step   84/100: prior_loss 0.82  task_loss 4.28 \n",
      "step   85/100: prior_loss 0.77  task_loss 4.29 \n",
      "step   86/100: prior_loss 0.72  task_loss 4.29 \n",
      "step   87/100: prior_loss 0.68  task_loss 4.29 \n",
      "step   88/100: prior_loss 0.65  task_loss 4.29 \n",
      "step   89/100: prior_loss 0.62  task_loss 4.29 \n",
      "step   90/100: prior_loss 0.60  task_loss 4.29 \n",
      "step   91/100: prior_loss 0.58  task_loss 4.28 \n",
      "step   92/100: prior_loss 0.56  task_loss 4.29 \n",
      "step   93/100: prior_loss 0.55  task_loss 4.29 \n",
      "step   94/100: prior_loss 0.54  task_loss 4.30 \n",
      "step   95/100: prior_loss 0.53  task_loss 4.29 \n",
      "step   96/100: prior_loss 0.52  task_loss 4.29 \n",
      "step   97/100: prior_loss 0.52  task_loss 4.29 \n",
      "step   98/100: prior_loss 0.52  task_loss 4.29 \n",
      "step   99/100: prior_loss 0.52  task_loss 4.30 \n",
      "step  100/100: prior_loss 0.52  task_loss 4.29 \n",
      "Elapsed: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(network_pkl, latent_space='w+', device=device)\n",
    "prior = Prior(gen, device=device, prior_type='l2', regularize_w_l2=0.05)\n",
    "\n",
    "blue_target = torch.cat((torch.zeros(2, 512, 512), 255*torch.ones(1, 512, 512)), 0)\n",
    "task = Task(device=device, target=blue_target)\n",
    "projector = Projector(gen, task, prior=prior, device=device)\n",
    "\n",
    "# Optimize projection.\n",
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=6,\n",
    "    num_steps=100,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d63374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 58it [00:59,  1.02s/it, center_shift=0.000000, iteration=58, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/100: prior_loss 221.40 task_loss 4.72 \n",
      "step    2/100: prior_loss 221.40 task_loss 4.72 \n",
      "step    3/100: prior_loss 212.69 task_loss 4.69 \n",
      "step    4/100: prior_loss 196.43 task_loss 4.61 \n",
      "step    5/100: prior_loss 174.78 task_loss 4.42 \n",
      "step    6/100: prior_loss 150.44 task_loss 4.26 \n",
      "step    7/100: prior_loss 126.03 task_loss 4.12 \n",
      "step    8/100: prior_loss 106.88 task_loss 4.05 \n",
      "step    9/100: prior_loss 91.67 task_loss 3.96 \n",
      "step   10/100: prior_loss 79.37 task_loss 3.89 \n",
      "step   11/100: prior_loss 69.20 task_loss 3.83 \n",
      "step   12/100: prior_loss 60.64 task_loss 3.79 \n",
      "step   13/100: prior_loss 53.32 task_loss 3.74 \n",
      "step   14/100: prior_loss 46.98 task_loss 3.70 \n",
      "step   15/100: prior_loss 41.44 task_loss 3.66 \n",
      "step   16/100: prior_loss 36.58 task_loss 3.63 \n",
      "step   17/100: prior_loss 32.31 task_loss 3.61 \n",
      "step   18/100: prior_loss 28.57 task_loss 3.58 \n",
      "step   19/100: prior_loss 25.29 task_loss 3.56 \n",
      "step   20/100: prior_loss 22.41 task_loss 3.54 \n",
      "step   21/100: prior_loss 19.89 task_loss 3.53 \n",
      "step   22/100: prior_loss 17.68 task_loss 3.52 \n",
      "step   23/100: prior_loss 15.76 task_loss 3.50 \n",
      "step   24/100: prior_loss 14.07 task_loss 3.49 \n",
      "step   25/100: prior_loss 12.60 task_loss 3.48 \n",
      "step   26/100: prior_loss 11.31 task_loss 3.48 \n",
      "step   27/100: prior_loss 10.18 task_loss 3.46 \n",
      "step   28/100: prior_loss 9.18  task_loss 3.46 \n",
      "step   29/100: prior_loss 8.29  task_loss 3.46 \n",
      "step   30/100: prior_loss 7.49  task_loss 3.45 \n",
      "step   31/100: prior_loss 6.78  task_loss 3.44 \n",
      "step   32/100: prior_loss 6.14  task_loss 3.44 \n",
      "step   33/100: prior_loss 5.57  task_loss 3.43 \n",
      "step   34/100: prior_loss 5.06  task_loss 3.42 \n",
      "step   35/100: prior_loss 4.63  task_loss 3.39 \n",
      "step   36/100: prior_loss 4.25  task_loss 3.37 \n",
      "step   37/100: prior_loss 3.89  task_loss 3.36 \n",
      "step   38/100: prior_loss 3.57  task_loss 3.35 \n",
      "step   39/100: prior_loss 3.28  task_loss 3.34 \n",
      "step   40/100: prior_loss 3.01  task_loss 3.34 \n",
      "step   41/100: prior_loss 2.78  task_loss 3.35 \n",
      "step   42/100: prior_loss 2.57  task_loss 3.34 \n",
      "step   43/100: prior_loss 2.38  task_loss 3.34 \n",
      "step   44/100: prior_loss 2.20  task_loss 3.34 \n",
      "step   45/100: prior_loss 2.04  task_loss 3.35 \n",
      "step   46/100: prior_loss 1.89  task_loss 3.35 \n",
      "step   47/100: prior_loss 1.76  task_loss 3.37 \n",
      "step   48/100: prior_loss 1.65  task_loss 3.36 \n",
      "step   49/100: prior_loss 1.54  task_loss 3.36 \n",
      "step   50/100: prior_loss 1.44  task_loss 3.35 \n",
      "step   51/100: prior_loss 1.35  task_loss 3.36 \n",
      "step   52/100: prior_loss 1.27  task_loss 3.36 \n",
      "step   53/100: prior_loss 1.20  task_loss 3.35 \n",
      "step   54/100: prior_loss 1.13  task_loss 3.36 \n",
      "step   55/100: prior_loss 1.07  task_loss 3.36 \n",
      "step   56/100: prior_loss 1.01  task_loss 3.36 \n",
      "step   57/100: prior_loss 0.96  task_loss 3.37 \n",
      "step   58/100: prior_loss 0.91  task_loss 3.35 \n",
      "step   59/100: prior_loss 0.88  task_loss 3.36 \n",
      "step   60/100: prior_loss 0.84  task_loss 3.35 \n",
      "step   61/100: prior_loss 0.81  task_loss 3.36 \n",
      "step   62/100: prior_loss 0.78  task_loss 3.36 \n",
      "step   63/100: prior_loss 0.76  task_loss 3.35 \n",
      "step   64/100: prior_loss 0.73  task_loss 3.37 \n",
      "step   65/100: prior_loss 0.70  task_loss 3.36 \n",
      "step   66/100: prior_loss 0.67  task_loss 3.38 \n",
      "step   67/100: prior_loss 0.66  task_loss 3.36 \n",
      "step   68/100: prior_loss 0.65  task_loss 3.36 \n",
      "step   69/100: prior_loss 0.63  task_loss 3.36 \n",
      "step   70/100: prior_loss 0.61  task_loss 3.36 \n",
      "step   71/100: prior_loss 0.59  task_loss 3.37 \n",
      "step   72/100: prior_loss 0.58  task_loss 3.36 \n",
      "step   73/100: prior_loss 0.58  task_loss 3.36 \n",
      "step   74/100: prior_loss 0.58  task_loss 3.35 \n",
      "step   75/100: prior_loss 0.56  task_loss 3.36 \n",
      "step   76/100: prior_loss 0.55  task_loss 3.37 \n",
      "step   77/100: prior_loss 0.53  task_loss 3.38 \n",
      "step   78/100: prior_loss 0.55  task_loss 3.36 \n",
      "step   79/100: prior_loss 0.56  task_loss 3.35 \n",
      "step   80/100: prior_loss 0.55  task_loss 3.35 \n",
      "step   81/100: prior_loss 0.53  task_loss 3.37 \n",
      "step   82/100: prior_loss 0.51  task_loss 3.38 \n",
      "step   83/100: prior_loss 0.51  task_loss 3.39 \n",
      "step   84/100: prior_loss 0.52  task_loss 3.35 \n",
      "step   85/100: prior_loss 0.53  task_loss 3.36 \n",
      "step   86/100: prior_loss 0.53  task_loss 3.37 \n",
      "step   87/100: prior_loss 0.52  task_loss 3.36 \n",
      "step   88/100: prior_loss 0.50  task_loss 3.37 \n",
      "step   89/100: prior_loss 0.49  task_loss 3.37 \n",
      "step   90/100: prior_loss 0.48  task_loss 3.38 \n",
      "step   91/100: prior_loss 0.48  task_loss 3.38 \n",
      "step   92/100: prior_loss 0.49  task_loss 3.37 \n",
      "step   93/100: prior_loss 0.49  task_loss 3.36 \n",
      "step   94/100: prior_loss 0.50  task_loss 3.35 \n",
      "step   95/100: prior_loss 0.50  task_loss 3.35 \n",
      "step   96/100: prior_loss 0.50  task_loss 3.35 \n",
      "step   97/100: prior_loss 0.50  task_loss 3.35 \n",
      "step   98/100: prior_loss 0.49  task_loss 3.35 \n",
      "step   99/100: prior_loss 0.49  task_loss 3.35 \n",
      "step  100/100: prior_loss 0.49  task_loss 3.35 \n",
      "Elapsed: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(network_pkl, latent_space='w+', device=device)\n",
    "prior = Prior(gen, device=device, prior_type='cluster', regularize_cluster_weight=0.01)\n",
    "\n",
    "blue_target = torch.cat((torch.zeros(2, 512, 512), 255*torch.ones(1, 512, 512)), 0)\n",
    "task = Task(device=device, target=blue_target)\n",
    "projector = Projector(gen, task, prior=prior, device=device)\n",
    "\n",
    "# Optimize projection.\n",
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=6,\n",
    "    num_steps=100,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fd100b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 108it [00:21,  5.02it/s, center_shift=0.000000, iteration=108, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/100: prior_loss 141760.52 task_loss 9.03 \n",
      "step    2/100: prior_loss 141760.52 task_loss 8.99 \n",
      "step    3/100: prior_loss 137204.35 task_loss 9.02 \n",
      "step    4/100: prior_loss 128445.52 task_loss 8.98 \n",
      "step    5/100: prior_loss 116174.78 task_loss 9.00 \n",
      "step    6/100: prior_loss 101363.99 task_loss 9.05 \n",
      "step    7/100: prior_loss 85179.39 task_loss 9.01 \n",
      "step    8/100: prior_loss 71358.27 task_loss 8.95 \n",
      "step    9/100: prior_loss 59644.76 task_loss 8.92 \n",
      "step   10/100: prior_loss 49784.50 task_loss 8.96 \n",
      "step   11/100: prior_loss 41538.85 task_loss 9.02 \n",
      "step   12/100: prior_loss 34685.85 task_loss 9.00 \n",
      "step   13/100: prior_loss 29021.61 task_loss 9.05 \n",
      "step   14/100: prior_loss 24363.93 task_loss 9.06 \n",
      "step   15/100: prior_loss 20549.63 task_loss 9.02 \n",
      "step   16/100: prior_loss 17436.30 task_loss 9.09 \n",
      "step   17/100: prior_loss 14900.57 task_loss 9.09 \n",
      "step   18/100: prior_loss 12835.36 task_loss 9.05 \n",
      "step   19/100: prior_loss 11151.67 task_loss 9.05 \n",
      "step   20/100: prior_loss 9775.24 task_loss 9.05 \n",
      "step   21/100: prior_loss 8644.41 task_loss 9.05 \n",
      "step   22/100: prior_loss 7707.47 task_loss 8.99 \n",
      "step   23/100: prior_loss 6923.34 task_loss 8.99 \n",
      "step   24/100: prior_loss 6259.17 task_loss 9.02 \n",
      "step   25/100: prior_loss 5689.48 task_loss 8.95 \n",
      "step   26/100: prior_loss 5194.06 task_loss 9.04 \n",
      "step   27/100: prior_loss 4757.06 task_loss 9.01 \n",
      "step   28/100: prior_loss 4366.19 task_loss 9.02 \n",
      "step   29/100: prior_loss 4012.53 task_loss 8.97 \n",
      "step   30/100: prior_loss 3689.80 task_loss 9.06 \n",
      "step   31/100: prior_loss 3393.14 task_loss 9.05 \n",
      "step   32/100: prior_loss 3118.63 task_loss 9.03 \n",
      "step   33/100: prior_loss 2863.78 task_loss 8.98 \n",
      "step   34/100: prior_loss 2626.42 task_loss 8.96 \n",
      "step   35/100: prior_loss 2405.11 task_loss 8.91 \n",
      "step   36/100: prior_loss 2199.00 task_loss 8.93 \n",
      "step   37/100: prior_loss 2007.75 task_loss 8.94 \n",
      "step   38/100: prior_loss 1830.52 task_loss 8.92 \n",
      "step   39/100: prior_loss 1666.21 task_loss 8.94 \n",
      "step   40/100: prior_loss 1514.72 task_loss 8.91 \n",
      "step   41/100: prior_loss 1375.33 task_loss 8.85 \n",
      "step   42/100: prior_loss 1247.06 task_loss 8.88 \n",
      "step   43/100: prior_loss 1129.57 task_loss 8.98 \n",
      "step   44/100: prior_loss 1021.98 task_loss 8.92 \n",
      "step   45/100: prior_loss 923.51 task_loss 8.94 \n",
      "step   46/100: prior_loss 833.72 task_loss 8.92 \n",
      "step   47/100: prior_loss 752.07 task_loss 8.95 \n",
      "step   48/100: prior_loss 678.28 task_loss 8.94 \n",
      "step   49/100: prior_loss 611.75 task_loss 8.95 \n",
      "step   50/100: prior_loss 551.56 task_loss 8.96 \n",
      "step   51/100: prior_loss 497.30 task_loss 8.97 \n",
      "step   52/100: prior_loss 448.40 task_loss 8.95 \n",
      "step   53/100: prior_loss 404.48 task_loss 8.98 \n",
      "step   54/100: prior_loss 364.91 task_loss 8.95 \n",
      "step   55/100: prior_loss 329.24 task_loss 8.96 \n",
      "step   56/100: prior_loss 297.10 task_loss 8.98 \n",
      "step   57/100: prior_loss 268.15 task_loss 8.94 \n",
      "step   58/100: prior_loss 242.10 task_loss 8.88 \n",
      "step   59/100: prior_loss 218.53 task_loss 8.88 \n",
      "step   60/100: prior_loss 197.36 task_loss 8.91 \n",
      "step   61/100: prior_loss 178.27 task_loss 8.90 \n",
      "step   62/100: prior_loss 161.00 task_loss 8.92 \n",
      "step   63/100: prior_loss 145.54 task_loss 9.00 \n",
      "step   64/100: prior_loss 131.79 task_loss 9.00 \n",
      "step   65/100: prior_loss 119.34 task_loss 8.97 \n",
      "step   66/100: prior_loss 108.14 task_loss 8.95 \n",
      "step   67/100: prior_loss 98.01 task_loss 8.97 \n",
      "step   68/100: prior_loss 88.82 task_loss 8.98 \n",
      "step   69/100: prior_loss 80.52 task_loss 8.94 \n",
      "step   70/100: prior_loss 72.92 task_loss 8.90 \n",
      "step   71/100: prior_loss 66.00 task_loss 8.88 \n",
      "step   72/100: prior_loss 59.77 task_loss 8.86 \n",
      "step   73/100: prior_loss 54.23 task_loss 8.85 \n",
      "step   74/100: prior_loss 49.34 task_loss 8.84 \n",
      "step   75/100: prior_loss 44.87 task_loss 8.84 \n",
      "step   76/100: prior_loss 40.66 task_loss 8.84 \n",
      "step   77/100: prior_loss 36.77 task_loss 8.86 \n",
      "step   78/100: prior_loss 33.36 task_loss 8.86 \n",
      "step   79/100: prior_loss 30.36 task_loss 8.80 \n",
      "step   80/100: prior_loss 27.69 task_loss 8.84 \n",
      "step   81/100: prior_loss 25.15 task_loss 8.87 \n",
      "step   82/100: prior_loss 22.86 task_loss 8.84 \n",
      "step   83/100: prior_loss 20.81 task_loss 8.86 \n",
      "step   84/100: prior_loss 18.93 task_loss 8.86 \n",
      "step   85/100: prior_loss 17.29 task_loss 8.88 \n",
      "step   86/100: prior_loss 15.77 task_loss 8.84 \n",
      "step   87/100: prior_loss 14.38 task_loss 8.83 \n",
      "step   88/100: prior_loss 13.20 task_loss 8.80 \n",
      "step   89/100: prior_loss 12.17 task_loss 8.83 \n",
      "step   90/100: prior_loss 11.22 task_loss 8.84 \n",
      "step   91/100: prior_loss 10.33 task_loss 8.84 \n",
      "step   92/100: prior_loss 9.50  task_loss 8.84 \n",
      "step   93/100: prior_loss 8.80  task_loss 8.83 \n",
      "step   94/100: prior_loss 8.21  task_loss 8.83 \n",
      "step   95/100: prior_loss 7.72  task_loss 8.81 \n",
      "step   96/100: prior_loss 7.36  task_loss 8.79 \n",
      "step   97/100: prior_loss 7.09  task_loss 8.76 \n",
      "step   98/100: prior_loss 6.90  task_loss 8.77 \n",
      "step   99/100: prior_loss 6.80  task_loss 8.77 \n",
      "step  100/100: prior_loss 6.75  task_loss 8.76 \n",
      "Elapsed: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(network_pkl, latent_space='z+', device=device)\n",
    "task = Task(device=device, task_type = 'clip_text', target_str = 'racecar')\n",
    "prior = Prior(gen, device=device, prior_type='cluster', regularize_cluster_weight=1.5, cluster_samples=2000)\n",
    "projector = Projector(gen, task, prior=prior, device=device)\n",
    "\n",
    "# Optimize projection.\n",
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=12,\n",
    "    num_steps=100,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f4c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "step    1/100: prior_loss 0.00  task_loss 9.26 \n",
      "step    2/100: prior_loss 0.00  task_loss 9.27 \n",
      "step    3/100: prior_loss 0.00  task_loss 8.56 \n",
      "step    4/100: prior_loss 0.00  task_loss 8.01 \n",
      "step    5/100: prior_loss 0.00  task_loss 7.51 \n",
      "step    6/100: prior_loss 0.00  task_loss 7.08 \n",
      "step    7/100: prior_loss 0.00  task_loss 6.70 \n",
      "step    8/100: prior_loss 0.00  task_loss 6.33 \n",
      "step    9/100: prior_loss 0.00  task_loss 5.96 \n",
      "step   10/100: prior_loss 0.00  task_loss 5.61 \n",
      "step   11/100: prior_loss 0.00  task_loss 5.36 \n",
      "step   12/100: prior_loss 0.00  task_loss 5.17 \n",
      "step   13/100: prior_loss 0.00  task_loss 4.97 \n",
      "step   14/100: prior_loss 0.00  task_loss 4.78 \n",
      "step   15/100: prior_loss 0.00  task_loss 4.56 \n",
      "step   16/100: prior_loss 0.00  task_loss 4.41 \n",
      "step   17/100: prior_loss 0.00  task_loss 4.26 \n",
      "step   18/100: prior_loss 0.00  task_loss 4.14 \n",
      "step   19/100: prior_loss 0.00  task_loss 4.01 \n",
      "step   20/100: prior_loss 0.00  task_loss 3.89 \n",
      "step   21/100: prior_loss 0.00  task_loss 3.77 \n",
      "step   22/100: prior_loss 0.00  task_loss 3.68 \n",
      "step   23/100: prior_loss 0.00  task_loss 3.58 \n",
      "step   24/100: prior_loss 0.00  task_loss 3.48 \n",
      "step   25/100: prior_loss 0.00  task_loss 3.36 \n",
      "step   26/100: prior_loss 0.00  task_loss 3.24 \n",
      "step   27/100: prior_loss 0.00  task_loss 3.14 \n",
      "step   28/100: prior_loss 0.00  task_loss 3.03 \n",
      "step   29/100: prior_loss 0.00  task_loss 2.92 \n",
      "step   30/100: prior_loss 0.00  task_loss 2.80 \n",
      "step   31/100: prior_loss 0.00  task_loss 2.67 \n",
      "step   32/100: prior_loss 0.00  task_loss 2.57 \n",
      "step   33/100: prior_loss 0.00  task_loss 2.48 \n",
      "step   34/100: prior_loss 0.00  task_loss 2.39 \n",
      "step   35/100: prior_loss 0.00  task_loss 2.30 \n",
      "step   36/100: prior_loss 0.00  task_loss 2.22 \n",
      "step   37/100: prior_loss 0.00  task_loss 2.16 \n",
      "step   38/100: prior_loss 0.00  task_loss 2.10 \n",
      "step   39/100: prior_loss 0.00  task_loss 2.04 \n",
      "step   40/100: prior_loss 0.00  task_loss 1.99 \n",
      "step   41/100: prior_loss 0.00  task_loss 1.93 \n",
      "step   42/100: prior_loss 0.00  task_loss 1.87 \n",
      "step   43/100: prior_loss 0.00  task_loss 1.83 \n",
      "step   44/100: prior_loss 0.00  task_loss 1.78 \n",
      "step   45/100: prior_loss 0.00  task_loss 1.73 \n",
      "step   46/100: prior_loss 0.00  task_loss 1.67 \n",
      "step   47/100: prior_loss 0.00  task_loss 1.61 \n",
      "step   48/100: prior_loss 0.00  task_loss 1.55 \n",
      "step   49/100: prior_loss 0.00  task_loss 1.48 \n",
      "step   50/100: prior_loss 0.00  task_loss 1.42 \n",
      "step   51/100: prior_loss 0.00  task_loss 1.38 \n",
      "step   52/100: prior_loss 0.00  task_loss 1.32 \n",
      "step   53/100: prior_loss 0.00  task_loss 1.25 \n",
      "step   54/100: prior_loss 0.00  task_loss 1.20 \n",
      "step   55/100: prior_loss 0.00  task_loss 1.13 \n",
      "step   56/100: prior_loss 0.00  task_loss 1.07 \n",
      "step   57/100: prior_loss 0.00  task_loss 1.03 \n",
      "step   58/100: prior_loss 0.00  task_loss 0.99 \n",
      "step   59/100: prior_loss 0.00  task_loss 0.98 \n",
      "step   60/100: prior_loss 0.00  task_loss 0.96 \n",
      "step   61/100: prior_loss 0.00  task_loss 0.93 \n",
      "step   62/100: prior_loss 0.00  task_loss 0.91 \n",
      "step   63/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   64/100: prior_loss 0.00  task_loss 0.89 \n",
      "step   65/100: prior_loss 0.00  task_loss 0.86 \n",
      "step   66/100: prior_loss 0.00  task_loss 0.84 \n",
      "step   67/100: prior_loss 0.00  task_loss 0.83 \n",
      "step   68/100: prior_loss 0.00  task_loss 0.81 \n",
      "step   69/100: prior_loss 0.00  task_loss 0.79 \n",
      "step   70/100: prior_loss 0.00  task_loss 0.77 \n",
      "step   71/100: prior_loss 0.00  task_loss 0.75 \n",
      "step   72/100: prior_loss 0.00  task_loss 0.73 \n",
      "step   73/100: prior_loss 0.00  task_loss 0.72 \n",
      "step   74/100: prior_loss 0.00  task_loss 0.71 \n",
      "step   75/100: prior_loss 0.00  task_loss 0.69 \n",
      "step   76/100: prior_loss 0.00  task_loss 0.68 \n",
      "step   77/100: prior_loss 0.00  task_loss 0.67 \n",
      "step   78/100: prior_loss 0.00  task_loss 0.66 \n",
      "step   79/100: prior_loss 0.00  task_loss 0.64 \n",
      "step   80/100: prior_loss 0.00  task_loss 0.63 \n",
      "step   81/100: prior_loss 0.00  task_loss 0.62 \n",
      "step   82/100: prior_loss 0.00  task_loss 0.62 \n",
      "step   83/100: prior_loss 0.00  task_loss 0.61 \n",
      "step   84/100: prior_loss 0.00  task_loss 0.60 \n",
      "step   85/100: prior_loss 0.00  task_loss 0.59 \n",
      "step   86/100: prior_loss 0.00  task_loss 0.58 \n",
      "step   87/100: prior_loss 0.00  task_loss 0.58 \n",
      "step   88/100: prior_loss 0.00  task_loss 0.57 \n",
      "step   89/100: prior_loss 0.00  task_loss 0.57 \n",
      "step   90/100: prior_loss 0.00  task_loss 0.56 \n",
      "step   91/100: prior_loss 0.00  task_loss 0.56 \n",
      "step   92/100: prior_loss 0.00  task_loss 0.56 \n",
      "step   93/100: prior_loss 0.00  task_loss 0.55 \n",
      "step   94/100: prior_loss 0.00  task_loss 0.55 \n",
      "step   95/100: prior_loss 0.00  task_loss 0.55 \n",
      "step   96/100: prior_loss 0.00  task_loss 0.55 \n",
      "step   97/100: prior_loss 0.00  task_loss 0.54 \n",
      "step   98/100: prior_loss 0.00  task_loss 0.54 \n",
      "step   99/100: prior_loss 0.00  task_loss 0.54 \n",
      "step  100/100: prior_loss 0.00  task_loss 0.54 \n",
      "Elapsed: 62.0 s\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(network_pkl, latent_space='z+', device=device)\n",
    "# prior = Prior(gen, device=device, prior_type='cluster', regularize_cluster_weight=1.5, cluster_samples=2000)\n",
    "prior = None\n",
    "blue_target = torch.cat((torch.zeros(2, 512, 512), 255*torch.ones(1, 512, 512)), 0)\n",
    "task = Task(device=device, target=blue_target)\n",
    "projector = Projector(gen, task, prior=prior, device=device)\n",
    "\n",
    "# Optimize projection.\n",
    "start_time = perf_counter()\n",
    "\n",
    "# later TODO mem optimization -> mixed precision, gradient checkpointing, multiGPU \n",
    "projected_w_steps = projector.project(\n",
    "    num_images=12,\n",
    "    num_steps=100,\n",
    ")\n",
    "\n",
    "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19bcc3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b8987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimization progress video \"out/test.mp4\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final projected frame and W vector.\n",
    "target_pil.save(f'{outdir}/target.png')\n",
    "projected_w = projected_w_steps[-1]\n",
    "synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='none')\n",
    "synth_image = (synth_image + 1) * (255/2)\n",
    "synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/projblue.png')\n",
    "np.savez(f'{outdir}/projected_wblue.npz', w=projected_w.unsqueeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c95fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601c0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import imageio\n",
    "import torch\n",
    "from projector import *\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from score import ScoreTrainer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e22762f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    hidden_dim = 4000\n",
    "    num_hidden = 7\n",
    "    hidden_w = 1\n",
    "    hidden_h = 512\n",
    "    normalize = True\n",
    "    network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-f.pkl\"\n",
    "    latent_space = 'w'\n",
    "    # batch_size = 64\n",
    "    # lr = 1e-4\n",
    "    # num_epochs = 500\n",
    "    data = 'gan'\n",
    "    sigma = 25\n",
    "    max_t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92d8167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using group norm group size = 32.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "args = Args\n",
    "\n",
    "generator = Generator(args.network_pkl, device=device, latent_space=args.latent_space)\n",
    "\n",
    "trainer = ScoreTrainer(args, device=device, sigma=args.sigma, im_width=1, im_height=512)\n",
    "trainer.load_network(\"/home/oleksiiv/logs/ominous-wraith-125/epoch_2000_ckpt.pth\")\n",
    "# trainer.train(args.data, args.num_epochs, args.batch_size, args.lr, generator, max_t=args.max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "918a0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimization progress video \"out8/gan_w_test.mp4\"\n",
      "Saving image out8/diff_samples\n"
     ]
    }
   ],
   "source": [
    "num_samples = 4 ** 2\n",
    "num_steps = 100\n",
    "\n",
    "samples = trainer.pc_sampler_seq(num_steps=num_steps, batch_size=num_samples)[:, :, 0] # 100, 16, 1, 1, 512 -> 100, 16, 1, 512\n",
    "outdir = \"out8\"\n",
    "\n",
    "num_rows = 4\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    video = imageio.get_writer(f'{outdir}/gan_w_test.mp4', mode='I', fps=25, codec='libx264', bitrate='16M')\n",
    "    print (f'Saving optimization progress video \"{outdir}/gan_w_test.mp4\"')\n",
    "    for i in range(num_steps):\n",
    "        sub_samples = samples[i]\n",
    "        ims = (generator.latent_to_image(sub_samples)+1)*(255/2)\n",
    "        ims = ims.clamp(0, 255).permute(0, 2, 3, 1).to(torch.uint8).cpu().numpy()\n",
    "        grid_image = einops.rearrange(ims, \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "        video.append_data(grid_image)\n",
    "        # video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n",
    "    video.close()\n",
    "\n",
    "name = \"diff_samples\"\n",
    "print (f'Saving image {outdir}/{name}')\n",
    "with torch.no_grad():\n",
    "    synth_image = generator.latent_to_image(samples[-1])\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "    grid_image = einops.rearrange(synth_image, \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "    plt.imsave(f'{outdir}/{name}.png', grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41e4bf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image out8/diff_samples2\n",
      "torch.Size([16, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "num_samples = 4 ** 2\n",
    "num_steps = 1000\n",
    "\n",
    "samples = trainer.pc_sampler(num_steps=num_steps, batch_size=num_samples)[:, 0] # 100, 16, 1, 1, 512 -> 100, 16, 1, 512\n",
    "outdir = \"out8\"\n",
    "\n",
    "num_rows = 4\n",
    "\n",
    "name = \"diff_samples2\"\n",
    "print (f'Saving image {outdir}/{name}')\n",
    "print(samples.shape)\n",
    "with torch.no_grad():\n",
    "    synth_image = generator.latent_to_image(samples)\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "    grid_image = einops.rearrange(synth_image, \"(n1 n2) h w c-> (n1 h) (n2 w) c\", n1=num_rows)\n",
    "    plt.imsave(f'{outdir}/{name}.png', grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae735d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 322.32it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = trainer.pc_sampler(num_steps=num_steps, batch_size=num_samples)\n",
    "ims = generator.latent_to_image(samples[:,0])*255\n",
    "ims = ims.clamp(0, 255).permute(0, 2, 3, 1).to(torch.uint8).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0336a3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAACfA0lEQVR4nOz9eZyk11Xfj7/vfbZau3rvmenZN+275E3esA0YLxhDsNkcMCQmEEISknzZkgC/JCzfBAIJ+RH8ImyB4JjNOGBsYdkyli1Z60ia0Wik2adneu+uverZ7v39cW5192hG0shYZsivz6jVXVXPVs9zz+ee8znLVdZaNmVTNmVTNor+276ATdmUTbn6ZBMYNmVTNuUS2QSGTdmUTblENoFhUzZlUy6RTWDYlE3ZlEtkExg2ZVM25RJ5RYBBKfV2pdQxpdRxpdSPvRLn2JRN2ZRXTtRXOo9BKeUBzwJfC8wADwPfbq19+it6ok3ZlE15xeSVsBheBRy31p601ibAR4D3vALn2ZRN2ZRXSPxX4JjTwLkNr2eAV7/YDkqpzfTLTdmUV16WrLUTV7LhKwEMVyRKqQ8BH/rbOv+mbMr/H8qZK93wlQCG88CODa+3u/cuEmvth4EPw6bFsCmbcrXJK8ExPAwcUErtUUqFwLcBH38FzrMpm7Ipr5B8xS0Ga22mlPoh4FOAB/ymtfbIV/o8m7Ipm/LKyVc8XPllXcSmK7Epm/LVkEettXdeyYabmY+bsimbcolsAsOmbMqmXCKbwLApm7Ipl8gmMGzKpmzKJbIJDJuyKZtyiWwCw6ZsyqZcIpvAsCmbsimXyCYwbMqmbMolsgkMm7Ipm3KJbALDpmzKplwim8CwKZuyKZfIJjBsyqZsyiWyCQybsimbcolsAsOmbMqmXCKbwLApm7Ipl8gmMGzKpmzKJbIJDJuyKZtyiWwCw6ZsyqZcIpvAsCmbsimXyCYwbMqmbMolsgkMm7Ipm3KJ/K2tRLVR9h84wC/92n8Hrcit9JwHsLk0j9ZaYS0YY9EaPK3wFBgrP9ZajLUYBVaBVgoLWABrUUq5FwoAYy0W0J5GAya3xHFOL05QSuH5kKUQ92MePfQEn/3cfbz3m97Nm199G7044ZnnTnLzdQfppzC/sMhjh57g+htupNttk8YJ27ZMUioVGKtVWW10qDeafP5zn6FWCHnrO97Fc8dPcWDPboIwQnvQ6/c5euxZTp88y7mzp3jPe9/JSG0UjcYPA0qFiCj0WFpa5Rd/9j9w64038Pp3vguUQiuNspoky9C+j1Iwt7DA6vIy+/bsZqhcJSyEaAy9bo/FpUV6vT4GRb3eIEkSzs+dZ2bmPKefO06v3eDNb30rr3vD69m9ezfVYokwDFhaaZHnlnq/wamzZ6hVJ4i8kD/66Ed54sHPsXXnND/1M/+OUm2YTjfjL//sz/jYR38HawxYwy233cGxZ47S73WJCgX+4Q//CMOTO/jN3/pN3nD3G8mMIcXjkYe+SHdljvrMybXx4Xse3/+hD3Hgxhs5uzBPq7FMt9FmuFIjDDS/8eHfoNVuX9FY275tmN37d9FqrTIxNcrKfIO55jLNbofQWno9gzKgUWzbWuG551psbKTu+4q77pxgy/QwS8tdTp+eYff2ERbn27SbKZVIsbBsqXfAuH2GK5qdk9BqG4yBCytgDOSXuT4FjBTBWoUOfHbuHCbp97hm7zi+zjn0XIN+J+b2W69n/zXXMjG5lWJxmLifoVEUPY/E5qw0O/T6PbrdDn1jib2M3/vvv3+FGnmVAENlaIjXvPmNGKVI3HvaKqfUA3VGlFuDrxRaNB2DxVqwiLJnWFAKUGRuN7X2M/hLHlrm3kmTjDzJ6PS6XJhb4OTZGUpjFbZv3cq+O+9gy54DbNm+lZvufiOesuy58WYSYxkfHydQlv233sLMwjJvveladk2MoBWkWULg+azUOxx9+jhfeuABJrbv4NpbbmNqx27q9VVuueVmPO2RZDmTuw6y7/wCjcV55udnuPHWayhXKvR6Xfq9LtNbx+n3T1KojPEt//AfM717F8UoZHZ2gXq9SRiFGBRbJiaZmJ9DKwjCgJFajYnxcXKTobRGWUWv1wdlKBRK5ECz06K+0mB1fpWnnzrEJz7+J3z4V3+dr/36d/JPfuSfs2PnVvpxxtzcAg89+BBnT81zx2sOcOcdd3LTra/i0EMP8tlP/SlHDz/HD/3IP6MQedxwzX4+9+lPEIYVWo0Fnj32DGkiTzdNEnZu3cbbv/GbeOAL9zE1NsYX7/8c73j3+3jm8YeY3LaNJ86fYrC0wdTUJG9561vYsnM3+1sdjh87QuhpyoWIrVu28LnP3Mtjh57CV5C9yEIEnlbsHK/QW16k1+1SnCqybasmrAzz0Jea5D6MVDxGx0L2XTNKfdnw3HOttf337Ciz86DHtddPEOeWE2eXsDksrqxSrgWUTMBrb9vHfV+aYaUjQKWB7VXNnfvLPPpEi/Ntg2fA0xACPQO+hm3DBabGyuzeMsreHUMUSgUmt2xhcmKE8zOzlKISlUqZQD3Nffc/xlvuvJ39B/dTLFfIbYFybYLcaJr1VYbLVWYWFtCej9WaeqdJXv47CAwAoQKjLAqwKAwWtKix+DuDT+RngBYKUG4/hSKw0DMZnvZQKKxaBwaBfo1CkzsgsVh8DcViRK1cYNvoMHt2bmdmcYWTM3OYVo+hapWjTx1jqFji9a+5hZHyEIeOnkSZAGUSykGJvH2ez9zzWb7t3W9jpFoh1D5Zboh7bSqRptVYZWj8NsbHRxkul3h4dpaZM+eoVCso69Gst5gcG8c3lmIY8vhjh3jvN76boR3baLU69HttHnz4UW64804IPLI0xY9KnDpxGi/wuWbfQXytGa5Uyfox3X6X4dowo8M1PKPodmMKhQjfCyiHEVmW4lmDtRDZgJFykbPdU7zxDW9k2+Q2Hnroi/zhR36H2bl5vvMDH+Ta6/cT+j7v+Lq3ce011/Dff/1/0Dg/z3XXX8ttN1/PN3/j1/Ij//xfcc89n+Hd77iboXKZW297Dc88/QS7dx/gyOFH1p61MYZHH36Qt77t6/j773sfv/s/f49nnnqCm66/kZuv2ctnPvVxNq53sv/AQW6+43Z6WQbaR2Hodpo88ciT3HTDTezfO81jh54i8CDLeEEpFSP+3vvew6c+dx+zTy2zstKgHcOO/RPoUNHvWpbTnNGapr7Q5uzJHoGGxE3te3aOctdrdnD02HOcPrfKiWMZnRZu8koph9BonKLeW7cFapHHwW1Vxoam2DKS0o27tPqQWvAUVIoB7//am3ndbQcZqQ2xtLpIZhTtbszY+Da8MGJ4THPqzBlu2X6QavEUhaJHnHapDQ8DitX6KmEUsbDSYnFxmeJ1N8hEoCxxr0fSajJaHX9Z+nhVcQwemgCNbxUaUXTPKbJSoJVGo1EotPvnofDdfgEaH02kPGxmULklsIpgwzbaCsBoFAHgWUXogCLNLRjFeKnMLbt3cNPuXVx7YB+vffVdvOedX8dDDz/FZx94igeeOE65NMxQpcqunds4sHea1915K+OjE/zV5x+m3uqysFJHWdg1OcUt1+9nbPsOKmM1vEARFCLuft1ryHJDmuZgLIuzcyzMzxN4Hgf3H2S0OsyDDz4IyrBtfIhSocyF2Tle/drX8syRp+l3c547eZaR2jBTw+NkcUKv26W+Uifp9xmvjTFSHSbwQvJcEQUl+p2EldU63W6M74ekqaLR6OD7PsVoiP07dnH4qScJopAf+qEf5gd+6F/y2CNf5KGHH+LX/uuvszi3jAp8brj2AN/17e/n1MxpDj15iA//1m9x+Omj/Oi/+mf8wW//BpnxKVVL/JN/+k/Zum0HO3buxveDi571occeJe60uO3WWwl9n363SZy0+Zl/82/4jvd/60Xb7ty1i8Ro2nFOrmDPvr08fvgh7n/g8/z27/wWW7fWKJdCchTlgscLyd7dW9hycIrrXrOLRjvjsSfnWFyc57P3HCbrWKyBOIVnTnQwvQCTKcwGC+TM2VXyDNCaieEa2oKyYI24BtZArxvT7K+jU9EzpP0eS/UeJ+f6LPUgRVxgz9e86/V7uevW/QSlEro0xLmFOs+dmeXsQpOHHn+KZ547TqPTptFq8ql7P8nC8hnaCTz08KP0en3SLKfTbrG6cI6kNU8pMoSeZXSoxmitxt7d+7j+4A1Uo+rL0sWrxmKAgTugscoi871am9XdfwDymbUMzAZrIcdirMFai7IQKY9enpImhigI0Z4nPIS15NZijCXJU3IUnrG0Oj1y62HSjLjXZ2Kshkpjer0udWM4c/I8e/bt4ZN/9dcszc0yMVKlFPi88WvuZt+BvQxVCtz92rs4cuQZ/vqhJzmwextJkjJSqZCbnL5JaXfr1FeXiaIK1XKJW268jjNnZ7j/gYc4ffwkvW7Mm978RjrdFl//tjfxl/fcy6FDR7nr1hv5i7+8h5tvvpXbbriZ5aU5Oo1VVpsdRqtDxL0OvueRZylae+zavp25+SWU8uj1OpTKJcIwpFIsUW+1MGj6qaHZaVMoFyDwCAs+e/fvx2rFA488xtzqEt/+He/n6LGjHDn6NN//vR/kwx/+DT7wwQ9w280HueWWa3jNidt5w913cv/nv8Qv/MIv8YPf/0G8QsgTx57DmozrbtzPD/6Tf8b/99f+G0MjE6wsXlh71idPHOejf/yHHLj+Fr7+nd/CE48/wonjTzN74QyFwjqIKKXYtn0bnX6PTjumsTxPQM4XvvhFXvOq2/jmd76Xa/ft4xOfup+Zc3NEkUenfznvHaZ2D6FHAhqtLnGSMzwBQVmze6LA0cMdjNutUILTM0vMrUBm1vdfbqX85aefYtfuAvXVPnEMlRqkCSR92LOlzDX7d/Kxzz3DYLTW+5bHZ2I6p2cwucIPIkhisHDbNZMc3D+G8i1nLpzDX5rn0NPHWZhfJs4Ue/dM44cRJ8+cYfeenbRbLYZHRzl6ps2jR87zwOfvZXh4lOXlFU489yx+UGDLti0cPfIYp07O8+aveQt791/DhfMLZPrlLfZ2VQCDBVJjycjJjEJ7Hh6W3BisstjckmU5aA1GZlhrLb7nCdFoLVqJlWGt7IfW+L6Pp6Df62MGnyvtSEtDmhtUEGBQhH5AhqWrQgLPo1CImBgbxisWiaoVJrZO4iU5r737dh74/APc91f34UUR/+f3P8LZ2bN0rM+B/QcphwXOz86ya3oL73/fN/Pa266nn6Y0lpbYMj4NqoAXFekmGV5m2b59O7v31pnetp200+Pxpw4xNDzMjh3beOPr7+bjn/gEZ86eZaG+ynve+S6Gq2U6nQ7GKMqhfM9CtUqtWiOJ+8zNz5IkCVumptC+R+BXaXU6rNQbjA3X0FpRGSqhfU11uAhaoZRidaVOs9PnxusOkuUJf/7nn2TH1Ba2btvOhblZDj97kne/95v41f/6X3jTW7+Ot7/1bm69+Vqmt27hu//++7jxhuv4mX//syy1l/jc4w/xmptv5Y8/+Vfcdetr+fvf/8M89sB9/O6v/zLWiqalacojDz3A297xHubmF/m+D/0Qf/zR32L/vl2srjbWxsZQrcpr3/AGYgyLzQXm507TXVlg36597Nm7n3KpyK5t27nx4H5OnDxPlpvLjjHPU9xw2z6ePnaEZ46dxgLlQolvfNddnDp+mrMnu3i+IreGaw56FAOf4Lzi5On+2jHq9ZjG4zFLC22GK4pbbqpSLIQ8e3yV6oRPt5ux1MwIAo8sF6uha6DbsIS+4R9+220UiyV+9Xe+SG4Nt94xzfDkCKcXT3Py5DkayynHn1vGCzQH9m0TUt1Apy2kcavTQumAeqvJ9JYSTx8/wrbxrZw/O8vJE2doNAxR4QjLGZRCD0+nrK4s88QTR4iGyi9LJ68KYMARhoHysG6gemiU1vgolGcxAVhHOiorroBSOJdjnaAUR2Gdp1AEVAtCNKZYx1ms0Zli1mHJixFYRc9mNBod5usr1GojbK8O0Wy30KWI3EtZbrR401vu5p1f+waW622masN0WnU+de8XOD+3wOFDT9BptfnTj/0p9/7lPbztG97K9/zgd9Pt9enHmmOnZpkY7jJUKaG0R6AtO6e38Yl77qFaHmJibIr55QU+98UHeePrX887vuHr+I+/9Mvs2r0XrQOS3DK3vMz0tp2EqSEII9K0R73TJs8y9uzZz9j4CL6n8TwPqzS1sWEWllZYXq0zMTVJUAiI+z1Gq2Vm5hYpRAUiZWhmGU8dO0mxWOGtb3o9Tx8+RqVY4qnHvsSJ40d5/3d9kLd/wzt5+PGHmZud5cnHHuVd73kn3/COr+XW22/jv/3XX+af/vN/xm//4i/S+v4f4Gte9yaOnDqOX/T4+ne+kz/6/d9kcnIL586cIMtSHnnoS9z7mT/nXe/+FsgNqyt1LlyYpbkhwjAyMkKhUuXCwhKBp9m1cyu//L9+l+uuP8Dy8jxeqDi7MMPQ8BDGSOTqchJGilPnZhhLapx6dpZyBaZGRjgwvYezTz/LN3/zTRht+MT/eZpey8Mv1di3b4yTp48C4PswsUUxO2OZP28JdxT4ujcfpBDUWF58jAN7qsydrbOwukpvgyuxb1uRYqTZMjXOXXfexR99/K9BWcbHPB459Az3ff5JlhsZ5dCysgCJhf1bAjqtLkm9y3CtwtnzKxw/c4E3vOlW/uwvvkSnn7G82KS12KC1u86RZ1vEHcNICM1WTmZgrp3z6KFjXJhfZWmpTlgMX5ZGXhXAoAC0hBsHig6DsKWouQbnaCj8DZEKjUU7omqwNK4RRnJtG+uOJZZJTgYESoPSmDwns5ZebsjznMAqto0M0ej4nF9YYnpqjGro4YUhqlalUCwQ5wpdLjAShigPquUJvuM7vpnllRX+4lOTmNTS6L6dz//1fXz8T/43X3r0QRaXF7nh2GG8KKDoB2RpTjvNMGlM0cLU5Ba+8OD9qCQlzXIuzJ0hTnvccv0NtFYaFK6v8ql77+XZZ47whte/mZV6HYtBpZbIC5ldXOSWW25gqFSi3e1TLEbESUYvSQjDkPGRYchzPn///fhhkVtuPECv2WRpdQVlYWx4iOPHn+P8mXPMnDrO8vIyadIni1O2Tk7x2CNf5Ff/4xm279gFKL73Qz/Aq+64nY99/E+x5YD3vfud7N6znZ//uZ/nPe9+B//rl/4ztbDEq9/wRp587HG2HLie0fEJvv27vofP3PMXPPjA5+n3+nzp8/fzg//wH3P+zByTU1vp91OeOPTE2tgYnxjn3Owc0fAIUZbQWZzl3Omz3Hbbfh5/7Cn+OG0wPj7GUmP+knE1VivQ6SUkqeFd77gTTc6RBx7ndXcc4MgTx1hcmONT93ySrVt3kYYpTzx1nkbDELcS3v/eXSzW10GmXI64+aZpZmdOgoFt01WS3LI4P0ee9smTIq9+9e2stA1Pn/i87BMptk4WsL7hlpu28tE/+gu+9NQi7/6G25gY0fzpnz2K8TWrDctSKpEyBTwz28fO9rEWzi93qTdSJidDjhx7miQ3NLuGThcqPtSGYpqxZUdN040Ns4kcIwGene/QiWcp+T69fu9l6eRVAQyg8LBgRVkHM7rwCE7pUQMGAs/aNdZULAhnJ1gktm/XXxpwkQmxFnztYbB04wTlBXhazhV5Pl4YkGcZcZ5Tq5QpRBHzS8u0UBSrZTzfx4YeNrMstTqQWVTo4fuGgrUkCt73re9gcWGV1WaPA/uv5cyb3sb//PX/RMETC6dYLLJ39zayPMcuNrFaMzs3z8zcPLffcRfXXHOQpJvgG8UXHvwiH/zAB1Em4+kjh5navpNSFPH5z36WOMu45uBBpqe3iXvl+5w9e47dO3fTj2NmLpyn3+8TxwmFUhE8jc5zhkol5uYX+J3ffpj5CzOkJqbdajNz6gy99ipJ3GF1eZE0TdaeziBC0D/fZeH8DABPP/kYe/YcoNnq8sl7Ps5nPv1X/Ot//zNMH9zF9/7Ij/Bf//uv8hu/+ossN1a46YY7SeKEA9fezJZtU3z9O97Fgw/cD1iefuJJ5ufOUxsZJkszut0uy8v1tXPfePPNXH/DzczOz+AnCb/+27/FuTMX+PwX7+f6mw8Qq5zl5QvMzpy+aESViwGvuu0aPvm5J5gYq/KB73w/Wd7j3/2HX+ZL9z/H5FiV4ZGUuu5y9MEnaDcSVpYMXgD9FGbn5ymHo2sWaBznnDzVRLmB9/CXFrgw06BYlOlrZrbDc6efYnGl48Yr7NsasLzUojzk8ezREzz0+Ao7pqd462vvoj5/jmu3hRy8fi+HnjrHQ0fba5OiBmLnEZ2e66GB5tmMoQjSrnyQA40Mjp7pkhq4kEPVkzBoZpwqADZJ6fZj+vnfQY5BYQkc2TjgSMwGS2GApOtug8tIsIPcBPdbDbBhQEo6S2MAEo6PMAoqYUQn6ZMbD+15BFqObNB4QJpbQs9n/5YJFlcbnD07x8T4KFFUoBRoEl+TaYUBtOdRbzQ4f+Ys05OjXLtrBymaya1T7N2/g73X7OE3fu2/cerkOW6/A9pZQq1cYM/2cVqdPtWRUZZWVtm1cxe6UEZnmrLnMTq2hW/++x/ibV/7DhZnz7KwcIHxkVGu2bcfqxStep2llVmWV5pcu/8azp46zfLyMlu3bKVarlKt1gjCkHK1DFpzfnaOyXKVm2+7nTAMiLtdDj/1FH/5l5+iv9pksd9k57Y9zFaGSJKc933bB6gOD3Hu9ClWlpaYmhrnnk/+BWfPnKDf7XL0iMzsYRTxqf/1+8zPz/AvfunnePO3vJNPPvLX7Nl/gN//L/+Z7/yBf8473/a13PGaV7GcGr7xW7+DP/2TP+bJxx+iUa9zz7338v5v+26KlSo/9VP/hnpjnWMoFcvoPGP/9m08+cB9PPHkU1x/8w7e/W3fwOkzp7hw6jy7R4dZnO9eNKZyYzn87GmshYkJn1Pnnma4VmZqqsrMhSVqkxN04zrdfsz5uYS4LvtlqVBZz565wIXzZ9cI7zjOOPncElpBsQTtFpw7G284Y5+xYZ+BxV6MQAeWG66fot/LeeTQPN3M4+67DnDkyUPEvTY7dmylUe/iqZBAQ+6Snp5Pkxg31o+d6oO1BFpCpEkOZR+WY1hOoRaC7yL5Q0ChAMu9nNRePpnqxeQqAQYx9TVWlN7NUNbZCcYpvbZ2HQascAWDLVn7xMl6qoMgpxULZJDXoBUMRQX6WcrM6ipJZpgcGSHOIQoDkjQl9xSep5ioDeEXI/A8fCDOLMUgIFUeSZbgeYrhkSEipjl96jznl1fYv2snhVKF8cIwk1tu5tnjb+fxJ5/iT/7kYyzMvZpvfM/XM1IKiIISrZ5i177dKE+zeGGBreOjnD5zhlNnTvOmt72VyZ3j7Nk/zYP3f55SqUiuIM9ytmzbRhT5RGHE0uoyQ5Ua1UqVVrfLuQsXCLyAyelpVttthqpF0iRGoai3mly4cIFut42JU973976VfXv/FUtzF/j85z/L8vIKp86d5+jxZ0ClTG7ZQW18nBtuvpVv+/YP8Nl77+G//sp/ZHV1Gc/zeNWb385b3/uN/On//C1+8kM/wE//yn9iKDC88xvexnOf+Twf/93/wWtfewdbd25natcB/KFhfubn/hPf953fzMryEr//e7/H+7/ju/iXP/rj/KMPfgd5vj6Mh2sVes1Fzh4/x1988hPUGz2GTZf//dGPkrc90qxFvVii0Vm3cAD6cca5Cw18D2pjhmYyj01rTO0fov0gPPDwDGPTisVzlrSzYSz6YDI4fvLihAhrYXBZncskWXoabr9pC48fWQIyjIGRSsiurTv4zIPHmV21eDrnvi8exWQ9hsoBSa6pFuDAni30Wm2WlxOWetC9zORugaFAQ6AopjmTQ4qjczn9WHIiAE53oeZBz4rVkOSy42gIS8l6VO9K5KoABsARis4qcKFIq6xTcIEIrJjja2HK9b03QMPaW/K/Af+wdh45h7GCHKHnMz06wqmZWY6fPEV1eJgwKlAsl1htdTm1ukK30cVqaPYTbrpuH0pbSiUfjMFkBpulqEJItTbO7TfXODk3y9HTpylHJYKwyM4d00SFiPd/8zsZHhrmP//8f2ZxaZkf/AfvYyKMSHSKUQYvimi0znPtrh0cbtR5z3vfSVSuEPmaPMvYNjXNc88+zf6du/CVZnlllbGxCcrlMrWxYUyuKUQRubYsLa9QjiIUHgsri3jWgzhltd2l34/ZuW0bjz/+CKEf0u60mF9cZGZugXd90/tprdbxQs0TTzzN0089Qb1ZZ/bCeR647z4KpQK333EH73zPd/KHH/kfJHGXBz/7SZK8xy1vegMn6meYnBzm2ttv4GO/83v85I/+GB/6wAf4k//1e/ybn/wposoIpy9c4IY77uCH/+VP8rM//eOcee5ZvnD/57n9ptsoForUWV1/jiblc/f+Hz7xiT/n0Ueeo1Dw2HVwjEfuO8WW6Qo3XLcD1fd46snlS8cUcN1NY9z2lmlWkgtgUsa3F5jaGnLuXMLqgoBCVILYgYN9kQSpgQxyr7SzUK0F39dMjU2SposApDkcPZfyxIlHWG1l7Jqqcvt12ygXQu756yO84VU3MzZcYrk+x9lz8yyuJCRWUwgMvcsosed0o+Rb5nuG1BkrAxAJgCENQx7sqcJsHYZKCtWyjFUL6FafuYQrlqsGGLRVLnlpcEvWuYOB62A3qP5a+cPgtewy2NW5Fet1ElZJfsPA3dBWciW0tQRKcf2O7cR5zmKrycnTZ+mkhunpKfbsmMbfqbDGcn55hZWVVfxcEU75hJFP1dP0lE8vMyR5Rpxrpqe3M71rO2kvpdeJ6ZsuYRnmF87x5q97Iz/9H/8dv/zL/51f/PU/4Dve9bXURmucWzrH17zqNeS9hOVmg1tvv4Xy0AhRqUCnl+H5lp07djJULfOFLz3M7TffhMktxWKZ0fExVht1CqWQsBRiLQwNlVldbTI1PsGOwjSdpM/k9u1cW6ugleHs+Tm2Te9iuDokfEsYcs3BayhWhtChzzNPP80dr7ub219/N4H2sFlMv5dw/LnnePzxR8iAr3n7N1NfWeCpxx/ioU/fw0P3fZrtr7qRxPR51xvexg/8yvfy9je9hXe/77382f/6Q06/7wO85vWvpRkXWay3+cZv+nt8+pN/zv1//RkeffABdm6ZIEn6bJQs63Pi9CmOHD6DyS3VYYune1RqGUHY59YbrufoY2foxxfb36XIY/81Vfa/aoSnjp2j2+2zc/cJCqHHG98xxJ/8xjLtZZkc/ALEXS4aUL4nJr19gWlWe1AZEpfCA3buDPnEvU/SaAmyGAP9TkKrDzsnC3zo2+7m+mv2cPbMWe794tOcPHWeetkwN9dkqWWZ7YBx1RWXO6UBtm+pEmtLa7lOP7nYIvaAXg61SK4NBTMNS6Sh2E3YMeIxN3/lDsVLAoNS6jeBdwEL1tob3XujwP8GdgOngfdZa1eVOPe/ArwD6ALfY6197EouRCmFNesK/3ybwG40CawQipcTxz/KMTfkTSsr4U6lkLQzpDDLG/y24CuPqDbM1M01ZpeXeezJZ7hQrnDjrddT8H12TE2itSVPEhaW6gwVS1TLRXKVkQJxluGFAVlmSfqGYhRQKFl837B1fIJnTp2ibXOmt2/hJ//lP+E//Pwv8oUto9zy6ttIspShwIekT6IsU8MT2NxAlqHyFKUDUiMRglffcRuHn3ocnwI3XL8Xi8XTil63SyGK8DyLyhV5krK0ssTk+CTDQYWFlSXm0j46N7SaHYrlCkp7LpdUkWQJWd6n1WoyNj5Op9ehNDqGjgLSZkZ1bITXbHs9d7z6dhbnl1mcn+NjH/8jtA45+eyTLMzNcP7Bw/zYd/1jfu5nf5af/ukf5zd+7ze5/vqb6Hba/NIv/Bz/89Y/YMjz6cQJy+0GP/PzP8sPfuiD/OkffoSVxTOsrKxc9Dzv+eSnUNrS7cRC3o5AkjcZm/S5/uBWxkaqJJeZ5W+/c5pv+uCd/MV9n6TTTjn3bMrSmR7Dk7BtWwFdtdADMuhcamysWQIbpRBq+olZG4/thgBAGMJNB/Zy2Myx0pLr9z1o9mH7SMDN0z73f+5hnn7qWc4ttin5cPz4Inia+brBqJfmACzw5EydIFCuJkhqLQZWdiLDnHof5vuSP2GBgoG52HBzxYdLAzcvKFeSEv3bwNuf996PAfdaaw8A97rXAN8AHHA/HwJ+7couYzD9D0qhLiqGZBCy3Lj52ifPMxvUiwCGta7gSv530eGUNWhrCHJLaGD78Bhff/erGK8WePLhQ5gkIcpTvDzFeIqoWGC102XVWDKjKOY5w4HPsO9R8iyVUKG9nKJWDKuA7WPjHDv6LHmrhW8M0xMT/IPv+U6OPHGUEydOoHoZSRxz+vw5NIrl5WUqpRLlQsBwJWSkEuAHHo1WnS0jw9x5+60ceeZJnjz8NKdPHOfUc8c4fuwYzx57ltZqHZvFjNUqXLNrigszp4n7PYaLFZJeytz8MtVikZHaMJValesP7GGkUqFYKJKlCZUoojY0QqFcJs8z6ivLDBc1RaVZOD/HmVNnCDzDDTfu4Sd/4t/yXd/3vbzqzV/PNTfeAVZx5K8f5vu/5x+w75abuPm2W7nvnk9hjeXJxx7i3k99gtlTz1EL4dizjzO+ZYwPfO/fJ8/6/PFHP36JMh5++iSnT88BMDWl2XfTEH4BhrcUmJld4sDOfTQbF6tVGMD26zR5pc3EeBGTZ1TGYX4Gjj0Mj97Xx8tgbBt40eXHS/48TVXATXtKBC7jepAGDZDlsDC7zMyF+kX77BzW7B+1LM53efLwMn9630n++qkFTq7kXGjB2VVD365HIF4omVsjM3g3sTQ6Bg+xICKgyHr0zQANA4lZ15Y+UDfQTK/AR9ogL2kxWGv/Wim1+3lvvwd4s/v7d4D7gB917/+ulfjWg0qpYaXUVmvt7EucZS2VeaNmW7vx5eU1XjkaQUqz7fMG1uDFIAVKUqIHZdxaaaxLo954Ft+6JCqluP2aA5xfWebwkee445brCC3kuaFeb7GlNkbeTihWCgSeD1j8LKNLThp4xFmGMZZG0mZ0okLFU3SWW6iSYjnps2vnbt73re/l3s/dz6MPP4xOcw7s3MXt1+3jxOnztJtNRsdrFAuaufl5FpaXqa80WDg/x8LKMu1uhz+/5y954xvu5q5bb+X82RlOnjrNoYcf4rqbrqVaKWOzFv1Og/NZzvat05w6eYJSqYKlhkkSFi4s0a9VKQQ+F2bnCQohI8M1FB5pmpIDs+fPcrbTIrcZn7v/fsZqNSrlAniK17zu9bzlTW/ihhuv47d//yPMzJyhtTLP7PEz/Nt/8WN4hZDTh58FIEkSfuJH/xVve9tbed/7/h6RqnPoqU8TVDO+7YNfzx/+zidZWGhtfIBkmWG13iEMNJPbSzz5eBubpwQaxoua//Oxv+TIkWcu2qcyCv2hJkdOHaVrYpbbluFhqG6F5TOwcE7Sng9s88nqGY0rCPFb4PhMm+wyU3uWw/2H1qfjSMHN0z7X7fI4dyGm24fVfJ0PsEgi03oujsz+BaD+vGMP3ITBcRMrSXkKMckHOQuDsGrOxdaHBkY0NJov/R03ypfLMUxtUPY5YMr9PQ2c27DdjHvvEmBQSn0IsSrYuXPnOlcIMOAD2MgwXCoCCkpIoAHJqAb7OeJyA9BsDGNaazHuFoo1YciN/CQmJ85yEpPTj1M8z2dkuMpDh46ybesW5ucWuHD+PA91uhSiEuMT43gGwjAgCDxavSaJsajcEEURYVnTbDZ4y9vezOhoDS/3SXp92lmX0bFJ3vDa13P22HE+9nsf4f/9hV+Q8uDprXR7fU4dP8WhQ4+j/YDQD/B8n2uuv4Y3bBmjn2t6vZTjzz3DibNnKXoe1+7fy87p7fiBJvI9zpydozpUo9vr0W61uG7vAXJlOHP6DKPDo+RJSq1cJkkTtk9vZ8v0ON1enycOHeLGG6/DU4ojj86jbEaW9tk6PM7QUI1qtUBuMz7+R39KrjXDk1vodzv4/vq8d/9ffRalNXZDNmKz0eTTn7qHHeMFxnZq5jvLtPsZ02OjpOmLaaglS/q0FzM8I66kHYP7H3qK1eY6LxH4sG0PHH1qhb37eoBH1QO7AHtGNWbFsNyV+oYjT2RUI9Z6e2ycUwJEATfKaocrkqoHo8WcVisnT8CmMsic6782u1tEcT1gfxFWYpnxB9ehEbBI3PVsvD4LxBu2KwFtd/ySO37qPutbON3gZcnfmHy01lqllH3pLS/Z78PAhwHuvPOOtf3tIEi5Rg9YNs7nGyFi4G4Y5xpIGwaNtZY0TTG5kfTqQL6mUpIyO0jeMcagldR2W2NlEFtLoDTKV3hGo7KcXpwyMTREa2WFz9/3OV71ulczNFzGGku5VKZULqGs1OfmaQJqCj8MGaqEWGuIfcP//JOPcuNNN+CXfEoqYLw2AQbyDOJ6gx3bt7FrehtZDucX65w8eZqZ8xdI4pTbb72ZnXt24YceoVb02z1yFLVCSBLn2HQPx048x97dO9m1RyoZfaUglh4MTxw5TCEsMlQqcmbmHJNj4/TbbZJSmampCazNsdZQqZUphCFaWXbv3s0jDz/K+bNnSI1l9+49jIzs5Zab72JkbJxiIWBleYWbb7rLFWUldG7p89bX3s09f/Ex/vIvP0arUZdGLc+TZrvDkcOH+aYbb2Klu8pEtca2qEqkA+DyJq+1llrVY3prxsoFSNpQHC/gFWoYsx4/1BpqBU2nZzj5qADNjq0wM6uwsaKUwpYDsLQEyyuQ9qGixXXoIbOtRsAifd6oHszKLyQjEWwtgc6gF1vOzyG9FzyY9GExEyX3kd+DHB2QcvGdQ7BUX7cAKu6z3uCuXObkvjvOwIoYbD+wNBSwYtfPc6Xy5QLD/MBFUEptBRbc++eBHRu22+7ee0lRg5DBRqoVF4kYuBTWwcZGEhIBBKmcBI3BKvB8D+1plFIordduqvYg8grOIrnUFrHOyhggO6UyKElkmh4fZtv4MIePHeXmV91KdbRKyYuENM0tofIkF8PkxHlOrjVKeRgyhqsjDJdrVLyAslEEVorBvILPli1V/FrIyPAWPvXFL9BN++zZtZu9113D9umtVEpl+v2YPMkoBgFaF7A2I09zfE8zMjLG7h0phx57BC+wHNy/H608MB67d+4izQyNRoNGs8GFc2c4/NijBEGBPO1z4fQxquUiY+NTVMpVOu02UeDR7/R47JFDvPlNb2R1dQFfga8s584cp9tepdlu8OQTh0HB/n0HGZ2YoFAqUxrZzQ/9xP/DG7/urfzcT/0k58+dvezznp2fY2l+jMmhIqWohN/NSbMXIIiASkGze19A/+mYcByqWxXPPtOlkV+c2BQqYN4QaGh1JCl0tg3tJUsrzlEBtOtiMWRWLIUcMeUHw84D4ssoYdmHTnZ5cPAV7B9RDPmW+WV4+pzjDqwAj1Yw5kM/k5k9RV6vZDLOzqRwprFupVhkuwHnHirI7cWwORijz4dS7fbzEJD4cuTLBYaPA98N/Lz7/Wcb3v8hpdRHgFcDjZfmF5zZj5E8R2slemAHTKJ1LsZAie1a2FKp9ddKuzJt5yZordcfoF23OtbbvA3OPUh5cIlVA3eD9ZRsYwzWQKA1B3bvZqhc5LHHnuSa6w9Q2zYhloWOJGU7S2WWVD5GOTcoVxQIoG8o5wrfZHiepY9mttWikcYkac7jTx7iPd/0jezZsx8v9PFQtLspJu/haw9PKfIcci0DWueGMPKJKgG7/G2o9CaOPXmE4UqJXdt3kHkZSwuLnDt7ismtW7n1hhsYLvoceuJJhse2UIoijh87x/lTy7SaDbZO76Raq7G4sEBYLDMyXGNhYR5PpRTKIYvzZzhx4iRnz/oUoiLNxjLWWI6bmB393VSGxzGFkHo5ZPsNB/jXP/8L/L//9l9z6sSJS575uQsrLM7WKdQNW/ftZnapx2rrhYdxbQjm57uszsPOMRitKY4Edq1UeiAVC70lGB2DZhMyBTPzYJ2PHyTQq69vnz/vd+D+vpzypzlUtZB7A+dlAGWjwOkFy1QVZvrQ3DBL98y62zDuic+fpJKENDhvbzDoNohBlLuIy2oM4VxL3h9kAW+0BAYWyEXHdOLx8rIfryRc+QcI0TiulJoBfgoBhI8qpb4POAO8z23+CSRUeRzhRj54xVdiLYrcZSa6YMkaGbkOCRtn+Q3BzPVYhhqYfAOVvziiYY3kNkiKhHNWHYMpvIRFKbE0LAMyU6G1xdocbS1TY+O85o7bueeez1J642uYmhpHpz3iNT/S5UdYSw4UtSVuxSS9BM/TtPsZK40GF5bqnF1cIs0N1994E9/8HddRiIqYTKMyha80YRDi4WGNxWoweU6e5eQ2Jww80gF97lv27NnD+MQw58+cZObUSVr1OnlssUHEzPkZuu02y/OztLo9FuqnuOGag0yOT7GcpzSXVnjqS19icX6e2tgYhEUKlSor8+dJ0xhjM/q9Dqurdaa2TFGpDLFt61ZsnpKmMUvzM3TaDbqZIRqqsmX3NFN7d/MTP/Pv+f/8+I9y7nmWQ7Ob89CDp/n61+9n9fgSW3fcgKdc5y5g61DEfDNeG/hpCl7mU/YSVAPmlgzx8xJ2Ig+sB88tQbEpYBAbaJr1kZCZdatgawGaiWxjWPfnn88tgBQs9bJLLYnBzFxQsGhguXGp2T6Y0XNgLgc/X7/ejRsrYBhosq7EBrEceqlwF3bD+wMZKHHIOiF5OdB4OcCgrL0cNn515c47b7df+tIDrCm4Yl1h1Xq95aWGplrnJNw/5bgGa42LOVnXZUecPas2xDTtIE1ar4GQGA16LXIxiJhYDGmakqYZ1liM1TS7Pe757H3c+do7mJieIs1zhrwiYW6E0DJgfUViDZ+67wH8UpFr9u6j0WgzVC0zMTaC1R5BpDn83EmiaplqdZikBX5u8QKFsZZQ+5jUorRzlbAorUiSGOWLXx34GmtSgtCnGGjqK0tE1sMmil5mWF1aJe72MIHGqoCoWKEWFcl7HbqtOlkS01xe4dmnj3L4yac5euoYQSmkXK5QG6pgVcb2HdtIk5jprVtQnk+SGaKowMH9+5mfPc/ZCzPMLq6gCxX2X3cNk9u3EeExf/IkP/PT/4Z+fHHy0kjZ4x+8+3qeerrB9TfdxK/8wSfIjWTAXjde4Zml9tpgHq3CdXsVvXmLSuFYA9rPs6F9REEzKxxBx17MCwxmzQFZt6UK7Z4UIw38+hcK6g2IwJTLA8c4otAZ6wr5fE5i42sFTCpYYZ3LUMCwgpZzGQZE4ktxnspdm4eAiM86wTm4duT1o9baO1/icMBVkvlocUSgdt7RWvbSRtvgcv6nXbMOtLVYa1DGoDAOGFKBmtyFKMkFaBTiOiDhSqWkb4FVg+JuQ25djecAOK0i8CNCvyBvWUupXOJ1d7+Wv/j0p/ma976duG9Y1QnjI0OUtIevDTGWRrdPs99less4JvLZuW+HtJ/LLTazGFJCDJEP3Uabsi6jPUWqpaNEChgvdySpQStF4Lvrx+D5HlblkqUVQqxzdCXAWAj9CC9VFCsFVueX6WYxNowo1ypEXkheAG8oIkAxPD5BcXSSaHKa2rPbaC7OMb1jmizpkOY9atUK+/ffShZ38cICiVG87g13o4yhvnCO7eM1ilqzXO8Tz9bJShVq2yZ41Wvu5Id+8EP8l1/9NZJ0Xa26saHd1+zdP81v/clfkbvoRahgvFbCLq2TiqOViO1D0OilnF1RdC4TN7RAyReFWE7X3xtIgHxW88QdONGCMuIWlLXz1836yNu4r3HbvdA0uvS81xooKWhvUHqfizmEIIAgWwcGC6w+z829kkCIRYBtAHrPB7eXSzzCVQIMwFpO87obIK/FOxhYBBeHHxmY+s7sXwMFI5wFWDAZytNYm7tjaQbtWoyRv7MslXCV8vC05AH6vnKJUOtkpFzm+rV5FvZMb+Utr3oNh79wiBtvu5WF2UXOnrlAuVym4GkqlRIemuNPH2N4bBSlPeJ+iucplM3wPCmVa7XbJIFipDyBl0NmUjyjKEZF+nmOCrU0r8kB60BUG/LckOUZJkuwNqXTbaBISXs9bJ5RKlYI/AJRVGTHnm3MLsxzan6WVtwi8nyUzVFApH1mTp8kSTWj2yb5lluv4SO/+WGypMX4aJVCcZRup8vy4jyFQkS1VGbnlmnu/dRfsLK0wtjIKNt37mH7DlhYXsULSxSKHqOVgF7S4lu/9Zv44he+wBcfenTt6cWZ5f7HTvANr7+NyeEyqz3xDZSGROUXDeikn3N2Bi7ULWdW5RNPwauuGaXXiTl0rkPVgz3jMNdeB4bB0FKIYiugZYTIs4jpHQATnnzuqpovCwBlLaG/jdGKkpL3nq98HmK5bJzafER5B9suppKXABfN6l+WbHQ9vhJy1QDDoNMzA5cABVYKqezao1WDsIEorVJIeGE9cclYhVJGXAeTkWcJNstlU29wBgWej/YCbG7wtEZrz1kMYkHkrt2whNuE3DQmB8TFSPOMJM/Ic0NtqMLymRlqr34V4eQYWZxTqZbo92JskpOnfZZmztKZ3U/lWk273cGrKHyT44cBWZKxMrNALU4pjQYk+GAsyoNmvko/iYkiyWNQOFLUgrbSpj43GSbLSLMYTJ8o9PCxeNrD5CmZtmR+Qhx00MpS0j5xr0uiNZ4ykCVoBTppYfuGqOZRqYR8zZtfy4lnDjNcjsBavGqJbq+HUpZ8dYlWq0Gv02F0bJhGY5VoSZP2OswvLbC0uoryYfv2rVxzzbUsd0u84+veyMOPHSLdMNs/c65N/c8fZKG7PqQ9D5S6eIjHmSFXIWdX1+fDiYrH933b1/A7f/wwnOswOuxxaiWnl6z71CXXn2BAR4x6wjls9NUtMssm9sVJutbztE4BYyU4f5lpfeByVBDgabntq85VMAjAbLyOAHEJelzq/gz+3pj/UECA7ZWQqwYYrM03RB5EOYUcsGszvGIACrLPgGGw1mKNJbMGk6fYPCVJ+qS9NmnSJ88HYU2LyXLyLMPkEv3I0xTr+eAFKC8kKhQpFIoEQYjvByil8P1Azq4G0RAFuZw/SxIqUcDr77yZs08/zL79NzE8NUoQ+OTVElmc8tgzh3n2+OPceusBhgPNcK1CFIWQ53hakWawY2qa6246iIfGGvB9D2MkIuIpOZd2IVvlFtQRnJSbYazB5jkoIUnTNMUYg/J8aYCrDEmSERbK7BuqSdanVoRa4SmLsinXXne9gLFVpEnC8Gvu4LobDuBpRZ70iZOYNLf4URnleWS5odPukKYxhSik32lz/NlnuOn668niDosLF1hdWeDIlxYZHilzzf697Nw2zomz61mCxsJkNWClH68l7OQGVhoX8xHLHYPW/Ytm8p0TJVCKfbsn+PxTZ+nHhngDQVhyEYSBvw6wkm9QLCUlyikwm8p2L7dtei998Vm6j3xeAbQvUQ2bCB/x/P1cHtRF33FwrRv5gpxXXnGvEmCwKJ1LkpAzv2R2zkTxXU+nNJfbpLTrHm3W3Ys8TaRJLI489IpElQJFpdBegNZ6TaG0i3SoNSJykCuxDjqDqq0BLFnAWwufKnwdYC3USqKUtVrEs098nHJ5gkJpFG00ka8g9CmWIRrqMrG3SFRS+DbEV9IS3yqLUR6FSoguaDyz3pNyUAEqHasGtOP6SluwHqPRaJTyBRjE2MIo6zL6FOlaaNc15rcGlF0bAL6SgLExliRLqOJR9LZi8xxfe/RaKywvLzAxsYVeNyY10I9TisUylXKRIAporq5w8LoDNOuL1IZK+MqirCGO2xjVY+bcGfbvmbwIGHJgqZ5SUuv+dD+DYwsXhy4zA/ON9deRB9ftGeLc2WO066soYKFt18z10IOhCOa6gxG2rvQWqHiwvQgnO+IKrJF9SsKaz1fQy49aWEmEIHyhmTtzP0ZJMlt9w3Gfz2MMtt8ogyZFcLElk7BuBb0ScpUAg0QRUpM7M9lgc+WWXNNrzoXn+5K4M4hSeOvcoPKCtfetXVfm9adwOfJyg6gBf2EHL9zbGyMia04O4BazsZIxafwK2/bfCjZGa0mHltCoppgbRouaSk1hlcEHVJ6CAq08gkCzbWqCgvLQ5C5kZySspsDToszaSqLM2iWv5WesJ32tgw1AjlWW3LlpmbGkKkOpAchpciug0DeSf6GtIc8TrAoweU6oNVhDQg8vgnarjs0MfqFClmaootwck+eEoc/oSIWQNr24iw4DhoYq6FaMXw4p1/bzge96F5974Cj9DSWRs930ou/1Qgq5cYYdLWlGaopOa5mVpfraPqVAyMuOK3LylPAJg+MOtqv4cKF/cfgxcg/6BfrJvqDEL70J/csc88UA4nLb/U3kpc7xfLkqgGEtOunJbOlZD3yfwbpTetDfyU2FasOeG+MXa7ELtUGZ1cazbMQHxeVCtWv9Gzbuv8EXvCj52xGfylq00ZSrB7hw7gTDVXe9xqCUxcaGoKeYm5nD3u6RGkWkpWBWch4UlVLZdb7WaGtFCZTMNHIPNL5zrQaXpdDkatDXUqIpSklY01gjLfLJSJXBWg9jFakyYGWxHuuygzSG3PTBgMoyrMnQSjJwsiDEZn1arTraGJK0D7HBixOyxJAlPitLDbKkhyahHFqKvqXgBXT7fbpNQ7vZY0hVCItDXH/dfvbtmuDIc+t5b8mVTM/Pk7Kv6fea9Jtdug0BGU/D3gkYGy7zhWMdFpw34iMzcUVB0YOlDC5s0ObB5324xL4PNKQv4iu8GCfhO5B5KULwhb566D67XHj05crLBZerAhgsYJTBYrA2d8SjwVpnHq+pwUb13wgPg9/qotfPP8faHxt88xfe8HkWxwttbN21GUOgAvJYkSYZxSjEKoXNM9I0Jvd9jj93jiSXsKOvNb61GJOjtMYLNFaZNevIrFGuAmA+A5ZeLBXrOlDlDjxyjBSFKZcoRg7KkJmcLLMCHMrD5s5VG0R0sOQuYqOtkbNIjjnK5vTjNiZJUNZDGelfgWfJ85Ri4EPWI4v7EsI0fUw5oNePyft9rMrJSMniHtoOQWIZLQ9x/d7pi4BhIC+19uRGyU3GyROx5KoI+UJqILMeh8526TptVcB4AEsuApBtAP3B75oPyxtseIWLKoBbJPmFrYiNSlvwIFDQyuQYEesZkpcT33HpORdHLwZScZ+vutch6zzEKy1XBTCA+KJKqTWeQcqiB03dBj2eL95no8VweTjg8p8OukpvPNRltl8HE7vORTxv4zXLwkrpd7FSwThFw0jRli569EmpX1ig024TlIvEGjKX/m2VJfcVmZJ7MFieb+PlGiAdrOLtXufWypoYyCpcKCNsvs0gT9GuJ5BV3triPYpAEq9c2zzrwrvKKPLcOIDwMFkOeS5WRQZa+dJhO03IjSN9jSGL+2iTE3nyWuWGwFrCYkS9vkqzWYfM0A4jiqUA37PcfO1O/uhTj1xy219Ort1iAxqtGHxoOO30FRyby9dCgIPns5qu+/rPn94tsJpd+t6Ac7Dm8qBwOUvB2IvzFC7Xu/GiY3iuViO/vLKv2PVx6wMFR6Z+NeQqAQYFSks/xjUC0sNaT4LaG7nitWl8Ix/wvMM5Pb7I6h/c5IEv7ojF5z+RSwbnxp0ueXqu5ZwjLrWCSlnWGyg52kgpRR5Cp9tn4fQFus0VKqVRYjRKueQqrcjc7OA7y8jDldoqyX4cgMFgps+VpFuLJTCgaHGjWJA1cy2xBFw1WE8A1q7fHM/65Hnmwrse1mpMbiVfwmiU1cJ2GPCsAe1jVCpMuZEEMg+FrzVeFKLI8T2Ik4Ryuczq8hJ5IjkWJrOkSY/RWoFCIIz+Rsm58pz+LhB48pM7Fm6wAG1FS/nyQDaWJ2/UqwLrUYPLiXLWwuU6rz//GhXrSjs4j3XfJ+TyxUxxtr7vC2HI4P0MCbN+teQqAQaw1mMQB9BKrZnoMLg5a3MzFymqq8pUl/l4491ew44NmLIGDly83UXvDMopBgTWAHQ2Htu9NhbCIMIGnpiHg27XKagUGvEyx545xMj4TnLro/3Q6bAlzQ2eVeSGta7Ya/0j3HXmDHxW6wae9MVUdp0bwQhPIe2OWQMCa9V6ezwj310phRksQmAEjJU1YHIBaDyMEe3VWpOnOcoaAt+gjFgbGovNpD2I5ytMltBsrGCsJUsNtWqVer1BdajEwsJxPC9jx85RKuUCvfqlhvaVGg3lAnT6cl8DJTkBHecmdY0oZ8A6KATuZ2P04KV6GhnLC1oxAwDbaEduHDsayVno2UtLrJ8vXw3X4OXKVQIMSmYrDUblsrrU2tpTIAVW6zUTgzspJvzz6WzR4uc/0PUH9/xS6/VZ/6KDX0JKcCnguJl87XqMIU5TUpMTRREqMygs9XqTJEuIyopHHv8Ct9z5VirlUSGWlHSa7vX7VKtlsZqUXFXOeiUI4DIwlXSdMs4NYb3SblCCrqzkfBirwUqlqkKhlQCPttLc1jjXBSPA4WAELeYAqBxNCtagyfG1IUtjNCmes+KsyfE8K7E4m5OnPRSSmm7zTPIhhmscP/o0n/3E/6Y6EnFisUv4AousXumk2E0uLpeGixWzgHw+6NxWReoINsoAGAZxrssBxUvN5IOuShYYCwWUeu5AnoLQCgc0HsJMfOn3ezFr4W9TXm4+xysj1qKtcsvICXNjUORGZju1FpFgHQisG8bOVB4UjQwqIp1BjRkw+wOSzhoyY8gwpO5vY1xthTVrO6oB0cFg5h8o57oFY9dO5BK2tSKOE44de5Y8y8XvR1rH9eMOobYsXDhHa2UFL7N4JkdhiHspnU4XX0GgFKFSRMigDpTMdBGymIinwFdyr7QFz8jKXJ5VhFbhG6ekuUG7f55SshBJZvCtQhmLxuBZi68UWsnvQEGkLZ5NUfTRJibQOQExoUrRto+vEmzaR5uUQqjwyPHI8JRBKwMmxWY9VpfniPttDBlZ1iRuzXPizAWefG6RP/nkMeLelQT5XliKHkRu4ZWNtQZFJLFpUDpdAIY9mKi8sIXgjCgARoN1MHkhCZGEpdAdu+q0aCVZBwWAFSMWy1AAysgsvHEmHriLL86OXf56X2m5KiyG5aVFfu0//xLDo8O8/k13s33ndrHdrSwn5ynPEUEGpQcrWlry3KA8ve4SPI8/EG5eEbtWbZmShKk0tSQmJyMnIKDse0ShRmtNaJV03zWWwXJ5dkMptpzcOvNdfnJjsSbHeopep8XsqVPom29CWUtuQfsenZUW5ThhKZtj5dwsUyM7iYKQSq7I+ilFA1G2ntAUWOvWLXD2gMkdp6AxClKryAap0cpKIxUtVkXuCXqZPEcpD8+TnzQzZJ4iN5IBalx+QxgIh+AbQ571sTol8jySdhvPyzE2xteK3MSyirPJMHmfNPOkRZ7JUcg5kn4fZXMaq4ugfEbHxkiymLFdo9z1hmv5yB8/hu9pKsMhS70X4+xfXApFGClCvy9E5EBKniQ3tRIBiapyKdGXyVAcKNhGwOjkL504lCCAA+uNVmA9kjE4tkXIyFW3TsSAQxk0UplQsmJUpOFEfuXpzRstjJfiZELk+kY8WHwZdddXBTBEYcjU6BhTW7dR9ivYHNCOOLMWaz3nSljXvFW7G20lf1aJWZ1Zx9AbMBhimxObnE7co9lcptvtcPLEDCeOnWLXnj2MT28j7sdUAo/xyREKw2WKhQhtDZFW+IR4Vtj2QPtUgyJFLfkJA7dFWUjznFhlrDRaLDWX0V5GL4nBKrSGLE5J65DkivP1WQ4/9ggjU9vZXdxJ2fNYjbt4Jse3Ch8riUwY8futi85YWbfTIu6LZy0pOZlNpRupsvieIlBCGBgLucoxJiVPDLmsokFgAoqeR6IyOnmX3GjCwEelGabfxKRNFJrcKLwsxvMt2qakaR+tLb4PSbtOc0WadmmlWF1YRvllrr/5Fhbnz9NYXaBZXyXLxCqZn53j2KmjNLvLhF7GtQfK9BI4MfvlA0O7fyl5qXBumCczcRmoRrAQQ90ZKINeji+UIxC/iC8zUGiNKPFA0Q3rpc4DsRu2lxze9fcH3EcfmNJuCF/h9x40vxuAw0sBw4AAvVxHqheTqwIYyuUK3/Rt7wdj6Xc6dBodCqUSSdIjyzL8Qok4MzR6LXRQpFisstps0uh00L5PoVhAa0U36dHorkrdRG5I8j6dbpsoCDh98hif+6vPsTy/xFOPPs745CRvesubaTXrPPXYIQ5cf4DX3H0XU1OjLC0uMLs4w65r9rFreidLi7NUh4bZu+s6ttTGifyAJE0wVuHZEO3lHD95hEOPPM6QX+HC2TPsuXYL8+eXaK20qRZLjNXGKEWadrvNs0ceY8f+PWybGiEYGSNL2yibE9oUjYe10sMhzXJ8zyP0NGmWYpUlzVMyk7hZOyXNE4aiMtoa4rxPnPcwJsUPihiT0Wwu4VnN6kqDNEsZGpugWK6S5THL9QXSNKFWrRISsjRziqzfwCuErCwv01pZ4cbrr6Xba3Pk6JNs376ViZFxVG5IO20aqyusLi+wMDtHcXiCXmeRo0ceod1sEEYR4OGhqa+uEtiApQt1lIXprQHPnvryQQGgf5lp3UNax/cyKAbQTNcJx5h1l6GsxBWovwixCJcJR17m/QwBoPGivF7tS5HUkItmDHIz2ggfEdt1iyR374+OgF259DqUu1Zr18uqawoKAXRSKdEeNHy9HJ55rAPfiwHe5eSqaNQyvW2L/cHv+06SNKEbx+hiES+M6HbbpHEfvIB2P6be6UAYUiyXqDeatDo9Mfc1eJ6in6SkJEQBZEkCypL0Y/IsJov7rF5YZnmhDn6Gji2YgNJwRLfdZqgWMrVlBO0p4iSj3e0wMhVRGxkCMqJAUyoNMTk2zOTYMKgcmym08jE5zJw4yelnL5B1Na1Gn/037KOz0ueZZ2ep1ar0Ol1qxRKBSSkUQ26+6zbufttb2bt3L08/dYihSpmDB29C+xEZhl7co9NtUy0PYUzC/NIpmkmDftym2VyhFIUknR6lqMSOyR0UvJALs2dpdZbwrcHXIe1OzIXzMyRJwsR4lVazAyqgPFylUIo4deY81lhuuvEAUeRz4dQ8hx49zJapEhrDp++/wF03jaM9xUMPn2ekFnLzjcOM1iJqxYgkTujUGyT9mJ7yIPcwcSoRGhRKB4RRiSy1tLt9mr0mmWfRRc2zZ3OOnv6bx982KoVC3KlaGfo9iHOZ+fpWeIOYdUuhggDG5eDJR0z8VdZn5kF25OXOv8OTVOxmJu3flJImMYOCp74DiJEAlh1pOuQUfnsZdo7CIzOwbNdrIyY92WakBEbDYlvcliGgHEkdyJEmDGs59lK+HhJV7vqLIc9flu6KG7VcFcAQhb7dPlUgzhPiHDIUUSGiUioQ97t0+znG00IyaSkMQivSNHPhNotNLNaz+CPSccEapAJSyaIiecdiu4APqij59FkiZmepKkx8oSC/81wyCr0AOl0I3GhKYigWoDoEYaggF0Iw6YLKoNO0NFaFFMxzSdFNU0iEsAcr79VCGKp6lKoeQeShdUatHFEolLFaYT1LP8nxvZzhWoHUJmT0yMmw2pJlhkApVI7kHCiPgufTXI1RyhJqyFPo9SxxT/iHWg08rUhiS2ZgZFQTx5Z6w7J1W4nhoYCR8ijz80v45JTwWG1bhkPL8moH7XloZQhCS0FBIdCkxpLGVsKxiFvhe5FUZ1owVtHPYbWT0u4aJqaKnF3oUB62nF9RPHX6Kzv2alVotqEQQZKI7x5n62HFgXkfKAGKln3hiMAksMy6dfBC0QMfOFCCSgRHV12EQ0mRVt9Iz0llZZZXQDMWMNJu39EQdvgw05XS7IZ7/2AAE6E0nw1K0jdypSUcShmoVuFIQ6Id+HCuLd9nQFoDDIdw9ssEhqvClTAmJw07eAoiI0iXpAkq6hH4OcXIogOwGsKSIs0s2pNQvcMK6dTsQ1SAyK0FnhtZKsza9VRbK6UCa8krXuBCM1a2TTOw7tielmXRw1Cid4FyzTYMxFhsLuGoUlnahI9uhe0uht4f9AozzqR0wYK1Wg2VE4Y5voaiD55NSa20MssVZK5XVwdAr7fqMgCBAE8YuCiJyennCVEZVO6iFR7yGgHBkgFfW/plmYGKvsGGHrWKxQ9T8k5K18gAz/oZPrBzyF8rFPCKVjoVJzLYs67BM1DIIey5qE8EVmXEOYCil0u4OevmVKKQdj0n7VoaOZDYF43tX6kMckh8bz3voNcXAjKx68cfKHUI1AowXYYnll7YP29zKWcwkMF1D2bmpS5c6K1bIwpZXHbIg1JRJoZ+LKHMgtsmRMZfksByvg4WgxZt9VTAxShZdq42IhZEqGCuIWnc2wuS0NVKJMsycsceQohX7IvnT7yYXBXAEEawbZtTGr3e5UbZTNDek9faA+tZl9ILnvQzGaTKo5WsIuVr0I6ONSlyp33327oBpNb3NRko7SoZXWmGyYAcCr5sEwagE7AZqMBZCkYAIbAuZdW1FVNaZq2B6TkALu25HAHkmIH7CVEoV6KQexbrAb4iz6SMOLWuV4CSY1sHNEHmFkvBAWQulotWch6txbT2LRQTCI2EdbVSZImhF+d4BajXDb4BmzbxfEUht6RxQqAVgbHUIsh70usgzaCfy30oxeI35zFEVYWnNUma0+1bch+09dEYKpFmuZWy0jAs9WHbGIxXFJ62a8u8fdniNDYIJPIArOViZNnaI5f1FxQcHNPMdyxzTfuixN0LRQgU6+tCWLddCXEXtpSgVoSVOnQzeeZRJJNEomA+X19Srg1sLUIWQ6ECtb7kZuwKoVqAXiyuUArEHviBgMF8R5acm+/CcBVOzwtv4QxkcmQyyYEoXSdHX65cFcBgDHS7DhgGWdA4hdfyudEy66eJm72d4ntu5s+MfB5n4nP5fYkda2ez5bE8KE87pj9w6a4uwBA5y8EzYvbpQABJOXM0yyDpyYMMIrEqjHgxcpzcXWcu51xLPjRiAWClJWMUge/ARufgWQGFzCX4ewWZ7TUexuaY3IVhtVgDapComMj3wR8kerljeg7gcHioBBh8rfCVW0ZPQ6AsoWfpeVADmh3opBaTWvwEyh7Y3FLA7W/Et20pIIVuU+5pO4W4B5NVi+nmpAqsD60+5L5BBR4qCNg1XiM4t8xUbgnCDCoh1TBlpf835xkUzpJxr4NA3Ip6C7wUigoqoRg/7UQiKzYQUE6uIJ1iY+cky8XhzD7yvKsRtFIol0WZc0kcZTGRprUamYxaAyDTMoabXWj1oFqW9wxCmpYKYgHMtiXDc3VOen+kzjVaTaDREAJSsZ6gNfjbsJ5rUUb4h5eTOXJVAAO4iJsS8sZ4jhtwM552iKm1DAA/dLNrtm5JaE+WJ1M52FRmD4UobNIXRc3dDOt7EFk5juQGQKokvdYzkDrw8KxETbUHmQbrbEXlOWCxcj5XZkBsxU0hF6UPDRTcegBZJteS5ustzJUF5VY8yQdWi2szrHW2Bly5S2XQRroAGWetGOVKe3MZeNo4MEQGWeSsi3xw7QayXHgCk9u10a4sYsNq8FN3PF/uTbMLXqooaA15gIlC0IpyMSSwPv2+JdKa2jVTrC4tkyaWyuQUJb/MSrNLkiVUChE7duyiPDFL3lkhCDJ6vZQ335TxzLkFFls5q92MzMLW0RKNVp/ui9U6Oxkt+nSTnGololL2qLf6dLryvbKurDmROtfNU1JrkLbsWvNx7b348QfyUuH/xJfnnls41XATjYJiBI2u9IYIFUwVZQXsnoHQPRu0uKK1EpxquXUtXNfqckGWtU+t8AylCKb6oujdXHpGDsCqosR18oFpD1q5AFoNWfXJAg9f2dcFrhJgkPbnrM2MKY4wcn/bTAZ5qKAUutnYymBPM8AT5fOV+JbKh9R3JrgV4i/L3SzvuY46RhQ3KMj2g0RHHYh14uqPCAsyA2pEabyyDDKUm72NXEO85jfI+VQuADKIfRVdRoyxLtnGOCtDUhDWEpRydx2ZAxGUgGKYOx924DJJmQV9d52eXU9jHRR3SrRGjmutNGnpY0k9CV9ZBbm7716qyBKPXuxDWCIvjkBQZHh6ijQOKYxOUoiGiMo1enHuVuIO6fRyFlc6pGWf4d19hgmlKCyLmTKGxYU5auWQxA8Y2jZM0mvRb7Vo9leIVZNdu8psiWPOnJ3j/EqPSClGygW69RdP91HAzslRlLWMjZbJCz5z9YSp8V1EXsDMhfOE7Q5hsUoYBHg6ZyS3zM7O0+m08DUk2UCt5OFJO0HwPF8ANdswx3rO7LsMShgj1lM6aEOvxN3cuHlBQ70n0YrB9S82BcDjWBS5bWDCl1BrkkIvd8lJU2LRttty7KEIkg5rbpTCRT4QS6+RSwq40jLGIwQgXo5cHcDgfO/UlcdGFUcsKlEQk7vyVCOzNE6Rsgz6KajQcQKscwTWkVIDBdFKTEztyb7ewE3Rcg7cLG4dZ6CsxMO7mQMGR3SmsQBVuQiRIyg9X0DCWKfMRpQzSSHznEvkrV+P5znz33Ef1ghB5ftuFvPcgHI8SJy6jsMhBM498kMBniyTGYbc3UcHCkq7GcSsR1oUFh0BStHoWYwfknpDxKVt5MPb2DZ9M0aN0O5rcl1gpd7lQjej3l5GNXP6nTZJd4Ek7kszlywmt6IYkU7I+x3qK6tkWYyxKWEhwlcWk/ZJ0xirYgKlJA3dShKWkIdutS/gzPIVrhwLnJiro4zl+IVlUhWhi5Mk1jAxEtBJQ/pZhqbAytx5er0ONk8wRtbpzJNLjzfoJJ49f3kruHyJpROTXexeDAjQnhKwDgMYG5Vn3FmWbVqZdJ3OtXALJQ+2hJAGYlVkzlothzBUlgjafFM6TxUCiEIoJAI069Amn6+VFGnZRnNpnchLyVUBDFkuNyKIoFJ25rgj1TINiVPefldMME+B8QWVC6EoYdp1M2AgMyxaTPZeLAM3Q2b/wUIwee4U1INOLKm1pYLMpKHnyEbfLUfWF55Aa0dieeIH+pk80MBZPAYBjb6UKogPiytycgqkPTEJA0/GWo5zRZCBU/TE1fE8cWuyfP16u5kjEn1JY7aAses9tAfVk57jOAxCYLmCSXHNooDC+H66tWnSwn46rZDz55Y4d+48dO4jbdVJkzbG5NJ3c3ABgx9rudjjHvxsrDWUz9PLKN/frEJiXSzQumgpqhSyBU625zlhB3dds7q4cQmYr7I4b82LwITrE4IFxqqy2G5zBaYnZGzbNoyMQ3lIxqlNYLkO/ghUfceHaOGhVADVDFZWHLmqoKahm0rUpVASiyNLZOJaGhSPXKFcFcBgERM+y5yCexJGdOX/RJEwv9ZZBkkK7Vi4hCyF4RJUSvJ+ah2j62bSPHMEpFuUsNOX4xZCsQyCXABhqCINRgPnlsSO3PONJK9kRrgK65Q4c7N8riQa4TswKZcVpqQwTtsHpKRbPkK8pVzAxDqewFg5XpqIOerH8r2150KlnvAeqVvvsG/EDO67KEmIXKN14OPhgMi6qIQPXsGjU5zgfLqTE/d0WT17lLR3iLTfd6EbhyprgbgvR6Fexsj7iksGNttQJTv4Hn+b1yTS68FyJPxYGMH0NBw8KBPa7r1CJK+cl8cwFMJQUZR6aARGJ2G5D9GQLKnXagnQTJRlv/4jMibKZZhbkgl2uChRkL4PWQBxACrhpevMN8hVAQxKQbcjgznzZFaxblB7vihH4IsfXYygVJOb0+tCpyFKnDhKNlcy6+cIfxAi4JED3T40W1ApOvPdkzwEz5fYfC+Xvz0HBNoXM6+bSLzZV3ItSjtzzpMJtRvLQw0UhKElCCXPIk6g3Vr39QcmYNyXOn2rhR1PWecaBmRgljuQcrzJgIgdcB99F6kJAgGHjkuiCmAtsasQyczSD8rMeLtZ4bVEo3eQbZ0nL7RJT5+CdB5Mg/Xs/Q7rwbi/pZn2KyZ/+6AA8mxbdXmuN90EW3eKm9BYhMoQYKGoIahCsQhx1yXGeRBWhWxUFsIiTEbQbUuuTx7C3oNw5iw02hJ18bXkPFSLsvxe24iuDBtg7sqv+aoAhjCAHVug1ZbQjuet8w5RJGZ5krlZtQuZcxu0C9/5zizPXHQAJQpebwqYBMqFHgNHaGaioDliXQRGHpTneAttxGpJe9BL5MfzwI9kho/89dwJzwXKw0jOpVzoMrGSeELgiE/HE7RjsLE8WOULax67eHuk5eHmTrHzRMzHggthDuZ0T8uaCINIRepCp6m7BzaR8Fyh4hON7uP46gGmb3w/737Lu5hb7eJNP8fTR08SKxf2qFtI62A7SGR+MNtuDNRtyt9EfA3lCtTGYLUBvQ4UnZs3XgSvCGFZFtkd2y53vtV0ZHQuIB+nUF+GkQk5VqJhwtXkXzgtLvH0qIxtPxLicXRc3FNP8XcPGNIU5ped6e9DZVj8rHII3YagYejLZ8aF/YolN7Mmkv47IFyMkZtilbgHoWPv4xhyZyFEGipV0Yl+IjfcuvyDUkn2yTN5mEFVeIS4KzP+wOXux6KIGnFJtHuAfoCQh55YHOVIrIkkFdCxVkBPRRAU5XsVciE1A89ttyF8WQgh7697754n+3gDUhOxXPxABorWMsByW4DxWzjTu5XXvut7OXDDTbTimDNzF1haPE177mlYfg7yedg9Ch0N3RD0CFRKhKUAkyRkM+cgnudFwUEX5SePkbxzzf8dFsdXTqyFkXHNmRlDVAIcob64LLxYFIqlGsbQXYJwSNzHPIPasNzRLIFSRULangHTF77KD2Bqq4DH7JwcK+rBxAT0Gm5SvMLQ7ECuCmAAUZ7ciLL0ehKaGdwAryCmfmrFDO90YKXtEoj6EsZMY4hK4j7kfZfo40nufO7CgGlfXJE8E+skc4CSD0KIzoQ3mXNZKhKRyKyzSDK5wbFzHYxyfRndVB5Ecn7fpcGmPTkn7rixEQtA5WImqsS5SFpAQQ0iHFauLcug0xOLqloWl2FwvTYXgNSey/33Je5drQZkyTCL/rXMt+/k7vf8IyZ3T3N2aYlnTp7g8JHHOf3FB0mffRZ0ClGKoo2aDCWSMz6MLnlCXp7rw+lVXgwUVLFG+YYbSbot8tU+Wvuosk+uUvLZBWguv8yRMFhT+qWyB75S8mL5j185yTJYntfsuSZiYbFHbmB4SMC83oThYegtQLkm423xDOiKuIo9R/30OxAMCfGdx0JcNtuSaxIVoVADsyT8gk7EchwqyvjpXq7p5IvIVQEMWsPomChDogUpjRFzPM0hackMnBuZHQ1y80wCxEIWal+Ub1CT0GrJNhniXpiWKJGpiWI2+4LCqVvpVPtioZCKWWesWBM6c8aIGzsalysRuSQV9/mAk/AjKBU0VYxU76XyHXKXVOO5RKM4FRfBOKunn0qs2g8B9x1zhWRtImBRkRXZCFytROBL9pxxWZ1DoyHByF4uLOym7b+Gu971nXRDj+NnT3Dy5FGeeOB+lh9/DNvvwa4SQVlTKIO1GV6lQO5BN0/QOiFpGuzTz8gIfCEplPEPTKDLyxTyjOq1O4kDhV/okfgae2A79Qcexy4vXflgiMriayUvN8B2heINudh43b3x1QIgaDYz5i9oekaTZIbSiKQ7ay18mYqh1wZvFDJfLq3VhyEjk4jNhYBv9CFWQAiqIO5krgUcJqchysU9NghnkeTQ/LsIDFkO9babeQNR5n4iZGHPFTUND7sswgzCzvr7YVWyzEwiTL8fSnJTpQSdROrWrQIzLLNxnjtXI3Wzt+MeipHMuiMVcSHSVBA3d+GhdAAQjkcoF4Xf8AouG9EIQdnqQq9niKz4kUHBzeqOPMRFTaLQxQCMcwkCVwWaiWuUK7GIQk8AY5CabTNQmSNkQzlWQUMUeqTRbr7wUIGxa1/Lnjd9I6fbiwQ0aVw4yZG/+gzLx47gj9QIto+Qxk2Kfkbke/Q8ia16gY/uZaA91KkFbOeFcwqi4WHGbr+WC+eOkJbLhLUy+MsEfkI7b+IHo3gBRAd3039wmSvtDa/LJdTITvITj/8NR9ULiQW/tAEYvrqyuJCw/WDEUiummwEdsQqtB7i6icVFsRyM47aW52FyDAjceHHjIUtlfLbc8hpl68KUq+ISD6Jd7a64KS9HrgpgMFZQLYxcIYiVmbUUCnESFMTsCnKoz4HpiD+l3dV3exIyzFJoN8Xn8iNRxqGCq4HQcvwMma3TAPKiI/Ssq5BElBOXg2CtuAudjrgPni8PxddihQzCoMq1BcoS57I4kjE3UrMRhFK+PWhhn/adW1B0YOfSY2PXHKDbheqI4wsGnxsZQBh5yHHivrOBou+RFfbw4ANFtr/229l+91uZWT5PEp8jPfMcZx58hJWZYwxvH0NHXZKsQ1D2KZU8vCAjszk2VERFTS9NoFsgO7Pwos9MjQ2RlNpSqWMzUpXgj3WweZu4Xic0KYFJKG+Zph8EgppXMhbqc+vVUK+E5C3otf6GBxnkmL58DiVPYXXeMDQWsXQ+JougkkHTOsJYS31HuSj1KK22hC2tB6fmwB+CyqhMOqESvmmk6pL9YiEgoyLMtyUit4AATK348q7zJYFBKbUD+F1gCtGjD1trf0UpNQr8b2A3cBp4n7V2VUljxF8B3oFQ3N9jrX3sxc7hafGPE9eOxjiFBJfZ50KGnhEyL/KAyGWbKahVRHH6PShXXaWdFT/fc/54Kn1b0FbAJvQkGaSdiKJmqYBPvy8Rg+qQgE+aSO7DYixfvtcVpW65VGWDWB6DEm1cVMLmjrz0IK7LvqWigNVwDYZcaCpL162GPIPltnw334U3w1ASphodKeIaq4n1EvguKSb3OdupsfpMkevf/Y8oHrieU+dOkqQzNI8dYvYLD5K05xnfVaNSTemRkgcBKtIoP8PSxfMsuUowaY8An+T0qiDZi0h/oUG/0kNSLrvYKvRsB8/2UHFCKUgYGiqz0LGXWgt6FMzK5Q9sUmgvvui5Xxl5OTkPfzNStb2SUiwWMLFEIVQkBHYldR5UDGZVbkUUSZbkuRlYWoGwD3tHoNGCsi9Ak7nxk/aATCJbnZ5Y03Eik+yVrvA1kCuxGDLgX1hrH1NKVYFHlVJ/BXwPcK+19ueVUj8G/Bjwo8A3AAfcz6uBX3O/X1C0lgQdbaTAxLovN2gI3Yvls8iX0JxSkHcFSIolUdxCSZTTZJJXEJYlWSR0zK4pumIUF9mwnpCFJcRtaDZhqCSAgRafv9VkrcCpEDiXwmWg5c6Ha3eht+TI+IokohQjIUKx0G3JNQ6V3YzgS8KR72oYvJJYI3FfHu7EuHNVXGfh3AoABZHrv6Bllhgqw+hojV6+nwcfMrz++36CbHIb87OzJMvPsnz4YWafPoTn9dh2wxRRaMl8SKxHgmW4rAl0n27SoUeGzaG5muC3dpAePffSoyLuSjpfpKTZQ1Fh+31Mv4cXe8RRjZkTfVrPPCc3eCDKo7ZtP43zj4s/d1n56vn9F4vmqxFJMQa67ZzRLT6N1Yw4lWpJHYJK5fk3YvmtPZg5Lzqybx/M1uHCvExYPSWThXH5CzYX3fF8mZC6fRlXhZLjrl6GvCQwWGtngVn3d0spdRSYBt4DvNlt9jvAfQgwvAf4XSutoR5USg0rpba641xWNDKze0rGW+wKkKo1F8JxiUSBqw8oBhAon1Y3p5tZCgWZ6cNAZv8kkeSfXi5fsBjK49ZFVyvhFL80SByqSAutsieZjmkuKFusCtJ2e2KNeC5U6Tt3IgjECuj4gu7FmiufdQ+/n7j0Zy3WjnWEpnIVjP1UuIaSL9vklrVmpiaRiIsfCsewddi5H8hDr41WaOY7eOgLCW/8vh/Hm97O6uICvaWjNJ55nNknH6E06TEyOQxhQMtYellC30IYaUI/Js9b5FGfNE3JuwmmXiQ+tiRm0YuJ5wnCpRr8YchK0iSg3cEYD9OKWDw6L87v86yFoFDmnR/8Qf7sv/04nZUXHBJ/C+LYvBddbfIrJ+1mytBohEoykr5MStWqkI/tWNznGBiOYHJcLq8Ti5U5e17yeaKidMsuREDmQueRy9MJZJ++Eauip1/0ci6Rl8UxKKV2A7cBXwKmNij7HOJqgIDGxilnxr33gqMgN6LY3UR+a+eTD0hB35FyaSZK27LQrmeSh15lkA1LP5e05cQVqBSKQmo2YhfOrLisx0SMxqGqjO8AyRQLlACAtcL0Z56UUjfagJbZX7HeOSmJ5aYXPAGIYghBKtZNbGR9g0TLMStF1nIt0lyIRa2FLW7lUjFnMtfhRyGjIoVa4EKiLq9CafC8gGOnipw+mfK2H/4Z9Pbd1FeWaZ5/kvkv/TUXTh5laGeJoRFNO+7R7mfkWSCVqOSEo4YgNCQkGGNRvRRTz4n6o/Tnz7/0QDBWWN0TdUFWW4dBn7vMSq+7FzBd016LmZOHKY9s+TKB4ZVMc/b5aoUvbQ5LCym1IYX2LLWa5OnEfVjpQrbiUuFxnJYv2a21MYmqlcqwdUompO4gz8flRiR9wAgBv2VUXOaXkw4NLwMYlFIV4I+Bf2atba6tsQBYa61S6mU9LaXUh4APgVgC3URm9UJJUG90XAhJbYX8q7dEmTLXMk05Ys7mkgRlEMXS3oYEpEQ4ip6L//dawuwnPeEtFuti1hec8kW+K54K5M7YEFodAQEvgOVVwMj2va4gda0m7onvy2dKi+sTunLu0JemIRpXGOWJtZBl4v8FLlKRJ/J9iYULiTw3MHLp1JRb0BFkSUDXTHB2ZoSv+f4fhalt9Lur9M4+yanPfIqV+dOM3TiFDmMaSZ9mJyFPHJVtQ0g7pFmf1Ma0+i3aLYNtWKJOhfScu2EvJdYImufZxf3Xr6hCyvLovR/DG5566U1fYP9XRgzCWoVcfqXJr7zEHUtS8hkdzrCIq2eHZXJIehJVazcEewmlXX6vLWCQZrBwXkjKalGI+p5zK5SCTlsISqvcRPMyse6KgEEpFSCg8PvW2j9xb88PXASl1FaEAAU4D+zYsPt2995FYq39MPBhgKCg7GpDzHevIBWNSdclFOXCB5SHxD2on5ebUhiRakNSR67kwjkUAqiMIAkhXQEVo8AvSKehTMFIjbWOTsWq3Pi0L6AxWnURjQKUKorJCZ9zp1NyAyOjcrw8k6jBUAWGKy45KxeFT6yQSZ0Eek0BurTvOIYRucbcFWhZXG6Gi3YUA9ctyrlASgkrXYzEpahWIlZbkxx+KuJtP/iv8PfsoN9tweJp5r/4BVYWzzB+2zQ2ymn3FEnmkecaVCiImVlUyRLWUoKwQ0SOQaO1R9FOsHDhHFekeNqh25cpnbnjsLzE1VLkxNoify6/+KsEDBhLc1VR3RqytBJLyb7LjQlysUS7iWvpkQgJ7QdQLItlnSHjpbUoVsVUVfTGGBguC9R1eq4VwMsM9FxJVEIB/wM4aq39pQ0ffRz4buDn3e8/2/D+DymlPoKQjo0X4xeAtQYpqZWZ3riMwMCHsCI+d25FAae2Cmrqgrzv+zLT1tzrPJWbF2cyQRLJLJ4r8MvSMCWJZaYem5Tzp8bVUSDVaAYZ9/U5S+RnBC7PvVyAUStcw3BJk3UNPnKetVoNJcBVLAgQjFQ0hWyI5aUmyhhxh1LhLTKH7rogPiNGquQK0XriVZqJBVKuaUx9mKcPebzlH/0o3tZt2DRGL53m/IOPskiXqbv2kkcZ/USTK08Wx/UsfrFGrkKssfhFCPwe1lMUSz7WWCrZKHOHl1xjhxcbDFqmJmskvbPtRuGXI39LeQSXygAQFKy1Yw156fWovjJi0oTllUAqJQcWl3NBPRfe9n3XiMjl17SbUC0BnhRKpW6/dk8yHT0rPFbuLO5BMV7jK1xdeTfwAeAppdQh995PIIDwUaXU9wFngPe5zz6BhCqPI+HKD77UCYIAakMyzhLEP8+MsKupksahrYYlNRLTndgiM22WCC+gPSHyshxMJCZXEgOZq1ZLJXxdmRBegUxudJIJoRNnQlhqI7NzuQyduqCuNhYViIXRaq/364i7hrgtJt+ghkH7cl2Dv6NAhtnSbMbqkmXbtYLevVgiJ0kq2waevOd70iXbOoteudqHwNMob4TDDyve/N3/lKEDB+hnfbKFsyw/9jRqKGRibBvtbIVOp0OeW5TJ0KpEsTqMoYSnIkyWUKrkFKsJserQS2JM07JyNiZevgLSbXhI2NlGV8wbczXM9n9T8RA1cE081oDilQYG7bTe0ksy1AZ8DSKYmoK8B7FL/Y8cDhd8oXLiPhSHXIjciVKOo8P9VtLZySIE98uRK4lK3M8abXaJvPUy21vgH7+ci0hTWF0VJclcXUFUlHqJMIS5jkW7OopOIuMyj8UEL4RyA8wAGV1RSbEoStpLJBphjYQuxyLFmROWwlZJsyaTSEjeBd0DOynnti5JKk+kG4/VTpFd2KfbE98tcVxHjKuGS2TGn5wUC6bdMyx325QqrLWky1xtRn1VSEvbl++Cq+3o92BqXECkVNYMDW3n8QdTXvf+H2bi9jvp533CeJmFQ4eZ3r+bk+1lSp6mubBInhmy3GJNhOTMRigCtFEYYrzAo5+BiiKSuE9Ikc6clpv1YhJE0r54eUUQdaXB1eEG/E1lUEXqqt/wkF7OL5z1+RU7r+fhTfjYNKegNEXTk4rdMpRLElHAitXY6sjEQSYt/nu+XHkUinVbCFxUOBM9amcy3iMtpGX+lXYlvhpirczYgasTyEBuUCjZioM261bJTTBGiptSVz/huWOQyU/gi8Lhi5Ja45qvdGBpxeJ7kDQkM7biQ6EKtS0SavSVIK9BrilRLpc9FqIndBmXVkuIqeCJa9PPYMvY+vVEgavIDGFoTG70oOFz7tJUw4K4HEORS3Ly5ZzFgiMnc0Xoj/L4g32ufeMH2P2aN9IlJ8xbnP3iQxy4Zj8nlxucnVkCb5mVZgvtVQj9YZrdnvSu9A2e6uNnPfKkgbGKHE2e5NgYaIUkdWd3vpiUCtJOqONmUvt3uXJy4Da4YpS1n0EJYg1Y4Suf0zCokR289MmXhFTq+QGVCUXkW7JMImHtvoTRbVfcickRWF6UAkI9wHIrl+nlrluXlbFokYhW7kj9mgftl0ELXRXAAEIMtjuS8JO4LEY0tF2uQaRlMZlB78TcSOJQ5m6G9DSUG+IHMqEVShKpCLQkQQ0VRPkLEaw0ZZYeKroqzUQIylAJ6OBMtoK7NquFkNy+zSfS0GhmkkxiEDBCeAHPIbwtCNBpLQqfGjELjZJ8jdFhscY9K7kOxgIuDVsZGQhRNMKZpzXT134dN3/Du+krS+Bbjt7zIKPVMnOLy3zuY/fSu3AKbBtqYyhvmNJYkTAs4pUj0iAnq8YUxnxMr029dQ4aq+hSgpcFxHUXJ31RURKGeSVTlb9qohEACN3PRmvBKa6KHEF1pTkNV5oYVUCsES0FLzpisCCxTTNW+j6TwylLF2JJuS9Aw42vgieWwpISgrxcEtd2yYGE78s6l3EiYy1U4vXlFraWJdX6/MvIBL86gMEKgZIDWcPNnmVXMdlH6hGqYhAoK6GYzMj7JhXT33dkje9SiFMjYcwsdu3RLIwOySxtYpiYlL9bTbGSy64/QzeXIVLwhLDsAVFNmsLUKtCt5+SehEl7qawAFDtCsdWX8KVVslZAIXRAgKvPyGG1K+5NKZA0WBCzr28UWeaRJ4ZiwdBphXRWNaXC9dz1Ld9JWvCIwoAn73sQ5RXwqiX+8iN/TO/YMSkdLZRhZh5rztI59bSwqX4AQYiqlGlPlbCjPtH0Dky1hgoaBKmi09RCeLzUA/o7DQqDKeNyHrHGpcGxvrRZQWaHKzYYBsd+KdcqRkDIHd8GksUXFqDTIg9geKKIyo/TTRLCIlKJ25NchdNnJZvR85xFGUuGbKkik5JxxtxkRXjkqAS6K1xd92UaP1cFMIQB7NkhytPoQGVy/RnluYBA7MyjzEi6c8UX67Y85h5HLuFJpcQK6PQlWuEVJKJQjcQPa3fd2g6ZLMgxVAM81pq84tKV80yyFCs1TamkaKzm9DvQ71rCUKqDUUL+aA1o11fBFUTh/MF2zxVCuWsvOBDyjSsKWwKtx/iaN7+DXjPlyScfY35ulhxNaLfzun/4g3gjI0RBgWcfPUJuA/bedSt//ke/Q6+/CpMR4TU7iMpTtP7qcZfdkku6cZpDKqZo7oLJcRihazXC7RMUR6p0G+exl1s6+v8qsRt+D34G3IJLJClE8lAHFXy9oeeVfj8/8UltOKaWffKXCnMaBBwiAe5yIAtCYIASRD6L9T7btg7RaC3huahCHsgEVQmlwVDX5fIMVcXtTHNZULdrpKAqTYXPanZlAsvSv6PVldZIuC5J5fl0W+LLG+ualcTiXkQVqNQcMvZF6Vbr4lL4uTPbrVQnZrkAMZ64ISYT4ElScQvQQui0YzHDhsoyw2d9AZxSRTiIctmjuZpKizgfTAEyLXxFFguXMTzs2sKlisCzdHtiFeQuhTXLpft1oyV5D92+gEW3Bf2uz3vf/m5mTizywBc+z+5rrydQPbKW5u5//E+ZuH4vgQo4+sQJGq2M6+6+m09/6RM0RofgDbeAl2LKit7hRUmdwzV8uNzspYAkxiwu0F9coK8UemLq/woK8WJ5qdnbIlEHp2FoMH309J3YQoTNPZhdgYULG/bJX+S4uXPHLvf5899zoKQAL4GilRyDoIpVIctxzM7tNxItPkZztsmo76JcRekfOlQSyzVNXYFfS8a9zWCiJpNf4qIYge/qjgBVQlbMvUK5KoABRGmDEIKKM7FdXsHKynpr9rKreMxTMdF7GZhA2NdaRZ5NPxP/y1gBgNxZEuQStdBaLILAF1NMucSotgG/Ko9tcRXSRUkpXfFS0lisCe2Ly+N74st1++LnrawIZ9BrW0oF142nJyGlfk+AanqLLBoTN2G1CXhQX4HJWpXZCxf47KfvpTw+SmlyjPYzz3DXd/4/7H3DrfhasThbZ7XbZ9fN1/KZ+77A+YUVqQ+3FrRPFvfguXmXU+ADqRAkA05tDNiJVIzVkVS0FoDGEF89eUZ/C3Ltaw5ycN8t/J8/+COKnVn0jutIMkW6EkoeyCVK7UQ7S2EQsn3BgrDn39gM6IMtiRncUEKeeRJaM8Ewz8wYrtlxkDx9gvZyStCTZDlPS/NjpcUD8X2J3PmBVB0XXISiGMjh0kwaFiVaiPGXI1cFMFgr2VyDfgo22JAi7Ez0saow/aEnxVGTo+A3pT7Cy4VrUI6dHRB+PVcjkfVdx6SiKzTpu842sSCvryFtugyyQEjLPIdT56Bbl5s9NA7VMWkbVwjkZg8NyYNprsLqksSY6y6FW/vSoSmPJSdodUFMwcW6lNHiChOTbovDTz+EV/DYeccNHH7gAXbe+bW89j1vpVKM8DMPr6pgv+boc0dYmptFhZGkuBFIDfliHbvUhMhD6xCjLN7WADOcYGsJbLFiSORACZTjTEwjR6UdTIdXPjr3VZUrR7nq9klUMcBaS9JahOWt2HIBa1wN/QsdaxBeetkyiEwYiAeREC2zhrVQqhLHPjOLHnt3HGSu/wx5mlOrSc5M4jLR23XWOotrLVZ3vScNWjwFS4symVklY77TfHlXeVUAw6BZceByEvpGvqynXEWlksnQyyFpimuAkbB6Ua1ng2U9wEq+gx+JJRFGsNxypdCeWA9xBmlHWtbHPZdpGcLYNrcMWC79GFoaxsYVYWAJfagEsLAKZ88hxOekC6N2XHZlH5p1Aa9+LqnXoSfnXm1CVPCoFXOUC6mGFrIkQ+k6k7t3snJujtxE7Lvjbp753JfYPr6dolek3a7zxKEv0cwVQ1FEBZ/K6CRWJ+jAkHuac7VnCItFthycZmV5lri/SNrLyRcVPOfM1QzoSd7EIIfDkvztVTm/YvJCUQLHJ2xQ9sOPHePJxhNgLWm3J1V3xoid/mIA82XfM0dAWSXtlzItGX3KSGFNkmG1ptUImA13susaWDp2lLxhKA1Lq4p6T8LqhUCAYbUhfTr80K1pmrtW8u6McV8SAF+OXBXA4PswNuZqDTLAdaJRnsth0BI5MC4PIIxEwaMSbhl5sLHUTvQawtBu2y7VkdbCZE0qgGtF6dHfrENjWVyJqAilcbFQIgvj48JN1GqK7VOKXt9gXW+GLIVKBWrjwn2kji3eMuEaY/hi0g2XxFJJGq70OpR+ktWhIS4s1MmslWXJIleirSPCasjpp46jdYHP/Y//QNJsuMInlwtOLpkvQYmgOEwyOkwUeRibYbMUu7JIL+5y6uQzV3bTX2a13d8tMazlKOgQr1AFLyKPU2dp9XGrl9A7tbBB/62knXYyMfW+4i6WBlUEXIWd57Itc8Q8thnkijxX9GLLovUZqV7Hln19jj50gnhZosZxJtZvpSrAUDMuKbAAFxbXy1gGEyouEZD/X3tnHiTXdd3n77z3epvunn0BMNgJEAtBhqtEaqFIRpRFWiXJLtmSolIUlypK4ixyKVWyVK6KK5XkDyeVeIkXmRUpJTtWJMu2IpleZImkRFKkSAIkQRIkQQIk9m32mV7fcm/+uPfNNDCgMCAwMz3A/aq6+vXr192n3/J755577rkXGE3fSlsIQ5r6m9jh0zk7fkALEJlAHQ0T7Mvb+gR+wVx0xbx5v7cf6l0wXjKR/tqkGdBU6DB378F+cyHGNpgTZGzug292cpAzgpPNmp1ZbWpyaJMVGdv5GsR8rjKFKeZSMco9M2EUOs6Y3zhzwghD3RZgKWXN5wY2aKamgYI5J0IF9bpHd/8Qx145io5jEiokaTT83AkWkxlIZogap5mcWNJDtEIxuQpKeeZCj6sYUYiZveJbL/yoAafeNO+n4+8t4nvoBTUdWtUkDQZnQfLmDpiIGTYrNsFK2wi752Nn1bNViqBeUxw5HuOt3kihfIyxM03C5lwN0FodVq8y55oXGa80TOys663/LTYxiouhLYQhsl2DSWKW0bZ9rsxFmoRmv2V8W8IqMd2MHQngG9dfhaapkc/A8DpTz0Epc5jqdZiagkbW/oYyIzVzHaY7B9+MCQpypntTN00eQs0mBMah6WKME3MQlGcy0Jo1W1jFsyKmjQBNTNmKTLa/edL2tsSqgedDqUuoVDUzMz6d5T6OHz5D1FzQmOW3IA9+EZEQkQQV2TbVVY0GqqCq6MYCVTRpmH7uc8gVCmQLBWbGx8/zodaxFr793Zi5kZotCVRJaHsvtEmg0jZA4Hnm9g7WLTYCpBPFdKXJ6fE83YUuCoUzRHUTFJ+qmJvkyARmqL4992eY61jNYlqQiosf69YWwpAo80d9m9GYyRjXPK2Eq20Gl+/bgrEhTFRNfkKs7eFIzPtNW6xCBTaHPGt2pNYmqJjtMMraqMBY1XgCuYJJrPJzoEeNwFRmrLAoc3fvaNgaED6zpeWTnP0D2jR14iZMNCDXD0Fk6jDoEKIKTE7BydGQbAfkghK961cRh11MjyckceoZpF1eF5tMFEFSAT+DJm8HeUQm+DErED/LL860vKeZa5+nbfWrW2Sa9TrN+rk5CllMJmMes/9gzhPxjCuqA/PaF8jUjcenEkzKdWDEQITZmYOUMi6z0ubzNk+mUhOkUaZeMc2eStXcpGoN80gzMgZLJu42Xp87im/3yLWFMACMT5psQ/FNkpPnmQsyLXVWrZrcnUzeDE/NZKEjtvUPEpPokc2bPtxIzXkL4zWzg4LAJEUFsR0tHIAqmjZakjHdn4LJEksnwEVM875UNkKfRFaEfOiwg7C8jGlOqNh4Ml7WxpG0yWXQygwnz/b6kC+xadc6uns2cuTgGMffPEqj3kQp05/uZzvI5jJ4no8XFOnoWYX2fBrVOlF1mjiaJI4a6Di2J9jZD/2z5oD4madIa8ChdbsrLir5Nmgd36AwguBhvIEcc96CPVFTFxLPuJYqAkIIx+wyIJ3mTqXTpoTNe/aw6yK7rgBZIVfqZNXQDupHDtFVSJiumSOWjvQVYMsq8zPNwMwrESTmuhDzzXie/coF0hbCkE4DX7NNBjAXatg0E3mCyd2JPVszMQNh1cQOgqKdKBabHWkHVnk28BjbngnRJpmp1GFm7AltMRQPo77VuglcZorms0kNtB0jUdWm+Zkvm5JacWSaHIk/l5GpAhv4V2b8hIpBogzlzjK7brmOoTWbUV4HE6cm2P3U80wePUGm2EPP+uvZvG0Xg2uGKfcPUCx3EmRyaM9DsnlTo6JRpzo1RWVykpnJaaKpKsn0DM3qBGPjJ6lPjZGEVeK4Spw0UOnkFJ6NwCeRbS97JnQtGfA7zZ0pHDNtp7My+a5WQUj3QTrKMl2XDrpK78NZbCkdu40V1tQD0Im5I6gEqBlPodUL9K2XITYFd9aZS2zATZtkhGIGr5ghU8zRuaqEzhbwixXyOVgrcHwEejvM+ZhoUzIe39Rq6M6b3rvxKThRNTfUeKUFH7XtqvQDW2Mha/IAmjGzRVqCnBEJ7ZkL2bMJHsqe+0rbUm72dc4OS0WZsRWBD4Qmp0H55s6e9czh7ijaUmodc7GBbCfQaYKGtYatZKbN+Agw7pzSdrSnTbhSDfDLefI9fWzYvION62+klBtmbLzBiwf3c+zV3cRTk+R6VrH1Fz/M0NYdlMs9oBS1KGJGKWQ6ol6ZIopjPF9IohAVR4hOiG156sQ3+bBeZ5mBNetphk2b1NhAiyZJImKJ0L5CpEkcTZNoTaKy+EEe4oAk8czku1NTxJPjZsdHke0mqZs/FWib7JHY0WrKFK1IYlsdJPUuEuz4UeaaQyut+ZEOrkpjBunoS8WcKAgmOTldn66zr1Uyd0Kmn/c828BPvyNrIuc538Q0IsVsWm1a3TUbQEcOLycUu4t093VQbx4Bv0Ezb8bk9JVARmGibtL9pxomvyeJTZikUoPekhnLoz3TM3oxiF7gDEGLSb5D9I23BySxZmwiMS6S39LaFRvQTcxpl16QaUA3Nr08s7NZC2Zfh6F1owLj3od10CNAtxnFGdixDdnAdPXkCzA6aa4RnZgd6tvh2kkEaJAIk7iWvs4KQU+B8ro1rF63g01bb2Ggdwu5TA+HD49wZqLCkdHThCokXy5Q6uoik8+jRNBRQlxvQpyg4gStNT5CWGugE41OFJ7SJCrG8zQ6itCJJkwSoijGQ2x2p6kZqJII8T3E0ygRlB+gJCIIQhIRNAE+QqBBKU2sIG7G6Jm6CYJobSYyKGYICgG+H5NH48UxKoH6TJWw2sTzBHX6lNkJumYUUXyTIDE7KKnOyvE8Uo+gddRlGutJhSJitkLv7H/MgdimhWCeg4yJAnqeaUNGodk3qg7RqPmc1welvNlfcWQqAxU6bFPEnpCNGn7Owwur5PyYpDpKOFEl02nP6dBM3SiYm55gu/rP9+/EnMsk7NFa37qQPdIeHoPATDWmIysUi6aJkMZf6qHJN0jPuSgxMYbYtt9nSY+p3TlZfy5JShKbJRaALgGTZryLKpqh0KmXJ1nbuxDa70mFviUJTnvglTOUNvbRv24zqzZso2dgCx2lYeJGQK2uePGNKmMzZ6g0Q+I4IcnmUInPTE0xVZsEFeElEVkBTwuijQioWJHEiiRMSKJULBIQZbLbEoXS2tyUlN1xGrRJwzPtJbHBLgV4MXiK2Fc2wBWjRaO1QsQ2OhuRedRCIwwNBbGgE0GyCi8nqDCmUW0Q1SOohqhZ5Q6NEgcF02WUZI36YuvSUWDuLtzaTm/Ynbr8NyVDKgzpBZ9nbhxFKnA5TBaMMleaXzTb+4G5a3mY7SSBuGKSXJS2A6tS7yLBjGqbstOvW1H1pqAhNjhmxTSKZqNHraHoZMyaaTfzbbd7en/P+sZbroYt2dpvo3XYFh5DkBfducH0z8bWZU/s/ve8ufJVWpvU5rhlR8yObUmYTSqjgUnxtUOz6Wh5P2SumnFrs/qtEJCcR6avg671a+hdey29q3bS3bOBrNfJ9HTEyNgU45MTVKKQZpwQNZqo2PZ1ptNMtQT9RTQ+GpXE6Ni46TqtjR9puwMwyieK2Sm0UrQ3154V37o2MhdTSLu+tBUKsaKhbDMgUXZEWWJVl7lobRKb7wk8c1vKYtxcAnNXS2f4qc+Yu6AoUE3zqFfMf/ZyJvgTBzZtOLEHJVXcNA1zOUibOdLy8FqeOzDCoDDFWtIeoxxmhoQ6SGjEUItpWqnI/H+d3q0WchW+veaWl2N2YhkwDkqQMU1grWwHiG2Gp16255HOkL6yPAalzWCkKds1GdhBUCqei6xi/thccWKP2YpNs/s4vQmlD7BRybf44bc6LgJewSfbX6Zr83p61m2js3cHgdeHDnPMTIQcPzTJTOUw1XqVqNlAx5FJwkgSGy5uuUi1vSiVedZKE+s0SUPZ7ChtP2sfmpbvsC6CpF1hNvDV2gcumLMlDXyhTXKH2B2ntbXDpvpqxVyALeYstUyy5kECTc+kl+ZsV422lXSlYII1KsRUwy2C120n96iZIcs6FYLU/VpO5JzlVlFI6z22xhVg7k5jk11oAqP2XJtu2ebt8PY+pyJMvhTmcCc2fok23fzZwAToEROTixq22X2RHkNbCIOOTTe8joHE9PunxyZVxkX3Oj0IyhkKQ/10XbOR7jXXkuvYhCd9VKdijh6oMD12jLBaJa5V0VHTGBdH5kjEyVwWiWCX07twMicO6d2Zlgt21p3B/vG0Pzxtv6SuaLoujSQlzJ3ECnN3S9vAsX2dntwpGnPY099NndUA6ATKzA3LzJuoauSb6FUT2xUkJoAmHUYYVGhyw6OK7eGoAxXaK8aQ7t+0BkNrXCG9DGwvwux+tscQ7Po2SBzLMpt24sFc8S2x9xhsw0XZ0cdw1qm0UNpCGFCgLzJl83LhFQPyg930bd9Oz/D1+Pn1NKs56hXN2Ik6M6NvUJ8cJ2k0TXCjGZpIZJLY59jKeGs6SXrBpTn7yTnvt17oivkXeKtQtF7AcPYJq8/5XPpovevR8rlWN7pVaEr2UeDs9nZLl4/yjFfTtF4IVeZ6yZugZzAD/mssnyCc7+xvzUNobTak++/cY/ezWP5mdxr/TOclaUUl5hClzex0iM3KFYYlRgLID3RS3rCRzvW7yBa3opIexk+HVKfr1KvjhJVp1MykGW4Zph5BbMcvtLrI6UWdPlpFIaX1tWb+hd16Z2p9L92+tW20EM63bet32CbJbOZeB6YNnWY6pjEAzVwzIPVkUr80faT7oR2Cia2/L+esbxXtNsHH7PoGpr77QiZ+aEmpEM8eCtua1DGz5d3Oiru1BCsXytUjDAKZjixda9dR3LILv2cbtZkSZ45HhBPThJWTqGbVZIEkoSnMHzVtP2XaTdXqrrdeHIr54qDPebQKw7kX6XLQmpxTxbjJrZ4MnCUG85Jsl1sEzkfrbbEd7TuHtByDAvEDtBczz9E7l5ZDMNvMTnvPrCMkNhFTh8xpvfMYzkEg31Wi59oddG66hYZazZmjdcKXJkimD5sJ/sKa9b8iTMGC1B1O74gRc3fRhPNfLG/nzr6cJMxNxbYSE5LOxwr7D01mw0B6pokUCyan5EK0/k3rPeiWEhQ6DSPJ2ds4YQATle0s0bPtOrrW3sb0TJFDL1YIR/ajp0dN1hJp9l4agKvYR8jZfc+tMYIVdvItiCvxP60Q0l0faXS8AFE4l1TTW0aSz7buWr//IrkihcEv+PTsvAYpbmdqJMuZR15Bz0zYEk9ppD6NyGCXpzAFEZe7W81x1fJ2LuI0dnBuEetL5IoThlJZ6Fw/wMhRRTT2PCRTzHkGLXntwFzXVdq150TBsUK5zDHVK0oYMoGweqibsdMKaUwimRidpF6Cw+FYKFeMMPgefOT+G/j0r/waZ2a6ePnIJC8fOcXrr7/BzOQ4lROHCBvj0KyRRBMm/8C1rR2O83LFCEOxFHDzzdfz3vfcw4wqUvm7J+gY6OGGd9zA0UNH2fOTx8gEeUpd/VQrM0h1hDdffILq5IkLf7nDcZVxxQjD8HAX23bdQaHUjYfHh+69g+l6zPP7DpCtnsTf2s/IyCjKm+L6XZu5+65P8+zzd/O//ud/Y/r0oeU23+FoKy6yfEP7smb1RtZecx2nRsYJxGPVQJlyJqLTr1FUTfKBz913vodf/ewn+Lm7bqLcEXHzdeu570MfwPP9C/+AY+nwPVO8MCMX3vYqxbNlEMVjtpLc5eSK8Bg8gY0b1pPJFBkdH6d/cIg4UQz0dHLXnXcwPVUhjiN2XredbdduZt8r+8l0BEwcOc26vh4yuQLNWuXSDRGh0FGkUW+QzZdo1iYv/Ttb8D2Pru4y+XwerQVPPGKlEU+IwpBcLofvge8JcawIo4hKpYpSZgy7VokZNuQJSZKwc+swQwODaJVhYqrGsRNnGJkapxm2VmdaeqQrTybwCc9TsdlhuNiqzxfLFSEM4gl+Ps+awdXk8mWUCEmsaCYeohO2bd/C6JljVKbGee75BmNnRtGnBRVG3HbrzfT1dnPiUoVBPPwgwx3vu4fHH/oBd977UR7/wf+jfpnEIZ/P8elPfZwPf+hexqenSGIQCejs7WfjNVtBKVMDQjQqUWSzWaZnZjjw2msUiwWazZiTJ49RzMBQucj3/vRr/Pvf/zzDQ9eRKwyRhHD44Eke/fHT/Nk3v8Ezz+1huWp16PGa60e6EIt8aC4oDCKSBx5lriTuX2itf1NENgHfxEyZugf4tNY6FJEc8CfALcAY8HGt9aFFst/a6JHtKOF7PiICUcKx46c5M17Fy2Q4cOANTp48wfGjR+nsGeTmW24kjBM8XUY3Y6Lk0juB/SCDSIZmrcF1N9zEpmvW8+wTpcsiDJ4H73/vrXzyFz5AUBxA57rZtmMHpc4y2UyGrJ3jPJMJ0EohCL4foLRm8zWbqU5PEcUR0zMbicfOMPXi8wz1JAwP9dDdtQElXXg5zfbrSmzbsZO77nwX//Jf/xue3LP7km13tHCZk5AWk4V4DE3gHq11RUQywOMi8nfAF4Df1lp/U0S+AnwW+CP7PKG13iIinwB+C/j4ItkPgJfxWDM8TDYIUMCRkRF2791L/8A6To+e4dWTr6MDGBxYx+bNG/jA3e9mdGKKE8dPks/m6BlexcjJk5dmhAbxNK/vP8B73/9RTp6cohFdnvteX1eZW6/fwHPP/ZTONTu5/a4P0L96NXEc02yGeFpQWggTqNZDfFEM9JSpNZsUuvvpKhU5fvgVunIex46fZHr8GNe9dxfoLdSSIicOHeSZHz/MsSOv0d/dwYaN2/lnn/w4L+9/lanKZWhiOZAAsqWA5uTKmBvwgiELbUjPjgxzs5PcA/yFXf914KN2+SP2Nfb9fywiixpFUkpx7MwYLx07xp5Db/LDp59hOk7Idnbw1CvPUKlNs23rNt5757vwg4BqpcFQd5kdWzeQyeaIk0tvyyZxk2KpxOp16/nB3z7I0UNvkiuUL/l7ReCW67dQyHlUZ+p0dvfQrNcYOT1CEsWE9QZRGOFpTa1SpV6dJmrWqDZDamFCFDUIRSj2raXQvYq+rTsJVQfbdtwPuTXMTM/w93/zIM/s3ctMnOGHj73I1/7s2wyuGeCD99x9yfY7DH7BR7VBGcWFsqAYg4j4mObCFuAPgIPApNazAz+PAcN2eRg4CqC1jkVkCtPcGD3nOz8HfO5S/wBAHCq+9ntf5Ztf+xbZQplybz8dPT2sWrOKRpwQN2K8rcKWbTfyj266kX3HThOFNYZ7ewg6spQyhcthBv39fbzx6nNU65rnnzwCcul3h76uEne95xbWrN9I3zU3snHbLjIi1Co1okwOX3yiSJPxhDiMqNWqTIQxYQIDqweJogbNZky+2I3nNUnyE4w1s2SKw4RxwsH9++kbXMXt97yf63bs4smfPsn+vS+S7+zjPXe/n7/8m78lvgxNrasdL+MTV1rPh7S5p9Cq/fbvgoRBa50AN4pIN/AdYPul/rDW+gHgAQARuQQpFRCPRq1Go1YDznDq8EEA9sJsf84z+QLf/D9fp7t/gHJfH5nOMvHYKJ2FgKNvHL7UvwPAoTcOoJIYpcRW0bh0Ydi+eZhiR5ntN9xOz5pNlAomltJdKKO0ZqpSI58LEDy6y2USDcWSh68SGlM1ao06hXyeIOsT4TE5U6XeUaR73QZGT50in8sTZLOoRBHW6uzcto1rN21hbGycYrmbUrnM5OTkpe+cqxwvC9qWbc5ksqzdeA0bN27m+T1PU280adRmaKdM3IvqldBaT4rII8AdQLeIBNZrWAsct5sdB9YBx0QkwEzUN3YZbT4HaakQez6jFWhFWJshrM0wffLIolkSt8YULrb65nnIZTPceed7eeddP0f30CqOHT1Eo5lQKHZz9NhJrt22nb7ebiJ8Tlcjsr5HpR7R19dN3vOohRHVagNfoFmrU603OXDgIJ2rB+gbXMXM66/z4x/9CC8IuHbbdTzx05/S29vH0NAqOjqKTI5PLlvPxBVHDNrOKXfzre/gnns/yNrhtdRrVbbuuIFvf+N/W3FoDxbSKzEARFYUCsC9mIDiI8DHMD0TnwG+az/yPfv6Sfv+w3rRzq60bM2VydDgIO96372ofImpWkiuWOa5vc+Q+HlOHD/NxHSdRrPJ2NgkMzM1PM8nmwvYvHkDG9atgTgialSZmZpg3YaNaPHY/9p+7r773eB5KDw6u3sZGBzi6Wd2c/DAG4BQqcxw+sQJjh0/wsxM+5ysK5lwOjIBahE2XXMNt93+ToqFIr/8yU+z64ab+cnjj/LGq88vt5mzLMRjWA183cYZPODPtdYPisjLwDdF5D8DzwFftdt/FfhTETmAKcz/iUWw25JWQF5oMc+VxYmTp/nib/wnVg+vo1TupW9gEIAgl6VvcJCgVCCX8eiSmMlmlWbUZPPG9QwO9uJ7MfWkRrU+hedppicmOHj4ME/85CluufkmHv7+I0xNTHLs2CkefewnvPjCS0xOTjA5MUEzbBDHsfMWLiMqtM2IbJaBodVs2bqNV/a9ws5d11FtNCl199FOlbQuKAxa6xeAm86z/g3gHedZ3wB+6bJYd0HScmxXJnES8/LLL/Hyyy/NrRQxVSU8Dy+bxRMhm8ngBRl8L+DBTAbPE7KBwgcygclzqNRDpus1Jicn+Xf/dh9hqEiShDiqobWrQ7FUFAodZLM5To+McOTYMUqVJg8//COCXJ5MLk/UfBtVnBaBKyLz8apCa1N/KklQdXMSXWy2xPSUyytcLuqNBruf3c3o5BQvv/wqxXIPzz75ODfc9k4ymcDMFdwGOGFwOJYQEaFSrfHID/+B8fFJuvpWkeiERt2OaWkTnDC0PRn7bGeLEtsVirZzBvhmQpggZ+edxHaVQpDN4qHxM1kKhQJ9A4PkMz6NWsWUF1eaRr1Go1EjbDaJ4xClEpJEEUexmVDXcVmJwib7X95HdXIMP5Ons3eINes2k88GeIubB3hROGGYRcgXe8l39iK+oNAkcYSqVYjDGnFignF6dqq5pVL31hhK3FKo2tavTKe9SzxTNzyx43B9H01AkMvjBUUkX0T5JYJCB309q6jXatSrVVRTE6sGYaxIogStIpTSLu6wSPT0D9JoRGiVkCt0Uq1UCEQYPRO2VbDXCYNl7fqN/Jf//nts2LQBpRRJEqF1QnWmwr5XD/D07r2sWztElCRMTc5w5PCbjE6MU282UHFEfWKcqFZBaTPEWamEMIpQoZ3O7rJHm61CpBewis9+S5m5cmo1AKEyIoyJhwQBnheAVqgkQidXbvC2Hdm4aRNvHDxEfmANqzds47VXXkEnikbdJ0raR4ydMFiuv2EXH/75exARckFgB0UJidLce9d7GDkzSl9fD3GiiOOY6ZkKp0fGCIIAX4TRsTFOjYxQLHbgZzKMjU1z5PhxqjMz6CSiEYdMTE1z6OhhXn91H2NvvmmniF8K5kRER/FKGeB3RXLi+HHq1WniKGZ66hliyVPM5rjhplt55okqU83jF/6SJcAJgyUMI7RSZDIZMr5PM4rRsUZ7gud5rF2zilhpgkCjcjlK5RJrh1fb5gVorfBbPftEo3VCrV4nl8shCPVmk9GxMZ7es5vf/PUvcvzoseX+244l5uTR1vR7D8n4TI+dZmq6Yj3L9uCKKe12qfiBh9aKRCmUMhPYioqROEElishOc6/toBeVKJRSqMSuQ9AiiCcoEfyMTzafo7uni2w+R5DLUO4qsfmajXzk5+/jtuuvX+6/7FhmvEweHdfp7u5k+7YdxHH7VKxyHoNleHgNuWwOtKA0aATf91HalI4DjbZxAq004mlEiwkYaTGzDStB0vnJPeM+JIrZoKXWGjyPTJChGSd0l1aj8j7FYplsoMnnsiRJQr3RoFqtUa3MEIf12dwFMylh+wSoLpZcPo9KEpIkme2aE4E2irktEQF+No+fyRNG07zvffdx0w07+cbX2iO5Ca5YYWidem5hnD49ShiGZIMsOgERH7GzgSo7Kg7PQ8RDSMzYLDRobScTFrz0JNdiYoKe4KFRLb0InngkGmaaTcI4BK9ARAzKQ+IEjSZB8HNZCrpMnM2hNMRRTNxsgD5fBkw6wxaYky5HJptFq5ggyCBegOcHaK2pTE/bORKXtmBIoaODa7fvZOPmLUTNJnuefpJsNsvIyAiNRvtcEEtBrqNIJl+kMn4GLyjSPzTIi3v3kFymwj6XgytUGHzmJqRdGFprfM8zPYAiCEKiIVGmmaAQvEQjohHEBP41iDYXpCeg4gRPwBPfpBvM+hhGELQn+L4QNRPCJKbWGIMG1C75/7Z6EiFJGJFEJu8+TAVDTHem6YZc2lu053ms37iZ3v5B7vvQh6nOVLnllts4deIYf/3X3+HUyatLGHwfmtVpQOH7gFLsfeEF2skbvEKF4eLvhq+9vp+Hf/IY61atIpvJk8lkKOQL5PLm7utnMmgEEW3HKwgiglIapU2dxVil4iKmuSGg8NAqwfeMoKAhjhWNWnUR/neKnvXPU2lauryL+dx8663cc+/93HDDTfQPDfKTJ55i93PP8sbrL6N1wuo1wzSbTcbHRi/8ZVcAcbNOFEYUSj1s2LSVPbt3c/C1ly78wSXkChWGi+fg/lf51C/+Etlcjny5TFexRFepSKmjQKFYZNOGzQwN9NE30E++q4uuzi76+vro6u2lVCyRz+cpFAqgtclVFEGLHfvpm7u21ookgThJZj2NK51MJsN733cXw2uG2bXrep7a8xw/+P732ffiU4T1CipOTE/OVRRoSJKEW269jVMj4+zYsZM33zzE9MTIcpt1Fk4YUjTUKhVqlQqMjXHqnLdl1kvAeAyeT6FQoKurk0JHB7lcnuENG1i1apB8IU+xs0yp3MXq1asoF4r09fbS2z9Af18Pr+3fz/j4+HL8yyVnaNUqisVOyqUSTzz5ON/+9l+y56c/JmrWrtLAoxHL+z54P3/1vb9mfHycNw7sa7v0c2kHpb600m5tjOeTyWXJZgIyQYZMPk+hkGd6YoLJ8Ynltm5J+MWPfYx//i8+TxQ2+MM//kMefeghatXps7YZXjPMtmu386NHH2mrgUSLhYgwODhEQsD01CRhY8kqce/RWt+6kA2dx7CYqISoXie6umJrs4gIu667nvXrNvDQw//As888PU8UAILA593vejfPPrebyampZbB0adFac/r0KUSkbZtQLsHJsWgUix14fsCjj/2Ixx57lKmJ85f+PHzkCF954PepLWpAtv1oV1EA5zE4FpFiqUSj0WDPs8+w78XniMO37qcfHbs6Yi4rBScMjkXD93yUTmg0Gpw6eYLkZ8QP2vjmeVXimhKORSMMm4yNjlCZmaJyoanu7DgTR3vgPAbHojExMcljj/2IbK6DqKUZ4QmoczyEIMhQKJWYmXBNinbACcNVRrFYplqtsBTpt0mScOC1A3i+f1ag7VxRAFNWva9/yAlDm+CaElcRQSbLNVt24PtLd9iV0sTRhVPUs9kc3T19S2CRYyE4j+EqIo5CXtj79HKbMY8gk6VQ6sHL5pbbFIfFeQyOZadvYIiPfOyf4AfuPtUuOGFwLDu+73PH7e8mn80utykOixMGx7LTDEPQCaOj5w5dcywXThgcy06Q7eDNI29w8viR5TbFYXHC4FhWgmyO+z/0C5w+fZrq9PwBVo7lwQmDY1np7O5l27advPbKPqJG+1RJvtpxwuBYVm697XaqlRle2rvHDZhoI5wwOJaNwaEhdu7YyeOPPszY6RPLbY6jBScMjmUhX8hz330/T71W4YXnf0oSuzk024kFC4OI+CLynIg8aF9vEpGnROSAiHxLRLJ2fc6+PmDf37hItjtWMP39/cRJwqM/epjxkTPLbY7jHC7GY/g88ErL698CfltrvQWYAD5r138WmLDrf9tu53BQLOQY7O/B94Wuri5+8vhj7H91n53rwtFWtE6f9lYPYC3wEHAP8CBm2qNRILDv3wF83y5/H7jDLgd2O7nA92v3uDoege9pQHv22T2W9LF7Ide71nrBHsPvAF8EUmnvAya11umwuWPAsF0eBo4C2Pen7PZnISKfE5HdIrJ7gTY4rgDixJxCKnFeQjtzQWEQkQ8BZ7TWey7nD2utH9Ba37rQctYOh2PpWMhwtncDHxaR+4E80An8LtAtIoH1CtYCx+32x4F1wDERCYAu4PzlgR0OR1tyQY9Ba/1lrfVarfVG4BPAw1rrTwGPAB+zm30G+K5d/p59jX3/Yd3OdbIdDsc8LiWP4deBL4jIAUwM4at2/VeBPrv+C8CXLs1Eh8Ox1Lgp6hyOq4cFT1HnMh8dDsc8nDA4HI55OGFwOBzzcMLgcDjm4YTB4XDMwwmDw+GYhxMGh8MxDycMDodjHk4YHA7HPJwwOByOeThhcDgc83DC4HA45uGEweFwzMMJg8PhmIcTBofDMQ8nDA6HYx5OGBwOxzycMDgcjnk4YXA4HPNwwuBwOObhhMHhcMzDCYPD4ZiHEwaHwzEPJwwOh2MeThgcDsc8nDA4HI55OGFwOBzzcMLgcDjm4YTB4XDMwwmDw+GYhxMGh8MxDycMDodjHk4YHA7HPJwwOByOeSxIGETkkIi8KCLPi8huu65XRH4gIq/b5x67XkTk90TkgIi8ICI3L+YfcDgcl5+L8Rju1lrfqLW+1b7+EvCQ1nor8JB9DXAfsNU+Pgf80eUy1uFwLA2X0pT4CPB1u/x14KMt6/9EG34KdIvI6kv4HYfDscQsVBg08A8iskdEPmfXDWmtT9rlU8CQXR4GjrZ89phddxYi8jkR2Z02TRwOR/sQLHC792itj4vIIPADEXm19U2ttRYRfTE/rLV+AHgA4GI/63A4FpcFeQxa6+P2+QzwHeAdwOm0iWCfz9jNjwPrWj6+1q5zOBwrhAsKg4gURaScLgMfAF4Cvgd8xm72GeC7dvl7wD+1vRO3A1MtTQ6Hw7ECWEhTYgj4joik239Da/33IvIM8Oci8lngMPDLdvu/Be4HDgA14Fcuu9UOh2NREa2Xv3kvIjPA/uW2Y4H0A6PLbcQCWCl2wsqxdaXYCee3dYPWemAhH15o8HGx2d+SH9HWiMjulWDrSrETVo6tK8VOuHRbXUq0w+GYhxMGh8Mxj3YRhgeW24CLYKXYulLshJVj60qxEy7R1rYIPjocjvaiXTwGh8PRRiy7MIjIB0Vkvx2m/aULf2JRbfmaiJwRkZda1rXl8HIRWScij4jIyyKyT0Q+3472ikheRJ4Wkb3Wzv9o128SkaesPd8Skaxdn7OvD9j3Ny6FnS32+iLynIg82OZ2Lm4pBK31sj0AHzgIbAaywF5g5zLacydwM/BSy7r/CnzJLn8J+C27fD/wd4AAtwNPLbGtq4Gb7XIZeA3Y2W722t8r2eUM8JT9/T8HPmHXfwX4V3b5V4Gv2OVPAN9a4v36BeAbwIP2dbvaeQjoP2fdZTv2S/ZH3uLP3QF8v+X1l4EvL7NNG88Rhv3Aaru8GpNzAfDHwCfPt90y2f1d4N52thfoAJ4F3olJvgnOPQ+A7wN32OXAbidLZN9aTG2Re4AH7YXUdnba3zyfMFy2Y7/cTYkFDdFeZi5pePlSYN3YmzB347az17rnz2MG2v0A4yVOaq3j89gya6d9fwroWwo7gd8Bvggo+7qvTe2ERSiF0Eq7ZD6uCLS++OHli42IlIC/BH5Naz1tx7QA7WOv1joBbhSRbszo3O3La9F8RORDwBmt9R4RuWuZzVkIl70UQivL7TGshCHabTu8XEQyGFH4M631X9nVbWuv1noSeATjkneLSHpjarVl1k77fhcwtgTmvRv4sIgcAr6JaU78bhvaCSx+KYTlFoZngK028pvFBHG+t8w2nUtbDi8X4xp8FXhFa/0/2tVeERmwngIiUsDEQV7BCMTH3sLO1P6PAQ9r2zBeTLTWX9Zar9Vab8Schw9rrT/VbnbCEpVCWKpgyc8IotyPiagfBH5jmW35v8BJIMK0wz6LaTc+BLwO/BDotdsK8AfW7heBW5fY1vdg2pkvAM/bx/3tZi9wA/CctfMl4D/Y9ZuBpzHD878N5Oz6vH19wL6/eRnOg7uY65VoOzutTXvtY1963VzOY+8yHx0OxzyWuynhcDjaECcMDodjHk4YHA7HPJwwOByOeThhcDgc83DC4HA45uGEweFwzMMJg8PhmMf/B0xgOU1FB8PQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(ims[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28675322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ims[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08770875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 11 14:14:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   38C    P0    86W / 260W |      0MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b35994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
